### YamlMime:UniversalReference
api_name: []
items:
- fullName: onnxruntime.backend.prepare
  langs:
  - python
  module: onnxruntime.backend
  name: prepare
  source:
    id: prepare
    path: C:\xavierdupre\microsoft_github\onnxruntime\build\docs\md\xavierdupre\microsoft_github\onnxruntime\build\Windows\Release\Release\onnxruntime\backend\backend.py
    remote:
      branch: doc
      path: C:\xavierdupre\microsoft_github\onnxruntime\build\docs\md\xavierdupre\microsoft_github\onnxruntime\build\Windows\Release\Release\onnxruntime\backend\backend.py
      repo: https://github.com/xadupre/onnxruntime.git
    startLine: 46
  summary: 'Load the model and creates a <xref:onnxruntime.InferenceSession>

    ready to be used as a backend.'
  syntax:
    content: prepare(*args, **kwargs)
    parameters:
    - defaultValue: <class 'inspect._empty'>
      description: 'ModelProto (returned by *onnx.load*),

        string for a filename or bytes for a serialized model'
      id: model
    - defaultValue: None
      description: 'requested device for the computation,

        None means the default one which depends on

        the compilation settings'
      id: device
    - defaultValue: <class 'inspect._empty'>
      description: see <xref:onnxruntime.SessionOptions>
      id: kwargs
    return:
      description: <xref:onnxruntime.InferenceSession>
  type: function
  uid: onnxruntime.backend.prepare
references: []
