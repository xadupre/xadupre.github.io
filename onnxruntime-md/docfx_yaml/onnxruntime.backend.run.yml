### YamlMime:UniversalReference
api_name: []
items:
- fullName: onnxruntime.backend.run
  langs:
  - python
  module: onnxruntime.backend
  name: run
  source:
    id: run
    path: C:\xavierdupre\microsoft_github\onnxruntime\build\docs\md\xavierdupre\microsoft_github\onnxruntime\build\Windows\Release\Release\onnxruntime\backend\backend.py
    remote:
      branch: doc
      path: C:\xavierdupre\microsoft_github\onnxruntime\build\docs\md\xavierdupre\microsoft_github\onnxruntime\build\Windows\Release\Release\onnxruntime\backend\backend.py
      repo: https://github.com/xadupre/onnxruntime.git
    startLine: 79
  summary: Compute the prediction.
  syntax:
    content: run(*args, **kwargs)
    parameters:
    - defaultValue: <class 'inspect._empty'>
      description: '<xref:onnxruntime.InferenceSession> returned

        by function *prepare*'
      id: model
    - defaultValue: <class 'inspect._empty'>
      description: inputs
      id: inputs
    - defaultValue: None
      description: 'requested device for the computation,

        None means the default one which depends on

        the compilation settings'
      id: device
    - defaultValue: <class 'inspect._empty'>
      description: see <xref:onnxruntime.RunOptions>
      id: kwargs
    return:
      description: predictions
  type: function
  uid: onnxruntime.backend.run
references: []
