<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>LRN &#8212; ONNX 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sample.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LSTM" href="onnx__LSTM.html" />
    <link rel="prev" title="IsNaN" href="onnx__IsNaN.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          ONNX Docs</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../onnx-api/index.html">API Overview</a></li>
                <li><a href="../operators/index.html">Op Schemas</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><p class="caption" role="heading"><span class="caption-text">API Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.checker.html">onnx.checker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.compose.html">onnx.compose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.external_data_helper.html">onnx.external_data_helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.helper.html">onnx.helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.hub.html">onnx.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.numpy_helper.html">onnx.numpy_helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.parser.html">onnx.parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.utils.html">onnx.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.version.version.html">onnx.version.version</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.version_converter.html">onnx.version_converter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Operators + OpSchemas</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">ONNX operators</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">LRN</a><ul>
<li><a class="reference internal" href="#lrn-13">LRN - 13</a></li>
<li><a class="reference internal" href="#lrn-1">LRN - 1</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="onnx__IsNaN.html" title="Previous Chapter: IsNaN"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; IsNaN</span>
    </a>
  </li>
  <li>
    <a href="onnx__LSTM.html" title="Next Chapter: LSTM"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">LSTM &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="../_sources/onnx_doc_folder/onnx__LRN.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="lrn">
<span id="l-onnx-doc-lrn"></span><h1>LRN<a class="headerlink" href="#lrn" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#lrn-13" id="id2">LRN - 13</a></p></li>
<li><p><a class="reference internal" href="#lrn-1" id="id3">LRN - 1</a></p></li>
</ul>
</nav>
<section id="lrn-13">
<span id="l-onnx-op-lrn-13"></span><h2><a class="toc-backref" href="#id2" role="doc-backlink">LRN - 13</a><a class="headerlink" href="#lrn-13" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong>
* <strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#LRN">LRN (GitHub)</a>
* <strong>domain</strong>: <strong>main</strong>
* <strong>since_version</strong>: <strong>13</strong>
* <strong>function</strong>: False
* <strong>support_level</strong>: SupportType.COMMON
* <strong>shape inference</strong>: True</p>
<p>This version of the operator has been available
<strong>since version 13</strong>.</p>
<p><strong>Summary</strong></p>
<p>Local Response Normalization proposed in the [AlexNet paper](<a class="reference external" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>).
It normalizes over local input regions.
The local region is defined across the channels. For an element X[n, c, d1, …, dk] in a tensor
of shape (N x C x D1 x D2, …, Dk), its region is
{X[n, i, d1, …, dk] | max(0, c - floor((size - 1) / 2)) &lt;= i &lt;= min(C - 1, c + ceil((size - 1) / 2))}.</p>
<p>square_sum[n, c, d1, …, dk] = sum(X[n, i, d1, …, dk] ^ 2),
where max(0, c - floor((size - 1) / 2)) &lt;= i &lt;= min(C - 1, c + ceil((size - 1) / 2)).</p>
<p>Y[n, c, d1, …, dk] = X[n, c, d1, …, dk] / (bias + alpha / size * square_sum[n, c, d1, …, dk] ) ^ beta</p>
<p><strong>Attributes</strong>
* <strong>alpha</strong>:</p>
<blockquote>
<div><p>Scaling parameter.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>beta</strong>:
The exponent.</p></li>
<li><p><strong>bias</strong>:</p></li>
<li><p><strong>size</strong> (required):
The number of channels to sum over</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions for image
case are (N x C x H x W), where N is the batch size, C is the number
of channels, and H and W are the height and the width of the data.
For non image case, the dimensions are in the form of (N x C x D1 x
D2 … Dn), where N is the batch size. Optionally, if dimension
denotation is in effect, the operation expects the input data tensor
to arrive with the dimension denotation of [DATA_BATCH,
DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE …].</p></li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
Output tensor, which has the shape and type as input tensor</p></li>
</ul>
<p><strong>Type Constraints</strong>
* <strong>T</strong> in (</p>
<blockquote>
<div><p>tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output  types to float tensors.</p>
</div></blockquote>
<p><strong>Examples</strong></p>
<p><strong>default</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.0002</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">nsize</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;LRN&quot;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
    <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
    <span class="n">size</span><span class="o">=</span><span class="n">nsize</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">square_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">ndindex</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
    <span class="n">square_sum</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
        <span class="n">x</span><span class="p">[</span>
            <span class="n">n</span><span class="p">,</span>
            <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">nsize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span>
                <span class="mi">5</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">nsize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">),</span>
            <span class="n">h</span><span class="p">,</span>
            <span class="n">w</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="p">((</span><span class="n">bias</span> <span class="o">+</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">nsize</span><span class="p">)</span> <span class="o">*</span> <span class="n">square_sum</span><span class="p">)</span> <span class="o">**</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_lrn&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>_default</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">nsize</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span><span class="s2">&quot;LRN&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">square_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">ndindex</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
    <span class="n">square_sum</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
        <span class="n">x</span><span class="p">[</span>
            <span class="n">n</span><span class="p">,</span>
            <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="n">nsize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span>
                <span class="mi">5</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">nsize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">),</span>
            <span class="n">h</span><span class="p">,</span>
            <span class="n">w</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="p">((</span><span class="n">bias</span> <span class="o">+</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">nsize</span><span class="p">)</span> <span class="o">*</span> <span class="n">square_sum</span><span class="p">)</span> <span class="o">**</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_lrn_default&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Differences</strong></p>
<table class="diff" id="difflib_chg_to95__top"
       cellspacing="0" cellpadding="0" rules="groups" >
    <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
    <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

    <tbody>
        <tr><td class="diff_next"><a href="#difflib_chg_to95__0">f</a></td><td class="diff_header" id="from95_1">1</td><td nowrap="nowrap">Local&nbsp;Response&nbsp;Normalization&nbsp;proposed&nbsp;in&nbsp;the&nbsp;[AlexNet&nbsp;paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</td><td class="diff_next"><a href="#difflib_chg_to95__0">f</a></td><td class="diff_header" id="to95_1">1</td><td nowrap="nowrap">Local&nbsp;Response&nbsp;Normalization&nbsp;proposed&nbsp;in&nbsp;the&nbsp;[AlexNet&nbsp;paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_2">2</td><td nowrap="nowrap">It&nbsp;normalizes&nbsp;over&nbsp;local&nbsp;input&nbsp;regions.</td><td class="diff_next"></td><td class="diff_header" id="to95_2">2</td><td nowrap="nowrap">It&nbsp;normalizes&nbsp;over&nbsp;local&nbsp;input&nbsp;regions.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_3">3</td><td nowrap="nowrap">The&nbsp;local&nbsp;region&nbsp;is&nbsp;defined&nbsp;across&nbsp;the&nbsp;channels.&nbsp;For&nbsp;an&nbsp;element&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;in&nbsp;a&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to95_3">3</td><td nowrap="nowrap">The&nbsp;local&nbsp;region&nbsp;is&nbsp;defined&nbsp;across&nbsp;the&nbsp;channels.&nbsp;For&nbsp;an&nbsp;element&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;in&nbsp;a&nbsp;tensor</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_4">4</td><td nowrap="nowrap">of&nbsp;shape&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x&nbsp;D2,&nbsp;...,&nbsp;Dk),&nbsp;its&nbsp;region&nbsp;is</td><td class="diff_next"></td><td class="diff_header" id="to95_4">4</td><td nowrap="nowrap">of&nbsp;shape&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x&nbsp;D2,&nbsp;...,&nbsp;Dk),&nbsp;its&nbsp;region&nbsp;is</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_5">5</td><td nowrap="nowrap">{X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;|&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))}.</td><td class="diff_next"></td><td class="diff_header" id="to95_5">5</td><td nowrap="nowrap">{X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;|&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))}.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_6">6</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_6">6</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_7">7</td><td nowrap="nowrap">square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;sum(X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;^&nbsp;2),</td><td class="diff_next"></td><td class="diff_header" id="to95_7">7</td><td nowrap="nowrap">square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;sum(X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;^&nbsp;2),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_8">8</td><td nowrap="nowrap">where&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2)).</td><td class="diff_next"></td><td class="diff_header" id="to95_8">8</td><td nowrap="nowrap">where&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2)).</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_9">9</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_9">9</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_10">10</td><td nowrap="nowrap">Y[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;/&nbsp;(bias&nbsp;+&nbsp;alpha&nbsp;/&nbsp;size&nbsp;*&nbsp;square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;)&nbsp;^&nbsp;beta</td><td class="diff_next"></td><td class="diff_header" id="to95_10">10</td><td nowrap="nowrap">Y[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;/&nbsp;(bias&nbsp;+&nbsp;alpha&nbsp;/&nbsp;size&nbsp;*&nbsp;square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;)&nbsp;^&nbsp;beta</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_11">11</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_11">11</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_12">12</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to95_12">12</td><td nowrap="nowrap">**Attributes**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_13">13</td><td nowrap="nowrap">*&nbsp;**alpha**:</td><td class="diff_next"></td><td class="diff_header" id="to95_13">13</td><td nowrap="nowrap">*&nbsp;**alpha**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;Scaling&nbsp;parameter.</td><td class="diff_next"></td><td class="diff_header" id="to95_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;Scaling&nbsp;parameter.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_15">15</td><td nowrap="nowrap">*&nbsp;**beta**:</td><td class="diff_next"></td><td class="diff_header" id="to95_15">15</td><td nowrap="nowrap">*&nbsp;**beta**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_16">16</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;exponent.</td><td class="diff_next"></td><td class="diff_header" id="to95_16">16</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;exponent.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_17">17</td><td nowrap="nowrap">*&nbsp;**bias**:</td><td class="diff_next"></td><td class="diff_header" id="to95_17">17</td><td nowrap="nowrap">*&nbsp;**bias**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_18">18</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_18">18</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_19">19</td><td nowrap="nowrap">*&nbsp;**size**&nbsp;(required):</td><td class="diff_next"></td><td class="diff_header" id="to95_19">19</td><td nowrap="nowrap">*&nbsp;**size**&nbsp;(required):</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;channels&nbsp;to&nbsp;sum&nbsp;over</td><td class="diff_next"></td><td class="diff_header" id="to95_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;channels&nbsp;to&nbsp;sum&nbsp;over</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_21">21</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_21">21</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_22">22</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to95_22">22</td><td nowrap="nowrap">**Inputs**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_23">23</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_23">23</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_24">24</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to95_24">24</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;Input&nbsp;data&nbsp;tensor&nbsp;from&nbsp;the&nbsp;previous&nbsp;operator;&nbsp;dimensions&nbsp;for&nbsp;image</td><td class="diff_next"></td><td class="diff_header" id="to95_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;Input&nbsp;data&nbsp;tensor&nbsp;from&nbsp;the&nbsp;previous&nbsp;operator;&nbsp;dimensions&nbsp;for&nbsp;image</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_26">26</td><td nowrap="nowrap">&nbsp;&nbsp;case&nbsp;are&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;H&nbsp;x&nbsp;W),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size,&nbsp;C&nbsp;is&nbsp;the&nbsp;number</td><td class="diff_next"></td><td class="diff_header" id="to95_26">26</td><td nowrap="nowrap">&nbsp;&nbsp;case&nbsp;are&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;H&nbsp;x&nbsp;W),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size,&nbsp;C&nbsp;is&nbsp;the&nbsp;number</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_27">27</td><td nowrap="nowrap">&nbsp;&nbsp;of&nbsp;channels,&nbsp;and&nbsp;H&nbsp;and&nbsp;W&nbsp;are&nbsp;the&nbsp;height&nbsp;and&nbsp;the&nbsp;width&nbsp;of&nbsp;the&nbsp;data.</td><td class="diff_next"></td><td class="diff_header" id="to95_27">27</td><td nowrap="nowrap">&nbsp;&nbsp;of&nbsp;channels,&nbsp;and&nbsp;H&nbsp;and&nbsp;W&nbsp;are&nbsp;the&nbsp;height&nbsp;and&nbsp;the&nbsp;width&nbsp;of&nbsp;the&nbsp;data.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_28">28</td><td nowrap="nowrap">&nbsp;&nbsp;For&nbsp;non&nbsp;image&nbsp;case,&nbsp;the&nbsp;dimensions&nbsp;are&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x</td><td class="diff_next"></td><td class="diff_header" id="to95_28">28</td><td nowrap="nowrap">&nbsp;&nbsp;For&nbsp;non&nbsp;image&nbsp;case,&nbsp;the&nbsp;dimensions&nbsp;are&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;D2&nbsp;...&nbsp;Dn),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size.&nbsp;Optionally,&nbsp;if&nbsp;dimension</td><td class="diff_next"></td><td class="diff_header" id="to95_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;D2&nbsp;...&nbsp;Dn),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size.&nbsp;Optionally,&nbsp;if&nbsp;dimension</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;denotation&nbsp;is&nbsp;in&nbsp;effect,&nbsp;the&nbsp;operation&nbsp;expects&nbsp;the&nbsp;input&nbsp;data&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to95_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;denotation&nbsp;is&nbsp;in&nbsp;effect,&nbsp;the&nbsp;operation&nbsp;expects&nbsp;the&nbsp;input&nbsp;data&nbsp;tensor</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;to&nbsp;arrive&nbsp;with&nbsp;the&nbsp;dimension&nbsp;denotation&nbsp;of&nbsp;[DATA_BATCH,</td><td class="diff_next"></td><td class="diff_header" id="to95_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;to&nbsp;arrive&nbsp;with&nbsp;the&nbsp;dimension&nbsp;denotation&nbsp;of&nbsp;[DATA_BATCH,</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;DATA_CHANNEL,&nbsp;DATA_FEATURE,&nbsp;DATA_FEATURE&nbsp;...].</td><td class="diff_next"></td><td class="diff_header" id="to95_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;DATA_CHANNEL,&nbsp;DATA_FEATURE,&nbsp;DATA_FEATURE&nbsp;...].</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_33">33</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_33">33</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_34">34</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to95_34">34</td><td nowrap="nowrap">**Outputs**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_35">35</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_35">35</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next" id="difflib_chg_to95__0"></td><td class="diff_header" id="from95_36">36</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to95_36">36</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_37">37</td><td nowrap="nowrap">&nbsp;&nbsp;Output&nbsp;tensor,&nbsp;which&nbsp;has&nbsp;the&nbsp;shape&nbsp;and&nbsp;type&nbsp;as&nbsp;input&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to95_37">37</td><td nowrap="nowrap">&nbsp;&nbsp;Output&nbsp;tensor,&nbsp;which&nbsp;has&nbsp;the&nbsp;shape&nbsp;and&nbsp;type&nbsp;as&nbsp;input&nbsp;tensor</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_38">38</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_38">38</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_39">39</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to95_39">39</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_40">40</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to95_40">40</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
        <tr><td class="diff_next"><a href="#difflib_chg_to95__top">t</a></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"><a href="#difflib_chg_to95__top">t</a></td><td class="diff_header" id="to95_41">41</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;tensor(bfloat16),</span></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_41">41</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to95_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to95_43">43</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_43">43</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to95_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to95_45">45</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from95_45">45</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to95_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
    </tbody>
</table></section>
<section id="lrn-1">
<span id="l-onnx-op-lrn-1"></span><h2><a class="toc-backref" href="#id3" role="doc-backlink">LRN - 1</a><a class="headerlink" href="#lrn-1" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong>
* <strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#LRN">LRN (GitHub)</a>
* <strong>domain</strong>: <strong>main</strong>
* <strong>since_version</strong>: <strong>1</strong>
* <strong>function</strong>: False
* <strong>support_level</strong>: SupportType.COMMON
* <strong>shape inference</strong>: True</p>
<p>This version of the operator has been available
<strong>since version 1</strong>.</p>
<p><strong>Summary</strong></p>
<p>Local Response Normalization proposed in the [AlexNet paper](<a class="reference external" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>).
It normalizes over local input regions.
The local region is defined across the channels. For an element X[n, c, d1, …, dk] in a tensor
of shape (N x C x D1 x D2, …, Dk), its region is
{X[n, i, d1, …, dk] | max(0, c - floor((size - 1) / 2)) &lt;= i &lt;= min(C - 1, c + ceil((size - 1) / 2))}.</p>
<p>square_sum[n, c, d1, …, dk] = sum(X[n, i, d1, …, dk] ^ 2),
where max(0, c - floor((size - 1) / 2)) &lt;= i &lt;= min(C - 1, c + ceil((size - 1) / 2)).</p>
<p>Y[n, c, d1, …, dk] = X[n, c, d1, …, dk] / (bias + alpha / size * square_sum[n, c, d1, …, dk] ) ^ beta</p>
<p><strong>Attributes</strong>
* <strong>alpha</strong>:</p>
<blockquote>
<div><p>Scaling parameter.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>beta</strong>:
The exponent.</p></li>
<li><p><strong>bias</strong>:</p></li>
<li><p><strong>size</strong> (required):
The number of channels to sum over</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions for image
case are (N x C x H x W), where N is the batch size, C is the number
of channels, and H and W are the height and the width of the data.
For non image case, the dimensions are in the form of (N x C x D1 x
D2 … Dn), where N is the batch size. Optionally, if dimension
denotation is in effect, the operation expects the input data tensor
to arrive with the dimension denotation of [DATA_BATCH,
DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE …].</p></li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
Output tensor, which has the shape and type as input tensor</p></li>
</ul>
<p><strong>Type Constraints</strong>
* <strong>T</strong> in (</p>
<blockquote>
<div><p>tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output  types to float tensors.</p>
</div></blockquote>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2022.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.1.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>