<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>ReduceLogSumExp &#8212; ONNX 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sample.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ReduceMax" href="onnx__ReduceMax.html" />
    <link rel="prev" title="ReduceLogSum" href="onnx__ReduceLogSum.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          ONNX Docs</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../onnx-api/index.html">API Overview</a></li>
                <li><a href="../operators/index.html">Op Schemas</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><p class="caption" role="heading"><span class="caption-text">API Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.checker.html">onnx.checker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.compose.html">onnx.compose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.external_data_helper.html">onnx.external_data_helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.helper.html">onnx.helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.hub.html">onnx.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.numpy_helper.html">onnx.numpy_helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.parser.html">onnx.parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.utils.html">onnx.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.version.version.html">onnx.version.version</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.version_converter.html">onnx.version_converter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Operators + OpSchemas</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">ONNX operators</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">ReduceLogSumExp</a><ul>
<li><a class="reference internal" href="#reducelogsumexp-13">ReduceLogSumExp - 13</a></li>
<li><a class="reference internal" href="#reducelogsumexp-11">ReduceLogSumExp - 11</a></li>
<li><a class="reference internal" href="#reducelogsumexp-1">ReduceLogSumExp - 1</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="onnx__ReduceLogSum.html" title="Previous Chapter: ReduceLogSum"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; ReduceLogSum</span>
    </a>
  </li>
  <li>
    <a href="onnx__ReduceMax.html" title="Next Chapter: ReduceMax"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">ReduceMax &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="../_sources/onnx_doc_folder/onnx__ReduceLogSumExp.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="reducelogsumexp">
<span id="l-onnx-doc-reducelogsumexp"></span><h1>ReduceLogSumExp<a class="headerlink" href="#reducelogsumexp" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#reducelogsumexp-13" id="id3">ReduceLogSumExp - 13</a></p></li>
<li><p><a class="reference internal" href="#reducelogsumexp-11" id="id4">ReduceLogSumExp - 11</a></p></li>
<li><p><a class="reference internal" href="#reducelogsumexp-1" id="id5">ReduceLogSumExp - 1</a></p></li>
</ul>
</nav>
<section id="reducelogsumexp-13">
<span id="l-onnx-op-reducelogsumexp-13"></span><h2><a class="toc-backref" href="#id3" role="doc-backlink">ReduceLogSumExp - 13</a><a class="headerlink" href="#reducelogsumexp-13" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong>
* <strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#ReduceLogSumExp">ReduceLogSumExp (GitHub)</a>
* <strong>domain</strong>: <strong>main</strong>
* <strong>since_version</strong>: <strong>13</strong>
* <strong>function</strong>: False
* <strong>support_level</strong>: SupportType.COMMON
* <strong>shape inference</strong>: True</p>
<p>This version of the operator has been available
<strong>since version 13</strong>.</p>
<p><strong>Summary</strong></p>
<p>Computes the log sum exponent of the input tensor’s element along the provided axes. The resulting
tensor has the same rank as the input if keepdims equals 1. If keepdims equals 0, then
the resulting tensor has the reduced dimension pruned.</p>
<p>The above behavior is similar to numpy, with the exception that numpy defaults keepdims to
False instead of True.</p>
<p><strong>Attributes</strong>
* <strong>axes</strong>:</p>
<blockquote>
<div><p>A list of integers, along which to reduce. The default is to reduce
over all the dimensions of the input tensor. Accepted range is [-r,
r-1] where r = rank(data).</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>keepdims</strong>:
Keep the reduced dimension or not, default 1 means keep reduced
dimension.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>data</strong> (heterogeneous) - <strong>T</strong>:
An input tensor.</p></li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li><p><strong>reduced</strong> (heterogeneous) - <strong>T</strong>:
Reduced output tensor.</p></li>
</ul>
<p><strong>Type Constraints</strong>
* <strong>T</strong> in (</p>
<blockquote>
<div><p>tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16),
tensor(int32),
tensor(int64),
tensor(uint32),
tensor(uint64)
):
Constrain input and output types to high-precision numeric tensors.</p>
</div></blockquote>
<p><strong>Examples</strong></p>
<p><strong>_do_not_keepdims</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">keepdims</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;ReduceLogSumExp&quot;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reduced&quot;</span><span class="p">],</span>
    <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
    <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span>
<span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># print(reduced)</span>
<span class="c1"># [[20., 2.31326175]</span>
<span class="c1"># [40.00004578, 2.31326175]</span>
<span class="c1"># [60.00671387, 2.31326175]]</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_do_not_keepdims_example&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_do_not_keepdims_random&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>_keepdims</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">keepdims</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;ReduceLogSumExp&quot;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reduced&quot;</span><span class="p">],</span>
    <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
    <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span>
<span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># print(reduced)</span>
<span class="c1"># [[[20., 2.31326175]]</span>
<span class="c1"># [[40.00004578, 2.31326175]]</span>
<span class="c1"># [[60.00671387, 2.31326175]]]</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_keepdims_example&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_keepdims_random&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>_default_axes_keepdims</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">axes</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">keepdims</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;ReduceLogSumExp&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reduced&quot;</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span>
<span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># print(reduced)</span>
<span class="c1"># [[[60.00671387]]]</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_default_axes_keepdims_example&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_default_axes_keepdims_random&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>_negative_axes_keepdims</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
<span class="n">keepdims</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;ReduceLogSumExp&quot;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;reduced&quot;</span><span class="p">],</span>
    <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
    <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span>
<span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># print(reduced)</span>
<span class="c1"># [[[20., 2.31326175]]</span>
<span class="c1"># [[40.00004578, 2.31326175]]</span>
<span class="c1"># [[60.00671387, 2.31326175]]]</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_negative_axes_keepdims_example&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">data</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">reduced</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_reduce_log_sum_exp_negative_axes_keepdims_random&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Differences</strong></p>
<table class="diff" id="difflib_chg_to169__top"
       cellspacing="0" cellpadding="0" rules="groups" >
    <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
    <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

    <tbody>
        <tr><td class="diff_next" id="difflib_chg_to169__0"><a href="#difflib_chg_to169__0">f</a></td><td class="diff_header" id="from169_1">1</td><td nowrap="nowrap">Computes&nbsp;the&nbsp;log&nbsp;sum&nbsp;exponent&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor's&nbsp;element&nbsp;along&nbsp;the&nbsp;provided&nbsp;axes.&nbsp;The&nbsp;resulting</td><td class="diff_next"><a href="#difflib_chg_to169__0">f</a></td><td class="diff_header" id="to169_1">1</td><td nowrap="nowrap">Computes&nbsp;the&nbsp;log&nbsp;sum&nbsp;exponent&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor's&nbsp;element&nbsp;along&nbsp;the&nbsp;provided&nbsp;axes.&nbsp;The&nbsp;resulting</td></tr>
        <tr><td class="diff_next"><a href="#difflib_chg_to169__1">n</a></td><td class="diff_header" id="from169_2">2</td><td nowrap="nowrap">tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;rank&nbsp;as&nbsp;the&nbsp;input&nbsp;if&nbsp;keepdims&nbsp;equals&nbsp;1.&nbsp;If&nbsp;keepdims&nbsp;equal&nbsp;0,&nbsp;then</td><td class="diff_next"><a href="#difflib_chg_to169__1">n</a></td><td class="diff_header" id="to169_2">2</td><td nowrap="nowrap">tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;rank&nbsp;as&nbsp;the&nbsp;input&nbsp;if&nbsp;keepdims&nbsp;equals&nbsp;1.&nbsp;If&nbsp;keepdims&nbsp;equal<span class="diff_add">s</span>&nbsp;0,&nbsp;then</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_3">3</td><td nowrap="nowrap">the&nbsp;result<span class="diff_chg">ed</span>&nbsp;tensor&nbsp;ha<span class="diff_chg">ve</span>&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;pruned.</td><td class="diff_next"></td><td class="diff_header" id="to169_3">3</td><td nowrap="nowrap">the&nbsp;result<span class="diff_chg">ing</span>&nbsp;tensor&nbsp;ha<span class="diff_chg">s</span>&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;pruned.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_4">4</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_4">4</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_5">5</td><td nowrap="nowrap">The&nbsp;above&nbsp;behavior&nbsp;is&nbsp;similar&nbsp;to&nbsp;numpy,&nbsp;with&nbsp;the&nbsp;exception&nbsp;that&nbsp;numpy&nbsp;defaults&nbsp;keepdims&nbsp;to</td><td class="diff_next"></td><td class="diff_header" id="to169_5">5</td><td nowrap="nowrap">The&nbsp;above&nbsp;behavior&nbsp;is&nbsp;similar&nbsp;to&nbsp;numpy,&nbsp;with&nbsp;the&nbsp;exception&nbsp;that&nbsp;numpy&nbsp;defaults&nbsp;keepdims&nbsp;to</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_6">6</td><td nowrap="nowrap">False&nbsp;instead&nbsp;of&nbsp;True.</td><td class="diff_next"></td><td class="diff_header" id="to169_6">6</td><td nowrap="nowrap">False&nbsp;instead&nbsp;of&nbsp;True.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_7">7</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_7">7</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_8">8</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to169_8">8</td><td nowrap="nowrap">**Attributes**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_9">9</td><td nowrap="nowrap">*&nbsp;**axes**:</td><td class="diff_next"></td><td class="diff_header" id="to169_9">9</td><td nowrap="nowrap">*&nbsp;**axes**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_10">10</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;integers,&nbsp;along&nbsp;which&nbsp;to&nbsp;reduce.&nbsp;The&nbsp;default&nbsp;is&nbsp;to&nbsp;reduce</td><td class="diff_next"></td><td class="diff_header" id="to169_10">10</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;integers,&nbsp;along&nbsp;which&nbsp;to&nbsp;reduce.&nbsp;The&nbsp;default&nbsp;is&nbsp;to&nbsp;reduce</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_11">11</td><td nowrap="nowrap">&nbsp;&nbsp;over&nbsp;all&nbsp;the&nbsp;dimensions&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor.&nbsp;Accepted&nbsp;range&nbsp;is&nbsp;[-r,</td><td class="diff_next"></td><td class="diff_header" id="to169_11">11</td><td nowrap="nowrap">&nbsp;&nbsp;over&nbsp;all&nbsp;the&nbsp;dimensions&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor.&nbsp;Accepted&nbsp;range&nbsp;is&nbsp;[-r,</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_12">12</td><td nowrap="nowrap">&nbsp;&nbsp;r-1]&nbsp;where&nbsp;r&nbsp;=&nbsp;rank(data).</td><td class="diff_next"></td><td class="diff_header" id="to169_12">12</td><td nowrap="nowrap">&nbsp;&nbsp;r-1]&nbsp;where&nbsp;r&nbsp;=&nbsp;rank(data).</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_13">13</td><td nowrap="nowrap">*&nbsp;**keepdims**:</td><td class="diff_next"></td><td class="diff_header" id="to169_13">13</td><td nowrap="nowrap">*&nbsp;**keepdims**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;Keep&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;or&nbsp;not,&nbsp;default&nbsp;1&nbsp;means&nbsp;keep&nbsp;reduced</td><td class="diff_next"></td><td class="diff_header" id="to169_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;Keep&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;or&nbsp;not,&nbsp;default&nbsp;1&nbsp;means&nbsp;keep&nbsp;reduced</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;dimension.</td><td class="diff_next"></td><td class="diff_header" id="to169_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;dimension.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_16">16</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_16">16</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_17">17</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to169_17">17</td><td nowrap="nowrap">**Inputs**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_18">18</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_18">18</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_19">19</td><td nowrap="nowrap">*&nbsp;**data**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to169_19">19</td><td nowrap="nowrap">*&nbsp;**data**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;An&nbsp;input&nbsp;tensor.</td><td class="diff_next"></td><td class="diff_header" id="to169_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;An&nbsp;input&nbsp;tensor.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_21">21</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_21">21</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_22">22</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to169_22">22</td><td nowrap="nowrap">**Outputs**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_23">23</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_23">23</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next" id="difflib_chg_to169__1"></td><td class="diff_header" id="from169_24">24</td><td nowrap="nowrap">*&nbsp;**reduced**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to169_24">24</td><td nowrap="nowrap">*&nbsp;**reduced**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;Reduced&nbsp;output&nbsp;tensor.</td><td class="diff_next"></td><td class="diff_header" id="to169_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;Reduced&nbsp;output&nbsp;tensor.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_26">26</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to169_26">26</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_27">27</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to169_27">27</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_28">28</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to169_28">28</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
        <tr><td class="diff_next"><a href="#difflib_chg_to169__top">t</a></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"><a href="#difflib_chg_to169__top">t</a></td><td class="diff_header" id="to169_29">29</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;tensor(bfloat16),</span></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to169_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to169_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16),</td><td class="diff_next"></td><td class="diff_header" id="to169_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32),</td><td class="diff_next"></td><td class="diff_header" id="to169_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int64),</td><td class="diff_next"></td><td class="diff_header" id="to169_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int64),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint32),</td><td class="diff_next"></td><td class="diff_header" id="to169_35">35</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint32),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_35">35</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint64)</td><td class="diff_next"></td><td class="diff_header" id="to169_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint64)</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to169_37">37</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from169_37">37</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;high-precision&nbsp;numeric&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to169_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;high-precision&nbsp;numeric&nbsp;tensors.</td></tr>
    </tbody>
</table></section>
<section id="reducelogsumexp-11">
<span id="l-onnx-op-reducelogsumexp-11"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">ReduceLogSumExp - 11</a><a class="headerlink" href="#reducelogsumexp-11" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong>
* <strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#ReduceLogSumExp">ReduceLogSumExp (GitHub)</a>
* <strong>domain</strong>: <strong>main</strong>
* <strong>since_version</strong>: <strong>11</strong>
* <strong>function</strong>: False
* <strong>support_level</strong>: SupportType.COMMON
* <strong>shape inference</strong>: True</p>
<p>This version of the operator has been available
<strong>since version 11</strong>.</p>
<p><strong>Summary</strong></p>
<p>Computes the log sum exponent of the input tensor’s element along the provided axes. The resulting
tensor has the same rank as the input if keepdims equals 1. If keepdims equal 0, then
the resulted tensor have the reduced dimension pruned.</p>
<p>The above behavior is similar to numpy, with the exception that numpy defaults keepdims to
False instead of True.</p>
<p><strong>Attributes</strong>
* <strong>axes</strong>:</p>
<blockquote>
<div><p>A list of integers, along which to reduce. The default is to reduce
over all the dimensions of the input tensor. Accepted range is [-r,
r-1] where r = rank(data).</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>keepdims</strong>:
Keep the reduced dimension or not, default 1 means keep reduced
dimension.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>data</strong> (heterogeneous) - <strong>T</strong>:
An input tensor.</p></li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li><p><strong>reduced</strong> (heterogeneous) - <strong>T</strong>:
Reduced output tensor.</p></li>
</ul>
<p><strong>Type Constraints</strong>
* <strong>T</strong> in (</p>
<blockquote>
<div><p>tensor(double),
tensor(float),
tensor(float16),
tensor(int32),
tensor(int64),
tensor(uint32),
tensor(uint64)
):
Constrain input and output types to high-precision numeric tensors.</p>
</div></blockquote>
<p><strong>Differences</strong></p>
<table class="diff" id="difflib_chg_to170__top"
       cellspacing="0" cellpadding="0" rules="groups" >
    <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
    <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

    <tbody>
        <tr><td class="diff_next"><a href="#difflib_chg_to170__0">f</a></td><td class="diff_header" id="from170_1">1</td><td nowrap="nowrap">Computes&nbsp;the&nbsp;log&nbsp;sum&nbsp;exponent&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor's&nbsp;element&nbsp;along&nbsp;the&nbsp;provided&nbsp;axes.&nbsp;The&nbsp;resulting</td><td class="diff_next"><a href="#difflib_chg_to170__0">f</a></td><td class="diff_header" id="to170_1">1</td><td nowrap="nowrap">Computes&nbsp;the&nbsp;log&nbsp;sum&nbsp;exponent&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor's&nbsp;element&nbsp;along&nbsp;the&nbsp;provided&nbsp;axes.&nbsp;The&nbsp;resulting</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_2">2</td><td nowrap="nowrap">tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;rank&nbsp;as&nbsp;the&nbsp;input&nbsp;if&nbsp;keepdims&nbsp;equals&nbsp;1.&nbsp;If&nbsp;keepdims&nbsp;equal&nbsp;0,&nbsp;then</td><td class="diff_next"></td><td class="diff_header" id="to170_2">2</td><td nowrap="nowrap">tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;rank&nbsp;as&nbsp;the&nbsp;input&nbsp;if&nbsp;keepdims&nbsp;equals&nbsp;1.&nbsp;If&nbsp;keepdims&nbsp;equal&nbsp;0,&nbsp;then</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_3">3</td><td nowrap="nowrap">the&nbsp;resulted&nbsp;tensor&nbsp;have&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;pruned.</td><td class="diff_next"></td><td class="diff_header" id="to170_3">3</td><td nowrap="nowrap">the&nbsp;resulted&nbsp;tensor&nbsp;have&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;pruned.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_4">4</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_4">4</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_5">5</td><td nowrap="nowrap">The&nbsp;above&nbsp;behavior&nbsp;is&nbsp;similar&nbsp;to&nbsp;numpy,&nbsp;with&nbsp;the&nbsp;exception&nbsp;that&nbsp;numpy&nbsp;defaults&nbsp;keepdims&nbsp;to</td><td class="diff_next"></td><td class="diff_header" id="to170_5">5</td><td nowrap="nowrap">The&nbsp;above&nbsp;behavior&nbsp;is&nbsp;similar&nbsp;to&nbsp;numpy,&nbsp;with&nbsp;the&nbsp;exception&nbsp;that&nbsp;numpy&nbsp;defaults&nbsp;keepdims&nbsp;to</td></tr>
        <tr><td class="diff_next" id="difflib_chg_to170__0"></td><td class="diff_header" id="from170_6">6</td><td nowrap="nowrap">False&nbsp;instead&nbsp;of&nbsp;True.</td><td class="diff_next"></td><td class="diff_header" id="to170_6">6</td><td nowrap="nowrap">False&nbsp;instead&nbsp;of&nbsp;True.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_7">7</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_7">7</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_8">8</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to170_8">8</td><td nowrap="nowrap">**Attributes**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_9">9</td><td nowrap="nowrap">*&nbsp;**axes**:</td><td class="diff_next"></td><td class="diff_header" id="to170_9">9</td><td nowrap="nowrap">*&nbsp;**axes**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_10">10</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;integers,&nbsp;along&nbsp;which&nbsp;to&nbsp;reduce.&nbsp;The&nbsp;default&nbsp;is&nbsp;to&nbsp;reduce</td><td class="diff_next"></td><td class="diff_header" id="to170_10">10</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;integers,&nbsp;along&nbsp;which&nbsp;to&nbsp;reduce.&nbsp;The&nbsp;default&nbsp;is&nbsp;to&nbsp;reduce</td></tr>
        <tr><td class="diff_next"><a href="#difflib_chg_to170__top">t</a></td><td class="diff_header" id="from170_11">11</td><td nowrap="nowrap">&nbsp;&nbsp;over&nbsp;all&nbsp;the&nbsp;dimensions&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor.</td><td class="diff_next"><a href="#difflib_chg_to170__top">t</a></td><td class="diff_header" id="to170_11">11</td><td nowrap="nowrap">&nbsp;&nbsp;over&nbsp;all&nbsp;the&nbsp;dimensions&nbsp;of&nbsp;the&nbsp;input&nbsp;tensor.<span class="diff_add">&nbsp;Accepted&nbsp;range&nbsp;is&nbsp;[-r,</span></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_12">12</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;r-1]&nbsp;where&nbsp;r&nbsp;=&nbsp;rank(data).</span></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_12">12</td><td nowrap="nowrap">*&nbsp;**keepdims**:</td><td class="diff_next"></td><td class="diff_header" id="to170_13">13</td><td nowrap="nowrap">*&nbsp;**keepdims**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_13">13</td><td nowrap="nowrap">&nbsp;&nbsp;Keep&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;or&nbsp;not,&nbsp;default&nbsp;1&nbsp;means&nbsp;keep&nbsp;reduced</td><td class="diff_next"></td><td class="diff_header" id="to170_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;Keep&nbsp;the&nbsp;reduced&nbsp;dimension&nbsp;or&nbsp;not,&nbsp;default&nbsp;1&nbsp;means&nbsp;keep&nbsp;reduced</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;dimension.</td><td class="diff_next"></td><td class="diff_header" id="to170_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;dimension.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_15">15</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_16">16</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_16">16</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to170_17">17</td><td nowrap="nowrap">**Inputs**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_17">17</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_18">18</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_18">18</td><td nowrap="nowrap">*&nbsp;**data**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to170_19">19</td><td nowrap="nowrap">*&nbsp;**data**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_19">19</td><td nowrap="nowrap">&nbsp;&nbsp;An&nbsp;input&nbsp;tensor.</td><td class="diff_next"></td><td class="diff_header" id="to170_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;An&nbsp;input&nbsp;tensor.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_20">20</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_21">21</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_21">21</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to170_22">22</td><td nowrap="nowrap">**Outputs**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_22">22</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_23">23</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_23">23</td><td nowrap="nowrap">*&nbsp;**reduced**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to170_24">24</td><td nowrap="nowrap">*&nbsp;**reduced**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_24">24</td><td nowrap="nowrap">&nbsp;&nbsp;Reduced&nbsp;output&nbsp;tensor.</td><td class="diff_next"></td><td class="diff_header" id="to170_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;Reduced&nbsp;output&nbsp;tensor.</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_25">25</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to170_26">26</td><td nowrap="nowrap"></td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_26">26</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to170_27">27</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_27">27</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to170_28">28</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_28">28</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to170_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to170_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16),</td><td class="diff_next"></td><td class="diff_header" id="to170_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32),</td><td class="diff_next"></td><td class="diff_header" id="to170_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int64),</td><td class="diff_next"></td><td class="diff_header" id="to170_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int64),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint32),</td><td class="diff_next"></td><td class="diff_header" id="to170_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint32),</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint64)</td><td class="diff_next"></td><td class="diff_header" id="to170_35">35</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(uint64)</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_35">35</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to170_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
        <tr><td class="diff_next"></td><td class="diff_header" id="from170_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;high-precision&nbsp;numeric&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to170_37">37</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;high-precision&nbsp;numeric&nbsp;tensors.</td></tr>
    </tbody>
</table></section>
<section id="reducelogsumexp-1">
<span id="l-onnx-op-reducelogsumexp-1"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">ReduceLogSumExp - 1</a><a class="headerlink" href="#reducelogsumexp-1" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong>
* <strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#ReduceLogSumExp">ReduceLogSumExp (GitHub)</a>
* <strong>domain</strong>: <strong>main</strong>
* <strong>since_version</strong>: <strong>1</strong>
* <strong>function</strong>: False
* <strong>support_level</strong>: SupportType.COMMON
* <strong>shape inference</strong>: True</p>
<p>This version of the operator has been available
<strong>since version 1</strong>.</p>
<p><strong>Summary</strong></p>
<p>Computes the log sum exponent of the input tensor’s element along the provided axes. The resulting
tensor has the same rank as the input if keepdims equals 1. If keepdims equal 0, then
the resulted tensor have the reduced dimension pruned.</p>
<p>The above behavior is similar to numpy, with the exception that numpy defaults keepdims to
False instead of True.</p>
<p><strong>Attributes</strong>
* <strong>axes</strong>:</p>
<blockquote>
<div><p>A list of integers, along which to reduce. The default is to reduce
over all the dimensions of the input tensor.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>keepdims</strong>:
Keep the reduced dimension or not, default 1 means keep reduced
dimension.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>data</strong> (heterogeneous) - <strong>T</strong>:
An input tensor.</p></li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li><p><strong>reduced</strong> (heterogeneous) - <strong>T</strong>:
Reduced output tensor.</p></li>
</ul>
<p><strong>Type Constraints</strong>
* <strong>T</strong> in (</p>
<blockquote>
<div><p>tensor(double),
tensor(float),
tensor(float16),
tensor(int32),
tensor(int64),
tensor(uint32),
tensor(uint64)
):
Constrain input and output types to high-precision numeric tensors.</p>
</div></blockquote>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2022.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.1.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>