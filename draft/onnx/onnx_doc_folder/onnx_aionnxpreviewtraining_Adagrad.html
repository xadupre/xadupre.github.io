<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>ai.onnx.preview.training - Adagrad &#8212; ONNX 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sample.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ai.onnx.preview.training - Adam" href="onnx_aionnxpreviewtraining_Adam.html" />
    <link rel="prev" title="ai.onnx.ml - ZipMap" href="onnx_aionnxml_ZipMap.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          ONNX Docs</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../onnx-api/index.html">API Overview</a></li>
                <li><a href="../operators/index.html">Op Schemas</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><p class="caption" role="heading"><span class="caption-text">API Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.checker.html">onnx.checker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.compose.html">onnx.compose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.external_data_helper.html">onnx.external_data_helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.helper.html">onnx.helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.hub.html">onnx.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.numpy_helper.html">onnx.numpy_helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.parser.html">onnx.parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.utils.html">onnx.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.version.version.html">onnx.version.version</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx-api/modules/onnx.version_converter.html">onnx.version_converter</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Operators + OpSchemas</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">ONNX operators</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">ai.onnx.preview.training - Adagrad</a><ul>
<li><a class="reference internal" href="#adagrad-1-ai-onnx-preview-training">Adagrad - 1 (ai.onnx.preview.training)</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="onnx_aionnxml_ZipMap.html" title="Previous Chapter: ai.onnx.ml - ZipMap"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; ai.onnx.ml - ZipMap</span>
    </a>
  </li>
  <li>
    <a href="onnx_aionnxpreviewtraining_Adam.html" title="Next Chapter: ai.onnx.preview.training - Adam"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">ai.onnx.previ... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="../_sources/onnx_doc_folder/onnx_aionnxpreviewtraining_Adagrad.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section id="ai-onnx-preview-training-adagrad">
<span id="l-onnx-docai-onnx-preview-training-adagrad"></span><h1>ai.onnx.preview.training - Adagrad<a class="headerlink" href="#ai-onnx-preview-training-adagrad" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#adagrad-1-ai-onnx-preview-training" id="id1">Adagrad - 1 (ai.onnx.preview.training)</a></p></li>
</ul>
</nav>
<section id="adagrad-1-ai-onnx-preview-training">
<span id="l-onnx-opai-onnx-preview-training-adagrad-1"></span><h2><a class="toc-backref" href="#id1" role="doc-backlink">Adagrad - 1 (ai.onnx.preview.training)</a><a class="headerlink" href="#adagrad-1-ai-onnx-preview-training" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong>
* <strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#ai.onnx.preview.training.Adagrad">Adagrad (GitHub)</a>
* <strong>domain</strong>: <strong>ai.onnx.preview.training</strong>
* <strong>since_version</strong>: <strong>1</strong>
* <strong>function</strong>: False
* <strong>support_level</strong>: SupportType.COMMON
* <strong>shape inference</strong>: True</p>
<p>This version of the operator has been available
<strong>since version 1 of domain ai.onnx.preview.training</strong>.</p>
<p><strong>Summary</strong></p>
<p>Compute one iteration of ADAGRAD, a stochastic gradient based optimization
algorithm. This operator can conduct the optimization of multiple tensor variables.</p>
<p>Let’s define the behavior of this operator. As you can imagine, ADAGRAD requires
some parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p>The initial learning-rate “R”.</p></li>
<li><p>The update count “T”. That is, the number of training iterations conducted.</p></li>
<li><p>A L2-norm regularization coefficient “norm_coefficient”.</p></li>
<li><p>A learning-rate decay factor “decay_factor”.</p></li>
<li><p>A small constant “epsilon” to avoid dividing-by-zero.</p></li>
</ul>
</div></blockquote>
<p>At each ADAGRAD iteration, the optimized tensors are moved along a direction
computed based on their estimated gradient and accumulated squared gradient. Assume
that only a single tensor “X” is updated by this operator. We need the value of “X”,
its gradient “G”, and its accumulated squared gradient “H”. Therefore, variables in
this operator’s input list are sequentially “R”, “T”, “X”, “G”, and “H”. Other
parameters are given as attributes because they are usually constants. Also, the
corresponding output tensors are the new value of “X” (called “X_new”), and then
the new accumulated squared gradient (called “H_new”). Those outputs are computed
from the given inputs following the pseudo code below.</p>
<p>Let “+”, “-”, “*”, and “/” are all element-wise arithmetic operations with
numpy-style broadcasting support. The pseudo code to compute those outputs is:</p>
<blockquote>
<div><p>// Compute a scalar learning-rate factor. At the first update of X, T is generally
// 0 (0-based update index) or 1 (1-based update index).
r = R / (1 + T * decay_factor);</p>
<p>// Add gradient of 0.5 * norm_coefficient * ||X||_2^2, where ||X||_2 is the 2-norm.
G_regularized = norm_coefficient * X + G;</p>
<p>// Compute new accumulated squared gradient.
H_new = H + G_regularized * G_regularized;</p>
<p>// Compute the adaptive part of per-coordinate learning rate. Note that Sqrt(…)
// computes element-wise square-root.
H_adaptive = Sqrt(H_new) + epsilon</p>
<p>// Compute the new value of “X”.
X_new = X - r * G_regularized / H_adaptive;</p>
</div></blockquote>
<p>If one assign this operators to optimize multiple inputs, for example, “X_1” and “X_2”, the same
pseudo code may be extended to handle all tensors jointly. More specifically, we can view “X” as a
concatenation of “X_1” and “X_2” (of course, their gradient and accumulate gradient should
be concatenated too) and then just reuse the entire pseudo code.</p>
<p>Note that ADAGRAD was first proposed in <a class="reference external" href="http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>.
In that reference paper, this operator is a special case of the Figure 1’s composite mirror
descent update.</p>
<p><strong>Attributes</strong>
* <strong>decay_factor</strong>:</p>
<blockquote>
<div><p>The decay factor of learning rate after one update.The effective
learning rate is computed by r = R / (1 + T * decay_factor). Default
to 0 so that increasing update counts doesn’t reduce the learning
rate.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>epsilon</strong>:
Small scalar to avoid dividing by zero.</p></li>
<li><p><strong>norm_coefficient</strong>:
Regularization coefficient in 0.5 * norm_coefficient * ||X||_2^2.
Default to 0, which means no regularization.</p></li>
</ul>
<p><strong>Inputs</strong>
Between 3 and 2147483647 inputs.</p>
<ul class="simple">
<li><p><strong>R</strong> (heterogeneous) - <strong>T1</strong>:
The initial learning rate.</p></li>
<li><p><strong>T</strong> (heterogeneous) - <strong>T2</strong>:
The update count of “X”. It should be a scalar.</p></li>
<li><p><strong>inputs</strong> (variadic) - <strong>T3</strong>:
The current values of optimized tensors, followed by their
respective gradients, followed by their respective accumulated
squared gradients.For example, if two tensor “X_1” and “X_2” are
optimized, The input list would be [“X_1”, “X_2”, gradient of “X_1”,
gradient of “X_2”, accumulated squared gradient of “X_1”,
accumulated squared gradient of “X_2”].</p></li>
</ul>
<p><strong>Outputs</strong>
Between 1 and 2147483647 outputs.</p>
<ul class="simple">
<li><p><strong>outputs</strong> (variadic) - <strong>T3</strong>:
Updated values of optimized tensors, followed by their updated
values of accumulated squared gradients. For example, if two tensor
“X_1” and “X_2” are optimized, the output list would be [new value
of “X_1,” new value of “X_2” new accumulated squared gradient of
“X_1”, new accumulated squared gradient of “X_2”].</p></li>
</ul>
<p><strong>Type Constraints</strong>
* <strong>T1</strong> in (</p>
<blockquote>
<div><p>tensor(double),
tensor(float)
):
Constrain input types to float scalars.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>T2</strong> in (
tensor(int64)
):
Constrain input types to 64-bit integer scalars.</p></li>
<li><p><strong>T3</strong> in (
tensor(double),
tensor(float)
):
Constrain input and output types to float tensors.</p></li>
</ul>
<p><strong>Examples</strong></p>
<p><strong>_adagrad</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define operator attributes.</span>
<span class="n">norm_coefficient</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">decay_factor</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Create operator.</span>
<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;Adagrad&quot;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;R&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;G&quot;</span><span class="p">,</span> <span class="s2">&quot;H&quot;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X_new&quot;</span><span class="p">,</span> <span class="s2">&quot;H_new&quot;</span><span class="p">],</span>
    <span class="n">norm_coefficient</span><span class="o">=</span><span class="n">norm_coefficient</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">decay_factor</span><span class="o">=</span><span class="n">decay_factor</span><span class="p">,</span>
    <span class="n">domain</span><span class="o">=</span><span class="n">AI_ONNX_PREVIEW_TRAINING_DOMAIN</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define operator inputs.</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># scalar</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>  <span class="c1"># scalar</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Compute expected outputs of Adagrad.</span>
<span class="n">x_new</span><span class="p">,</span> <span class="n">h_new</span> <span class="o">=</span> <span class="n">apply_adagrad</span><span class="p">(</span>
    <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">norm_coefficient</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">decay_factor</span>
<span class="p">)</span>

<span class="c1"># Check results.</span>
<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_new</span><span class="p">,</span> <span class="n">h_new</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_adagrad&quot;</span><span class="p">,</span>
    <span class="n">opset_imports</span><span class="o">=</span><span class="p">[</span>
        <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_opsetid</span><span class="p">(</span><span class="n">AI_ONNX_PREVIEW_TRAINING_DOMAIN</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>_adagrad_multiple</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define operator attributes.</span>
<span class="n">norm_coefficient</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">decay_factor</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">node</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s2">&quot;Adagrad&quot;</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;R&quot;</span><span class="p">,</span> <span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="s2">&quot;G1&quot;</span><span class="p">,</span> <span class="s2">&quot;G2&quot;</span><span class="p">,</span> <span class="s2">&quot;H1&quot;</span><span class="p">,</span> <span class="s2">&quot;H2&quot;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X1_new&quot;</span><span class="p">,</span> <span class="s2">&quot;X2_new&quot;</span><span class="p">,</span> <span class="s2">&quot;H1_new&quot;</span><span class="p">,</span> <span class="s2">&quot;H2_new&quot;</span><span class="p">],</span>
    <span class="n">norm_coefficient</span><span class="o">=</span><span class="n">norm_coefficient</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">decay_factor</span><span class="o">=</span><span class="n">decay_factor</span><span class="p">,</span>
    <span class="n">domain</span><span class="o">=</span><span class="n">AI_ONNX_PREVIEW_TRAINING_DOMAIN</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define operator inputs.</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># scalar</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>  <span class="c1"># scalar</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Compute expected outputs of Adagrad.</span>
<span class="n">x1_new</span><span class="p">,</span> <span class="n">h1_new</span> <span class="o">=</span> <span class="n">apply_adagrad</span><span class="p">(</span>
    <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">norm_coefficient</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">decay_factor</span>
<span class="p">)</span>
<span class="n">x2_new</span><span class="p">,</span> <span class="n">h2_new</span> <span class="o">=</span> <span class="n">apply_adagrad</span><span class="p">(</span>
    <span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">g2</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">norm_coefficient</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">decay_factor</span>
<span class="p">)</span>

<span class="c1"># Check results.</span>
<span class="n">expect</span><span class="p">(</span>
    <span class="n">node</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">x1_new</span><span class="p">,</span> <span class="n">x2_new</span><span class="p">,</span> <span class="n">h1_new</span><span class="p">,</span> <span class="n">h2_new</span><span class="p">],</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test_adagrad_multiple&quot;</span><span class="p">,</span>
    <span class="n">opset_imports</span><span class="o">=</span><span class="p">[</span>
        <span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">make_opsetid</span><span class="p">(</span><span class="n">AI_ONNX_PREVIEW_TRAINING_DOMAIN</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2022.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.1.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>