
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>ONNX Concepts &#8212; ONNX 1.12.0 documentation</title>
  <script>
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "auto";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=9b1a4fa89bdd0e95b23b" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=9b1a4fa89bdd0e95b23b" rel="stylesheet">

  
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/6.1.2/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sample.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=9b1a4fa89bdd0e95b23b">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorial_python/concepts';</script>
    <link rel="shortcut icon" href="../_static/onnx-favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ONNX with Python" href="python.html" />
    <link rel="prev" title="Introduction to ONNX" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="auto">

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fas fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../index.html">

  
  
  
  
  
  
  

  
    <img src="../_static/ONNX_logo_main.png" class="logo__image only-light" alt="Logo image">
    <img src="../_static/ONNX_logo_main.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
    </div>

    <div id="navbar-end">
      <div class="navbar-end-item navbar-end__search-button-container">
        
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search">
  <i class="fas fa-search"></i>
</button>
      </div>
      
      <div class="navbar-end-item">
        <span class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle">
    <a class="theme-switch" data-mode="light"><i class="fas fa-sun"></i></a>
    <a class="theme-switch" data-mode="dark"><i class="far fa-moon"></i></a>
    <a class="theme-switch" data-mode="auto"><i class="fas fa-adjust"></i></a>
</span>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          <a href="https://github.com/onnx/onnx" title="GitHub" class="nav-link" rel="noopener" target="_blank"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>


  
  <div class="search-button-container--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search">
  <i class="fas fa-search"></i>
</button>
  </div>

  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <span class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle">
    <a class="theme-switch" data-mode="light"><i class="fas fa-sun"></i></a>
    <a class="theme-switch" data-mode="dark"><i class="far fa-moon"></i></a>
    <a class="theme-switch" data-mode="auto"><i class="fas fa-adjust"></i></a>
</span>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          <a href="https://github.com/onnx/onnx" title="GitHub" class="nav-link" rel="noopener" target="_blank"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
<nav class="bd-links" id="bd-docs-nav" aria-label="Section navigation">
    <p class="bd-links__title" role="heading" aria-level="1">
    </p>
    <div class="bd-toc-item navbar-nav">
      <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Introduction to ONNX
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label class="toctree-toggle" for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     ONNX Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python.html">
     ONNX with Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="converters.html">
     Converters
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_python/index.html">
   API Reference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_doc_folder/index.html">
   ONNX Operators
  </a>
 </li>
</ul>

    </div>
  </nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

      </div>
      <main class="bd-main">
        
        
        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                
            </div>
            
            
            <article class="bd-article" role="main">
              
  <section id="onnx-concepts">
<h1>ONNX Concepts<a class="headerlink" href="#onnx-concepts" title="Permalink to this heading">#</a></h1>
<p>ONNX can be compared to a programming language specialized
in mathematical functions. It defines all the necessary operations
a machine learning model needs to implement its inference function
with this langage. A linear regression could be represented
the following way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">onnx_linear_regressor</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="s2">&quot;ONNX code for a linear regression&quot;</span>
    <span class="k">return</span> <span class="n">onnx</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="n">onnx</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">),</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p id="index-0">This example is very similar to an expression a developper could
write in Python. It can be also represented as a graph which shows
step by step how to transform the features to get a prediction.
That’s why a machine learning model implemented with ONNX is often
referenced as an <strong>ONNX graph</strong>.</p>
<img alt="../_images/linreg1.png" src="../_images/linreg1.png" />
<p>ONNX aims at providing a common language any machine learning framework
can use to describe its models. The first scenario is to make it easier
to deploy a machine learning model in production. An ONNX interpretor
(or <strong>runtime</strong>) can be specifically implemented and optimized for this task
in the environment where it is deployed. With ONNX, it is possible
to build a unique process to deploy a model in production and independant
from the learning framework used to build the model.</p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#input-output-node-initializer-attributes" id="id1">Input, Output, Node, Initializer, Attributes</a></p></li>
<li><p><a class="reference internal" href="#serialization-with-protobuf" id="id2">Serialization with protobuf</a></p></li>
<li><p><a class="reference internal" href="#metadata" id="id3">Metadata</a></p></li>
<li><p><a class="reference internal" href="#list-of-available-operators-and-domains" id="id4">List of available operators and domains</a></p></li>
<li><p><a class="reference internal" href="#supported-types" id="id5">Supported Types</a></p></li>
<li><p><a class="reference internal" href="#what-is-an-opset-version" id="id6">What is an opset version?</a></p></li>
<li><p><a class="reference internal" href="#subgraphs-tests-and-loops" id="id7">Subgraphs, tests and loops</a></p></li>
<li><p><a class="reference internal" href="#extensibility" id="id8">Extensibility</a></p></li>
<li><p><a class="reference internal" href="#functions" id="id9">Functions</a></p></li>
<li><p><a class="reference internal" href="#shape-and-type-inference" id="id10">Shape (and Type) Inference</a></p></li>
<li><p><a class="reference internal" href="#tools" id="id11">Tools</a></p></li>
</ul>
</nav>
<section id="input-output-node-initializer-attributes">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Input, Output, Node, Initializer, Attributes</a><a class="headerlink" href="#input-output-node-initializer-attributes" title="Permalink to this heading">#</a></h2>
<p>Building an ONNX graph means implementing a function
with the ONNX language or more precisely the <a class="reference internal" href="../onnx_doc_folder/index.html#l-onnx-operators"><span class="std std-ref">ONNX Operators</span></a>.
A linear regression would be written this way.
The following lines do not follow python syntax.
It is just a kind of pseudo code to illustrate the model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">axc</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">onnx</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="n">axc</span>
</pre></div>
</div>
<p>This code implements a function with the signature <cite>f(x, a, c) -&gt; axc</cite>.
And <em>x</em>, <em>a</em>, <em>c</em> are the <strong>inputs</strong>, <em>axc</em> is the <strong>output</strong>.
<em>ax</em> is an intermediate result.
Inputs and outputs are changing at each inference.
<em>MatMul</em> and <em>Add</em> are the <strong>nodes</strong>. They also have inputs and outputs.
A node has also a type, one of the operators in
<a class="reference internal" href="../onnx_doc_folder/index.html#l-onnx-operators"><span class="std std-ref">ONNX Operators</span></a>. This graph was built with the example
in Section <a class="reference internal" href="python.html#l-onnx-linear-regression-onnx-api"><span class="std std-ref">A simple example: a linear regression</span></a>.</p>
<p>The graph could also have an <strong>initializer</strong>. When an input
never changes such as the coefficients of the linear regression,
it is most efficient to turn it into a constant stored in the graph.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">initializer</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">initializer</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">axc</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">onnx</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="n">axc</span>
</pre></div>
</div>
<p>Visually, this graph would look like the following image.
The right side describes operator <em>Add</em> where the second input
is defined as an initializer. This graph was obtained with this
code <a class="reference internal" href="python.html#l-onnx-linear-regression-onnx-api-init"><span class="std std-ref">Initializer, default value</span></a>.</p>
<img alt="Snapshot of Netron" src="../_images/linreg2.png" />
<p>An <strong>attribute</strong> is a fixed parameter of an operator. Operator <a class="reference internal" href="../onnx_doc_folder/onnx__Gemm.html#l-onnx-doc-gemm"><span class="std std-ref">Gemm</span></a>
has four attributes, <em>alpha</em>, <em>beta</em>, <em>transA</em>, <em>transB</em>. Unless the runtime
allows it through its API, once it has loaded the ONNX graph, these values
cannot be changed and remain frozen for all the predictions.</p>
</section>
<section id="serialization-with-protobuf">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Serialization with protobuf</a><a class="headerlink" href="#serialization-with-protobuf" title="Permalink to this heading">#</a></h2>
<p>The deployment of a machine learned model into production
usually requires to replicate the entire ecosystem used to
train the model, most of the time with a <em>docker</em>.
Once a model is converted into ONNX, the production environment
only needs a runtime to execute the graph defined with ONNX
operators. This runtime can be developped in any language
suitable for the production application, C, java, python, javascript,
C#, Webassembly, ARM…</p>
<p>But to make that happen, the ONNX graph needs to be saved.
ONNX uses <em>protobuf</em> to serialize the graph into
one single block
(see <a class="reference external" href="https://developers.google.com/protocol-buffers/docs/pythontutorial#parsing-and-serialization">Parsing and Serialization</a>). It aims at optimizing the model size
as much as possible.</p>
</section>
<section id="metadata">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Metadata</a><a class="headerlink" href="#metadata" title="Permalink to this heading">#</a></h2>
<p>Machine learned models are continuously refreshed. It is important
to keep track of the model version, the author of the model,
how it was trained. ONNX offers the possibility to store additional data
into the model itself.</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>doc_string</strong>: Human-readable documentation for this model.</dt><dd><p>Markdown is allowed.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>domain</strong>: A reverse-DNS name to indicate the model namespace or domain,</dt><dd><p>for example, ‘org.onnx’</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>metadata_props</strong>: Named metadata as dictionary <cite>map&lt;string,string&gt;</cite>,</dt><dd><p><cite>(values, keys)</cite> should be distinct.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>model_author</strong>: A comma-separated list of names,</dt><dd><p>The personal name of the author(s) of the model, and/or their organizations.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>model_license</strong>: The well-known name or URL of the license</dt><dd><p>under which the model is made available.</p>
</dd>
</dl>
</li>
<li><p><strong>model_version</strong>: The version of the model itself, encoded in an integer.</p></li>
<li><p><strong>producer_name</strong>: The name of the tool used to generate the model.</p></li>
<li><p><strong>producer_version</strong>: The version of the generating tool.</p></li>
<li><dl class="simple">
<dt><strong>training_info</strong>: An optional extension that contains</dt><dd><p>information for training (see <a class="reference internal" href="../onnx_python/classes.html#l-traininginfoproto"><span class="std std-ref">TrainingInfoProto</span></a>)</p>
</dd>
</dl>
</li>
</ul>
</section>
<section id="list-of-available-operators-and-domains">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">List of available operators and domains</a><a class="headerlink" href="#list-of-available-operators-and-domains" title="Permalink to this heading">#</a></h2>
<p>The main list is described here: <a class="reference internal" href="../onnx_doc_folder/index.html#l-onnx-operators"><span class="std std-ref">ONNX Operators</span></a>.
It merges standard matrix operators (Add, Sub, MatMul, Transpose,
Greater, IsNaN, Shape, Reshape…),
reductions (ReduceSum, ReduceMin, …)
image transformations (Conv, MaxPool, …),
deep neural networks layer (RNN, DropOut, …),
activations functions (Relu, Softmax, …).
It covers most of the operations needed to implement
inference functions from standard and deep machine learning.
ONNX does not implement every existing machine learning operator,
the list of operator would be infinite.</p>
<p>The main list of operators is identified with a domain <strong>ai.onnx</strong>.
A <strong>domain</strong> can be defined as a set of operators.
A few operators in this list are dedicated to text but they hardly cover
the needs. The main list is also missing tree based models very
popular in standard machine learning.
These are part of another domain <strong>ai.onnx.ml</strong>,
it includes tree bases models (TreeEnsmble Regressor, …),
preprocessing (OneHotEncoder, LabelEncoder, …), SVM models
(SVMRegressor, …), imputer (Imputer).</p>
<p>ONNX only defines these two domains. But the library onnx
supports any custom domains and operators
(see <a class="reference internal" href="#l-onnx-extensibility"><span class="std std-ref">Extensibility</span></a>).</p>
</section>
<section id="supported-types">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Supported Types</a><a class="headerlink" href="#supported-types" title="Permalink to this heading">#</a></h2>
<p>ONNX specifications is optimized for numerical competition with
tensors. A <em>tensor</em> is a multidimensional array. It is defined
by:</p>
<ul class="simple">
<li><p>a type: the element type, the same for all elements in the tensor</p></li>
<li><p>a shape: an array with all dimensions, this array can be empty,
a dimension can be null</p></li>
<li><p>a contiguous array: it represents all the values</p></li>
</ul>
<p>This definition do not include <em>strides</em> or the possibility to define
a view of a tensor based on an existing tensor. An ONNX tensor is a dense
full array with no stride.</p>
<section id="element-type">
<h3>Element Type<a class="headerlink" href="#element-type" title="Permalink to this heading">#</a></h3>
<p>ONNX was initially developped to help deploying deep learning model.
That’s why the specifications was initially designed for floats (32 bits).
The current version supports all common types. Dictionary
<a class="reference internal" href="../onnx_python/mapping.html#l-onnx-types-mapping"><span class="std std-ref">NP_TYPE_TO_TENSOR_TYPE</span></a> gives the correspondance between <em>ONNX</em>
and <a class="reference external" href="https://numpy.org/doc/stable/reference/index.html#module-numpy" title="(in NumPy v1.23)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">TensorProto</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;^[0-9A-Z_]+$&#39;</span><span class="p">)</span>

<span class="n">values</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">att</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">dir</span><span class="p">(</span><span class="n">TensorProto</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">att</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;DESCRIPTOR&#39;</span><span class="p">}:</span>
        <span class="k">continue</span>
    <span class="k">if</span> <span class="n">reg</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">att</span><span class="p">):</span>
        <span class="n">values</span><span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">TensorProto</span><span class="p">,</span> <span class="n">att</span><span class="p">)]</span> <span class="o">=</span> <span class="n">att</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">att</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">si</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">si</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">si</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">si</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: onnx.TensorProto.</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">si</span><span class="p">,</span> <span class="n">att</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> 0: onnx.TensorProto.UNDEFINED
 1: onnx.TensorProto.FLOAT
 2: onnx.TensorProto.UINT8
 3: onnx.TensorProto.INT8
 4: onnx.TensorProto.UINT16
 5: onnx.TensorProto.INT16
 6: onnx.TensorProto.INT32
 7: onnx.TensorProto.INT64
 8: onnx.TensorProto.STRING
 9: onnx.TensorProto.BOOL
10: onnx.TensorProto.FLOAT16
11: onnx.TensorProto.DOUBLE
12: onnx.TensorProto.UINT32
13: onnx.TensorProto.UINT64
14: onnx.TensorProto.COMPLEX64
15: onnx.TensorProto.COMPLEX128
16: onnx.TensorProto.BFLOAT16
</pre></div>
</div>
<p>ONNX is strongly typed and its definition does not support
implicit cast. It is impossible to add two tensors or matrices
with different types even if other languages does. That’s why explicit
cast must be inserted in a graph.</p>
</section>
<section id="sparse-tensor">
<h3>Sparse Tensor<a class="headerlink" href="#sparse-tensor" title="Permalink to this heading">#</a></h3>
<p>Sparse tensors are useful to represent arrays having many null coefficients.
ONNX supports 2D sparse tensor. Class <a class="reference internal" href="../onnx_python/classes.html#l-onnx-sparsetensor-proto"><span class="std std-ref">SparseTensorProto</span></a>
defines attributes <cite>dims</cite>, <cite>indices</cite> (int64) and <cite>values</cite>.</p>
</section>
<section id="other-types">
<h3>Other types<a class="headerlink" href="#other-types" title="Permalink to this heading">#</a></h3>
<p>In addition to tensors and sparse tensors, ONNX supports sequences of tensors,
map of tensors, sequences of map of tensors through types
<a class="reference internal" href="../onnx_python/classes.html#l-onnx-sequence-proto"><span class="std std-ref">SequenceProto</span></a>, <a class="reference internal" href="../onnx_python/classes.html#l-onnx-map-proto"><span class="std std-ref">MapProto</span></a>. They are rarely used.</p>
</section>
</section>
<section id="what-is-an-opset-version">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">What is an opset version?</a><a class="headerlink" href="#what-is-an-opset-version" title="Permalink to this heading">#</a></h2>
<p>The opset is mapped to the version of the <em>onnx</em> package.
It is incremented every time the minor version increases.
Every version brings updated or new operators.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onnx</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="s2">&quot; opset=&quot;</span><span class="p">,</span> <span class="n">onnx</span><span class="o">.</span><span class="n">defs</span><span class="o">.</span><span class="n">onnx_opset_version</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1.12.0  opset= 18
</pre></div>
</div>
<p>An opset is also attached to every ONNX graphs. It is a global
information. It defines the version of all operators inside the graph.
Operator <em>Add</em> was updated in version 6, 7, 13 and 14. If the
graph opset is 15, it means operator <em>Add</em> follows specifications
version 14. If the graph opset is 12, then operator <em>Add</em> follows
specifications version 7. An operator in a graph follows its most
recent definition below (or equal) the global graph opset.</p>
<p>A graph may include operators from several domains, <cite>ai.onnx</cite> and
<cite>ai.onnx.ml</cite> for example. In that case, the graph must defines a
global opset for every domain. The rule is applied to every
operators within the same domain.</p>
</section>
<section id="subgraphs-tests-and-loops">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Subgraphs, tests and loops</a><a class="headerlink" href="#subgraphs-tests-and-loops" title="Permalink to this heading">#</a></h2>
<p>ONNX implements tests and loops. They all take another ONNX
graphs as an attribute. These structures are usually slow and complex.
It is better to avoid them if possible.</p>
<section id="if">
<h3>If<a class="headerlink" href="#if" title="Permalink to this heading">#</a></h3>
<p>Operator <a class="reference internal" href="../onnx_doc_folder/onnx__If.html#l-onnx-doc-if"><span class="std std-ref">If</span></a> executes
one of the two graphs depending one the condition evaluation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>If(condition) then
    exeute this ONNX graph (`then_branch`)
else
    exeute this ONNX graph (`else_branch`)
</pre></div>
</div>
<p>Those two graphs can use any result already computed in the
graph and must produce the exact same number of outputs.
These outputs will be the output of the operator <cite>If</cite>.</p>
<img alt="../_images/dot_if.png" src="../_images/dot_if.png" />
</section>
<section id="scan">
<span id="l-operator-scan-onnx-tutorial"></span><h3>Scan<a class="headerlink" href="#scan" title="Permalink to this heading">#</a></h3>
<p>Operator <a class="reference internal" href="../onnx_doc_folder/onnx__Scan.html#l-onnx-doc-scan"><span class="std std-ref">Scan</span></a> implements a loop with a fixed number of iterations.
It loops over the rows (or any other dimension) of the inputs and concatenate
the outputs along the same axis. Let’s see an example which implements
pairwise distances: <span class="math">M(i,j) = \norm{X_i - X_j}^2</span>.</p>
<img alt="../_images/dot_scan.png" src="../_images/dot_scan.png" />
<p>This loop is efficient even if it is still slower than a custom implementation
of pairwise distances. It assumes inputs and outputs are tensors and
automatically concatenate the outputs of every iteration into single
tensors. The previous example only have one but it could have several.</p>
</section>
<section id="loop">
<h3>Loop<a class="headerlink" href="#loop" title="Permalink to this heading">#</a></h3>
<p>Operator <a class="reference internal" href="../onnx_doc_folder/onnx__Loop.html#l-onnx-doc-loop"><span class="std std-ref">Loop</span></a> implements a for and a while loop. It can do a fixed
number of iterators and/or ends when a condition is not met anymore.
Outputs are processed in two different ways. First one is similar to
loop <a class="reference internal" href="../onnx_doc_folder/onnx__Scan.html#l-onnx-doc-scan"><span class="std std-ref">Scan</span></a>, outputs are concatenate into tensors (along the first
dimension). This also means that these outputs must have compatible shapes.
Second mechanism concatenates tensors into a sequence of tensors.</p>
</section>
</section>
<section id="extensibility">
<span id="l-onnx-extensibility"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Extensibility</a><a class="headerlink" href="#extensibility" title="Permalink to this heading">#</a></h2>
<p>ONNX defines a list of operators as the standard: <a class="reference internal" href="../onnx_doc_folder/index.html#l-onnx-operators"><span class="std std-ref">ONNX Operators</span></a>.
However it is very possible
to define your own operators under this domain or a new one.
<em>onnxruntime</em> defines custom operators to improve inference.
Every node has a type, a name,
named inputs and outputs, and attributes. As long as a node is described
under these constraints, a node can be added to any ONNX graph.</p>
<p>Pairwise distances can be implemented with operator Scan.
However, a dedicated operator called CDist is proved significantly
faster, significantly enough to make the effort to implement a dedicated runtime
for it.</p>
</section>
<section id="functions">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Functions</a><a class="headerlink" href="#functions" title="Permalink to this heading">#</a></h2>
<p>Functions are one way to extend ONNX specifications. Some model requires
the same combination of operators. This can be avoid by created a function
itself defined with existing ONNX operators. Once defined, a function behaves
like any other operators. It has inputs, outputs and attributes.</p>
<p>There are two advantages of using functions. The first one is to have a
shorter code and easier to read. The second one is that any onnxruntime
can leverage that information to run predictions faster. The runtime
could have a specific implementation for a function not relying on the
implementation of the existing operators.</p>
</section>
<section id="shape-and-type-inference">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Shape (and Type) Inference</a><a class="headerlink" href="#shape-and-type-inference" title="Permalink to this heading">#</a></h2>
<p>Knowing the shapes of results is not necessary to execute an ONNX graph
but this information can be used to make it faster. If you have the following
graph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">z</span>
<span class="n">Abs</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">w</span>
</pre></div>
</div>
<p>If <em>x</em> and <em>y</em> have the same shape, then <em>z</em> and <em>w</em> also have the same
shape. Knowing that, it is possible to reuse the buffer allocated for <em>z</em>,
to compute the absolute value <em>w</em> inplace. Shape inference helps the
runtime to manage the memory and therefore to be more efficient.</p>
<p>ONNX package can compute in most of the cases the output shape
knowing the input shape for every standard operator. It cannot
obviously do that for any custom operator outside of the official
list.</p>
</section>
<section id="tools">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Tools</a><a class="headerlink" href="#tools" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://netron.app/">netron</a>
is very useful to help visualize ONNX graphs.
That’s the only one without programming. The first screenshot was
made with this tool.</p>
<img alt="../_images/linreg1.png" src="../_images/linreg1.png" />
<p><a class="reference external" href="https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/onnx2py.py">onnx2py.py</a>
creates a python file from an ONNX graph. This script can create
the same graph. It may be modified by a user to change the graph.</p>
<p><a class="reference external" href="https://github.com/zetane/viewer">zetane</a>
can load onnx model and show intermediate results
when the model is executed.</p>
</section>
</section>


            </article>
            
            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="index.html" title="previous page">
      <i class="fas fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Introduction to ONNX</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="python.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">ONNX with Python</p>
  </div>
  <i class="fas fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
          </div>
        </footer>
        
      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=9b1a4fa89bdd0e95b23b"></script>

  <footer class="bd-footer"><div class="bd-footer__inner container">
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022.<br>

</p>

  </div>
  
  <div class="footer-item">
    
<p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.2.1.<br>
</p>

  </div>
  
</div>
  </footer>
  </body>
</html>