
.. _l-onnx-doc-NegativeLogLikelihoodLoss:

=========================
NegativeLogLikelihoodLoss
=========================

.. contents::
    :local:


.. _l-onnx-op-negativeloglikelihoodloss-13:

NegativeLogLikelihoodLoss - 13
==============================

**Version**

* **name**: `NegativeLogLikelihoodLoss (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#NegativeLogLikelihoodLoss>`_
* **domain**: **main**
* **since_version**: **13**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 13**.

**Summary**

A NegativeLogLikelihoodLoss operator computes (weighted) negative log likelihood loss.
Its &#34;input&#34; tensor has the shape of (N, C, d1, d2, ..., dk) where k &gt;= 0.
The &#34;input&#34; tensor contains log-probabilities for input[n, :, d_1, d_2,..., d_k] being in a class of [0, C).
The operator&#39;s &#34;target&#34; input tensor has the shape of (N, d1, d2, ..., dk). It encodes class labels (one of C classes)
or it may contain a special value (indicated by an attribute ignore_index) for N x d1 x d2 x ... x dk samples.
The loss value for input[n, :, d_1, d_2,...d_k] being classified as class c = target[n][d_1][d_2]...[d_k] is computed as:

    loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k].

When an optional &#34;weight&#34; is provided, the sample loss is calculated as:

    loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k] * weight[c].

loss is zero for the case when target-value equals ignore_index.

    loss[n][d_1][d_2]...[d_k] = 0, when target[n][d_1][d_2]...[d_k] = ignore_index

If &#34;reduction&#34; attribute is set to &#34;none&#34;, the operator&#39;s output will be the above loss with shape (N, d1, d2, ..., dk).
If &#34;reduction&#34; attribute is set to &#34;mean&#34; (the default attribute value), the output loss is (weight) averaged:

    mean(loss), if &#34;weight&#34; is not provided,

or if weight is provided,

    sum(loss) / sum(weight[target[n][d_1][d_2]...[d_k]]]), for all samples.

If &#34;reduction&#34; attribute is set to &#34;sum&#34;, the output is a scalar:
    sum(loss).

See also https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss.

Example 1:

    // negative log likelihood loss, &#34;none&#34; reduction
    N, C, d1 = 2, 3, 2
    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],
             [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]
    target = [[2, 1], [0, 2]]

    loss = np.zeros((N, d1))
    for n in range(N):
        for d_1 in range(d1):
            c = target[n][d_1]
            loss[n][d_1] = -input[n][c][d_1]

    // print(loss)
    // [[-3. -2.]
    //  [-0. -2.]]

Example 2:

    // weighted negative log likelihood loss, sum reduction
    N, C, d1 = 2, 3, 2
    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],
            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]
    target = [[2, 1], [0, 2]]
    weight = [0.2, 0.3, 0.1]
    loss = np.zeros((N, d1))
    for n in range(N):
        for d_1 in range(d1):
            c = target[n][d_1]
            loss[n][d_1] = -input[n][c][d_1] * weight[c]

    loss = np.sum(loss)
    // print(loss)
    // -1.1

Example 3:

    // weighted negative log likelihood loss, mean reduction
    N, C, d1 = 2, 3, 2
    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],
            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]
    target = [[2, 1], [0, 2]]
    weight = [0.2, 0.3, 0.1]
    loss = np.zeros((N, d1))
    weight_total = 0
    for n in range(N):
        for d_1 in range(d1):
            c = target[n][d_1]
            loss[n][d_1] = -input[n][c][d_1] * weight[c]
            weight_total = weight_total + weight[c]

    loss = np.sum(loss) / weight_total
    // print(loss)
    // -1.57

**Attributes**

* **ignore_index**:
  Specifies a target value that is ignored and does not contribute to
  the input gradient. It&#39;s an optional value.
* **reduction**:
  Type of reduction to apply to loss: none, sum, mean (default).
  &#39;none&#39;: the output is the loss for each sample. &#39;sum&#39;: the output
  will be summed. &#39;mean&#39;: the sum of the output will be divided by the
  sum of applied weights.

**Inputs**

Between 2 and 3 inputs.

* **input** (heterogeneous) - **T**:
  Input tensor of shape (N, C) or (N, C, d1, d2, ..., dk).
* **target** (heterogeneous) - **Tind**:
  Target tensor of shape (N) or (N, d1, d2, ..., dk). Target element
  value shall be in range of [0, C). If ignore_index is specified, it
  may have a value outside [0, C) and the target values should either
  be in the range [0, C) or have the value ignore_index.
* **weight** (optional, heterogeneous) - **T**:
  Optional rescaling weight tensor. If given, it has to be a tensor of
  size C. Otherwise, it is treated as if having all ones.

**Outputs**

* **loss** (heterogeneous) - **T**:
  The negative log likelihood loss

**Type Constraints**

* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input, weight, and output types to floating-point tensors.
* **Tind** in (
  tensor(int32),
  tensor(int64)
  ):
  Constrain target to integer types

**Examples**

**_input_shape_is_NC**

::

    import numpy as np
    import onnx

    reduction = &#34;none&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C = 3, 5
    np.random.seed(0)
    input = np.random.rand(N, C).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N,)).astype(np.int64)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=None, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NC&#34;,
    )

**_input_shape_is_NCd1d2**

::

    import numpy as np
    import onnx

    reduction = &#34;none&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=None, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2&#34;,
    )

**_input_shape_is_NCd1d2_reduction_mean**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=None, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_reduction_mean&#34;,
    )

**_input_shape_is_NCd1d2_reduction_sum**

::

    import numpy as np
    import onnx

    reduction = &#34;sum&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=None, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_reduction_sum&#34;,
    )

**_input_shape_is_NCd1d2_with_weight**

::

    import numpy as np
    import onnx

    reduction = &#34;none&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_with_weight&#34;,
    )

**_input_shape_is_NCd1d2_with_weight_reduction_mean**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_with_weight_reduction_mean&#34;,
    )

**_input_shape_is_NCd1d2_with_weight_reduction_sum**

::

    import numpy as np
    import onnx

    reduction = &#34;sum&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_with_weight_reduction_sum&#34;,
    )

**_input_shape_is_NCd1d2_with_weight_reduction_sum_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;sum&#34;
    ignore_index = np.int64(0)
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)
    target[0][0][0] = np.int64(0)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_with_weight_reduction_sum_ii&#34;,
    )

**_input_shape_is_NCd1d2_no_weight_reduction_mean_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    ignore_index = np.int64(1)
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C, dim1, dim2 = 3, 5, 6, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2)).astype(np.int64)
    target[0][0][0] = np.int64(1)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2_no_weight_reduction_mean_ii&#34;,
    )

**_input_shape_is_NCd1**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, d1 = 3, 5, 2
    np.random.seed(0)
    input = np.random.rand(N, C, d1).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, d1)).astype(np.int64)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=None, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1&#34;,
    )

**_input_shape_is_NCd1_weight**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, d1 = 3, 5, 2
    np.random.seed(0)
    input = np.random.rand(N, C, d1).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, d1)).astype(np.int64)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1_weight&#34;,
    )

**_input_shape_is_NCd1_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    ignore_index = np.int64(1)
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C, d1 = 3, 5, 2
    np.random.seed(0)
    input = np.random.rand(N, C, d1).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, d1)).astype(np.int64)
    target[0][0] = np.int64(1)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=None, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1_ii&#34;,
    )

**_input_shape_is_NCd1_weight_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    ignore_index = np.int64(1)
    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C, d1 = 3, 5, 2
    np.random.seed(0)
    input = np.random.rand(N, C, d1).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, d1)).astype(np.int64)
    target[0][0] = np.int64(1)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1_weight_ii&#34;,
    )

**_input_shape_is_NCd1d2d3d4d5_mean_weight**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;

    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)
    target = np.random.randint(
        0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5)
    ).astype(np.int64)
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2d3d4d5_mean_weight&#34;,
    )

**_input_shape_is_NCd1d2d3d4d5_none_no_weight**

::

    import numpy as np
    import onnx

    reduction = &#34;none&#34;

    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
    )

    N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)
    target = np.random.randint(
        0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5)
    ).astype(np.int64)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, reduction=reduction
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2d3d4d5_none_no_weight&#34;,
    )

**_input_shape_is_NCd1_mean_weight_negative_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;mean&#34;
    ignore_index = np.int64(-1)

    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C, dim1 = 3, 5, 6
    np.random.seed(0)
    input = np.random.rand(N, C, dim1).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1)).astype(np.int64)
    target[0][0] = -1
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1_mean_weight_negative_ii&#34;,
    )

**_input_shape_is_NCd1d2d3_none_no_weight_negative_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;none&#34;
    ignore_index = np.int64(-5)

    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C, dim1, dim2, dim3 = 3, 5, 6, 6, 5
    np.random.seed(0)
    input = np.random.rand(N, C, dim1, dim2, dim3).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3)).astype(
        np.int64
    )
    target[0][0][0][0] = -5

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2d3_none_no_weight_negative_ii&#34;,
    )

**_input_shape_is_NCd1d2d3_sum_weight_high_ii**

::

    import numpy as np
    import onnx

    reduction = &#34;sum&#34;
    ignore_index = np.int64(10)

    node = onnx.helper.make_node(
        &#34;NegativeLogLikelihoodLoss&#34;,
        inputs=[&#34;input&#34;, &#34;target&#34;, &#34;weight&#34;],
        outputs=[&#34;loss&#34;],
        reduction=reduction,
        ignore_index=ignore_index,
    )

    N, C = 3, 5
    np.random.seed(0)
    input = np.random.rand(N, C).astype(np.float32)
    target = np.random.randint(0, high=C, size=(N)).astype(np.int64)
    target[0] = 10
    weight = np.random.rand(C).astype(np.float32)

    negative_log_likelihood_loss = compute_negative_log_likelihood_loss(
        input, target, weight=weight, reduction=reduction, ignore_index=ignore_index
    )

    expect(
        node,
        inputs=[input, target, weight],
        outputs=[negative_log_likelihood_loss],
        name=&#34;test_nllloss_NCd1d2d3_sum_weight_high_ii&#34;,
    )

.. toctree::

    text_diff_NegativeLogLikelihoodLoss_12_13

.. _l-onnx-op-negativeloglikelihoodloss-12:

NegativeLogLikelihoodLoss - 12
==============================

**Version**

* **name**: `NegativeLogLikelihoodLoss (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#NegativeLogLikelihoodLoss>`_
* **domain**: **main**
* **since_version**: **12**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 12**.

**Summary**

A NegativeLogLikelihoodLoss operator computes (weighted) negative log likelihood loss.
Its &#34;input&#34; tensor has the shape of (N, C, d1, d2, ..., dk) where k &gt;= 0.
The &#34;input&#34; tensor contains log-probabilities for input[n, :, d_1, d_2,..., d_k] being in a class of [0, C).
The operator&#39;s &#34;target&#34; input tensor has the shape of (N, d1, d2, ..., dk). It encodes class labels (one of C classes)
or it may contain a special value (indicated by an attribute ignore_index) for N x d1 x d2 x ... x dk samples.
The loss value for input[n, :, d_1, d_2,...d_k] being classified as class c = target[n][d_1][d_2]...[d_k] is computed as:
    loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k].
When an optional &#34;weight&#34; is provided, the sample loss is calculated as:
    loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k] * weight[c].
loss is zero for the case when target-value equals ignore_index.

    loss[n][d_1][d_2]...[d_k] = 0, when target[n][d_1][d_2]...[d_k] = ignore_index
If &#34;reduction&#34; attribute is set to &#34;none&#34;, the operator&#39;s output will be the above loss with shape (N, d1, d2, ..., dk).
If &#34;reduction&#34; attribute is set to &#34;mean&#34; (the default attribute value), the output loss is (weight) averaged:
    mean(loss), if &#34;weight&#34; is not provided,
or if weight is provided,
    sum(loss) / sum(weight[target[n][d_1][d_2]...[d_k]]]), for all samples.
If &#34;reduction&#34; attribute is set to &#34;sum&#34;, the output is a scalar:
    sum(loss).
See also https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss.
Example 1:
    // negative log likelihood loss, &#34;none&#34; reduction
    N, C, d1 = 2, 3, 2
    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],
             [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]
    target = [[2, 1], [0, 2]]
    loss = np.zeros((N, d1))
    for n in range(N):
        for d_1 in range(d1):
            c = target[n][d_1]
            loss[n][d_1] = -input[n][c][d_1]
    // print(loss)
    // [[-3. -2.]
    //  [-0. -2.]]
Example 2:
    // weighted negative log likelihood loss, sum reduction
    N, C, d1 = 2, 3, 2
    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],
            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]
    target = [[2, 1], [0, 2]]
    weight = [0.2, 0.3, 0.1]
    loss = np.zeros((N, d1))
    for n in range(N):
        for d_1 in range(d1):
            c = target[n][d_1]
            loss[n][d_1] = -input[n][c][d_1] * weight[c]
    loss = np.sum(loss)
    // print(loss)
    // -1.1
Example 3:
    // weighted negative log likelihood loss, mean reduction
    N, C, d1 = 2, 3, 2
    input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],
            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]
    target = [[2, 1], [0, 2]]
    weight = [0.2, 0.3, 0.1]
    loss = np.zeros((N, d1))
    weight_total = 0
    for n in range(N):
        for d_1 in range(d1):
            c = target[n][d_1]
            loss[n][d_1] = -input[n][c][d_1] * weight[c]
            weight_total = weight_total + weight[c]
    loss = np.sum(loss) / weight_total
    // print(loss)
    // -1.57

**Attributes**

* **ignore_index**:
  Specifies a target value that is ignored and does not contribute to
  the input gradient. It&#39;s an optional value.
* **reduction**:
  Type of reduction to apply to loss: none, sum, mean (default).
  &#39;none&#39;: the output is the loss for each sample. &#39;sum&#39;: the output
  will be summed. &#39;mean&#39;: the sum of the output will be divided by the
  sum of applied weights.

**Inputs**

Between 2 and 3 inputs.

* **input** (heterogeneous) - **T**:
  Input tensor of shape (N, C) or (N, C, d1, d2, ..., dk).
* **target** (heterogeneous) - **Tind**:
  Target tensor of shape (N) or (N, d1, d2, ..., dk). Target element
  value shall be in range of [0, C). If ignore_index is specified, it
  may have a value outside [0, C) and the target values should either
  be in the range [0, C) or have the value ignore_index.
* **weight** (optional, heterogeneous) - **T**:
  Optional rescaling weight tensor. If given, it has to be a tensor of
  size C. Otherwise, it is treated as if having all ones.

**Outputs**

* **loss** (heterogeneous) - **T**:
  The negative log likelihood loss

**Type Constraints**

* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input, weight, and output types to floating-point tensors.
* **Tind** in (
  tensor(int32),
  tensor(int64)
  ):
  Constrain target to integer types
