
.. _l-onnx-doc-SoftmaxCrossEntropyLoss:

=======================
SoftmaxCrossEntropyLoss
=======================

.. contents::
    :local:


.. _l-onnx-op-softmaxcrossentropyloss-13:
SoftmaxCrossEntropyLoss - 13
============================
**Version**
* **name**: `SoftmaxCrossEntropyLoss (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#SoftmaxCrossEntropyLoss>`_
* **domain**: **main**
* **since_version**: **13**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 13**.

**Summary**
Loss function that measures the softmax cross entropy
between 'scores' and 'labels'.
This operator first computes a loss tensor whose shape is identical to the labels input.
If the input is 2-D with shape (N, C), the loss tensor may be a N-element vector L = (l_1, l_2, ..., l_N).
If the input is N-D tensor with shape (N, C, D1, D2, ..., Dk),
the loss tensor L may have (N, D1, D2, ..., Dk) as its shape and L[i,][j_1][j_2]...[j_k] denotes a scalar element in L.
After L is available, this operator can optionally do a reduction operator.

shape(scores): (N, C) where C is the number of classes, or (N, C, D1, D2,..., Dk),
        with K >= 1 in case of K-dimensional loss.
shape(labels): (N) where each value is 0 <= labels[i] <= C-1, or (N, D1, D2,..., Dk),
        with K >= 1 in case of K-dimensional loss.

The loss for one sample, l_i, can caculated as follows:
    l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk], where i is the index of classes.
or
    l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk] * weights[c], if 'weights' is provided.

loss is zero for the case when label-value equals ignore_index.
    l[i][d1][d2]...[dk]  = 0, when labels[n][d1][d2]...[dk] = ignore_index

where:
    p = Softmax(scores)
    y = Log(p)
    c = labels[i][d1][d2]...[dk]

Finally, L is optionally reduced:
If reduction = 'none', the output is L with shape (N, D1, D2, ..., Dk).
If reduction = 'sum', the output is scalar: Sum(L).
If reduction = 'mean', the output is scalar: ReduceMean(L), or if weight is provided: ReduceSum(L) / ReduceSum(W),
where tensor W is of shape (N, D1, D2, ..., Dk) and W[n][d1][d2]...[dk] = weights[labels[i][d1][d2]...[dk]].

**Attributes**
* **ignore_index**:
  Specifies a target value that is ignored and does not contribute to
  the input gradient. It's an optional value.
* **reduction**:
  Type of reduction to apply to loss: none, sum, mean(default).
  'none': no reduction will be applied, 'sum': the output will be
  summed. 'mean': the sum of the output will be divided by the number
  of elements in the output.

**Inputs**
Between 2 and 3 inputs.

* **scores** (heterogeneous) - **T**:
  The predicted outputs with shape [batch_size, class_size], or
  [batch_size, class_size, D1, D2 , ..., Dk], where K is the number of
  dimensions.
* **labels** (heterogeneous) - **Tind**:
  The ground truth output tensor, with shape [batch_size], or
  [batch_size, D1, D2, ..., Dk], where K is the number of dimensions.
  Labels element value shall be in range of [0, C). If ignore_index is
  specified, it may have a value outside [0, C) and the label values
  should either be in the range [0, C) or have the value ignore_index.
* **weights** (optional, heterogeneous) - **T**:
  A manual rescaling weight given to each class. If given, it has to
  be a 1D Tensor assigning weight to each of the classes. Otherwise,
  it is treated as if having all ones.

**Outputs**
Between 1 and 2 outputs.

* **output** (heterogeneous) - **T**:
  Weighted loss float Tensor. If reduction is 'none', this has the
  shape of [batch_size], or [batch_size, D1, D2, ..., Dk] in case of
  K-dimensional loss. Otherwise, it is a scalar.
* **log_prob** (optional, heterogeneous) - **T**:
  Log probability tensor. If the output of softmax is prob, its value
  is log(prob).

**Type Constraints**
* **T** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.
* **Tind** in (
  tensor(int32),
  tensor(int64)
  ):
  Constrain target to integer types

**Examples**

**Differences**

.. raw:: html

        <table class="diff" id="difflib_chg_to217__top"
               cellspacing="0" cellpadding="0" rules="groups" >
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

            <tbody>
                <tr><td class="diff_next"><a href="#difflib_chg_to217__0">f</a></td><td class="diff_header" id="from217_1">1</td><td nowrap="nowrap">Loss&nbsp;function&nbsp;that&nbsp;measures&nbsp;the&nbsp;softmax&nbsp;cross&nbsp;entropy</td><td class="diff_next"><a href="#difflib_chg_to217__0">f</a></td><td class="diff_header" id="to217_1">1</td><td nowrap="nowrap">Loss&nbsp;function&nbsp;that&nbsp;measures&nbsp;the&nbsp;softmax&nbsp;cross&nbsp;entropy</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_2">2</td><td nowrap="nowrap">between&nbsp;'scores'&nbsp;and&nbsp;'labels'.</td><td class="diff_next"></td><td class="diff_header" id="to217_2">2</td><td nowrap="nowrap">between&nbsp;'scores'&nbsp;and&nbsp;'labels'.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_3">3</td><td nowrap="nowrap">This&nbsp;operator&nbsp;first&nbsp;computes&nbsp;a&nbsp;loss&nbsp;tensor&nbsp;whose&nbsp;shape&nbsp;is&nbsp;identical&nbsp;to&nbsp;the&nbsp;labels&nbsp;input.</td><td class="diff_next"></td><td class="diff_header" id="to217_3">3</td><td nowrap="nowrap">This&nbsp;operator&nbsp;first&nbsp;computes&nbsp;a&nbsp;loss&nbsp;tensor&nbsp;whose&nbsp;shape&nbsp;is&nbsp;identical&nbsp;to&nbsp;the&nbsp;labels&nbsp;input.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_4">4</td><td nowrap="nowrap">If&nbsp;the&nbsp;input&nbsp;is&nbsp;2-D&nbsp;with&nbsp;shape&nbsp;(N,&nbsp;C),&nbsp;the&nbsp;loss&nbsp;tensor&nbsp;may&nbsp;be&nbsp;a&nbsp;N-element&nbsp;vector&nbsp;L&nbsp;=&nbsp;(l_1,&nbsp;l_2,&nbsp;...,&nbsp;l_N).</td><td class="diff_next"></td><td class="diff_header" id="to217_4">4</td><td nowrap="nowrap">If&nbsp;the&nbsp;input&nbsp;is&nbsp;2-D&nbsp;with&nbsp;shape&nbsp;(N,&nbsp;C),&nbsp;the&nbsp;loss&nbsp;tensor&nbsp;may&nbsp;be&nbsp;a&nbsp;N-element&nbsp;vector&nbsp;L&nbsp;=&nbsp;(l_1,&nbsp;l_2,&nbsp;...,&nbsp;l_N).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_5">5</td><td nowrap="nowrap">If&nbsp;the&nbsp;input&nbsp;is&nbsp;N-D&nbsp;tensor&nbsp;with&nbsp;shape&nbsp;(N,&nbsp;C,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk),</td><td class="diff_next"></td><td class="diff_header" id="to217_5">5</td><td nowrap="nowrap">If&nbsp;the&nbsp;input&nbsp;is&nbsp;N-D&nbsp;tensor&nbsp;with&nbsp;shape&nbsp;(N,&nbsp;C,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_6">6</td><td nowrap="nowrap">the&nbsp;loss&nbsp;tensor&nbsp;L&nbsp;may&nbsp;have&nbsp;(N,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk)&nbsp;as&nbsp;its&nbsp;shape&nbsp;and&nbsp;L[i,][j_1][j_2]...[j_k]&nbsp;denotes&nbsp;a&nbsp;scalar&nbsp;element&nbsp;in&nbsp;L.</td><td class="diff_next"></td><td class="diff_header" id="to217_6">6</td><td nowrap="nowrap">the&nbsp;loss&nbsp;tensor&nbsp;L&nbsp;may&nbsp;have&nbsp;(N,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk)&nbsp;as&nbsp;its&nbsp;shape&nbsp;and&nbsp;L[i,][j_1][j_2]...[j_k]&nbsp;denotes&nbsp;a&nbsp;scalar&nbsp;element&nbsp;in&nbsp;L.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_7">7</td><td nowrap="nowrap">After&nbsp;L&nbsp;is&nbsp;available,&nbsp;this&nbsp;operator&nbsp;can&nbsp;optionally&nbsp;do&nbsp;a&nbsp;reduction&nbsp;operator.</td><td class="diff_next"></td><td class="diff_header" id="to217_7">7</td><td nowrap="nowrap">After&nbsp;L&nbsp;is&nbsp;available,&nbsp;this&nbsp;operator&nbsp;can&nbsp;optionally&nbsp;do&nbsp;a&nbsp;reduction&nbsp;operator.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_8">8</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_8">8</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_9">9</td><td nowrap="nowrap">shape(scores):&nbsp;(N,&nbsp;C)&nbsp;where&nbsp;C&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;classes,&nbsp;or&nbsp;(N,&nbsp;C,&nbsp;D1,&nbsp;D2,...,&nbsp;Dk),</td><td class="diff_next"></td><td class="diff_header" id="to217_9">9</td><td nowrap="nowrap">shape(scores):&nbsp;(N,&nbsp;C)&nbsp;where&nbsp;C&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;classes,&nbsp;or&nbsp;(N,&nbsp;C,&nbsp;D1,&nbsp;D2,...,&nbsp;Dk),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_10">10</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;K&nbsp;&gt;=&nbsp;1&nbsp;in&nbsp;case&nbsp;of&nbsp;K-dimensional&nbsp;loss.</td><td class="diff_next"></td><td class="diff_header" id="to217_10">10</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;K&nbsp;&gt;=&nbsp;1&nbsp;in&nbsp;case&nbsp;of&nbsp;K-dimensional&nbsp;loss.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_11">11</td><td nowrap="nowrap">shape(labels):&nbsp;(N)&nbsp;where&nbsp;each&nbsp;value&nbsp;is&nbsp;0&nbsp;&lt;=&nbsp;labels[i]&nbsp;&lt;=&nbsp;C-1,&nbsp;or&nbsp;(N,&nbsp;D1,&nbsp;D2,...,&nbsp;Dk),</td><td class="diff_next"></td><td class="diff_header" id="to217_11">11</td><td nowrap="nowrap">shape(labels):&nbsp;(N)&nbsp;where&nbsp;each&nbsp;value&nbsp;is&nbsp;0&nbsp;&lt;=&nbsp;labels[i]&nbsp;&lt;=&nbsp;C-1,&nbsp;or&nbsp;(N,&nbsp;D1,&nbsp;D2,...,&nbsp;Dk),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_12">12</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;K&nbsp;&gt;=&nbsp;1&nbsp;in&nbsp;case&nbsp;of&nbsp;K-dimensional&nbsp;loss.</td><td class="diff_next"></td><td class="diff_header" id="to217_12">12</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;K&nbsp;&gt;=&nbsp;1&nbsp;in&nbsp;case&nbsp;of&nbsp;K-dimensional&nbsp;loss.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_13">13</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_13">13</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_14">14</td><td nowrap="nowrap">The&nbsp;loss&nbsp;for&nbsp;one&nbsp;sample,&nbsp;l_i,&nbsp;can&nbsp;caculated&nbsp;as&nbsp;follows:</td><td class="diff_next"></td><td class="diff_header" id="to217_14">14</td><td nowrap="nowrap">The&nbsp;loss&nbsp;for&nbsp;one&nbsp;sample,&nbsp;l_i,&nbsp;can&nbsp;caculated&nbsp;as&nbsp;follows:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;l[i][d1][d2]...[dk]&nbsp;=&nbsp;-y[i][c][d1][d2]..[dk],&nbsp;where&nbsp;i&nbsp;is&nbsp;the&nbsp;index&nbsp;of&nbsp;classes.</td><td class="diff_next"></td><td class="diff_header" id="to217_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;l[i][d1][d2]...[dk]&nbsp;=&nbsp;-y[i][c][d1][d2]..[dk],&nbsp;where&nbsp;i&nbsp;is&nbsp;the&nbsp;index&nbsp;of&nbsp;classes.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_16">16</td><td nowrap="nowrap">or</td><td class="diff_next"></td><td class="diff_header" id="to217_16">16</td><td nowrap="nowrap">or</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_17">17</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;l[i][d1][d2]...[dk]&nbsp;=&nbsp;-y[i][c][d1][d2]..[dk]&nbsp;*&nbsp;weights[c],&nbsp;if&nbsp;'weights'&nbsp;is&nbsp;provided.</td><td class="diff_next"></td><td class="diff_header" id="to217_17">17</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;l[i][d1][d2]...[dk]&nbsp;=&nbsp;-y[i][c][d1][d2]..[dk]&nbsp;*&nbsp;weights[c],&nbsp;if&nbsp;'weights'&nbsp;is&nbsp;provided.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_18">18</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_18">18</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_19">19</td><td nowrap="nowrap">loss&nbsp;is&nbsp;zero&nbsp;for&nbsp;the&nbsp;case&nbsp;when&nbsp;label-value&nbsp;equals&nbsp;ignore_index.</td><td class="diff_next"></td><td class="diff_header" id="to217_19">19</td><td nowrap="nowrap">loss&nbsp;is&nbsp;zero&nbsp;for&nbsp;the&nbsp;case&nbsp;when&nbsp;label-value&nbsp;equals&nbsp;ignore_index.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;l[i][d1][d2]...[dk]&nbsp;&nbsp;=&nbsp;0,&nbsp;when&nbsp;labels[n][d1][d2]...[dk]&nbsp;=&nbsp;ignore_index</td><td class="diff_next"></td><td class="diff_header" id="to217_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;l[i][d1][d2]...[dk]&nbsp;&nbsp;=&nbsp;0,&nbsp;when&nbsp;labels[n][d1][d2]...[dk]&nbsp;=&nbsp;ignore_index</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_21">21</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_21">21</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_22">22</td><td nowrap="nowrap">where:</td><td class="diff_next"></td><td class="diff_header" id="to217_22">22</td><td nowrap="nowrap">where:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_23">23</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;p&nbsp;=&nbsp;Softmax(scores)</td><td class="diff_next"></td><td class="diff_header" id="to217_23">23</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;p&nbsp;=&nbsp;Softmax(scores)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_24">24</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;Log(p)</td><td class="diff_next"></td><td class="diff_header" id="to217_24">24</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;Log(p)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;c&nbsp;=&nbsp;labels[i][d1][d2]...[dk]</td><td class="diff_next"></td><td class="diff_header" id="to217_25">25</td><td nowrap="nowrap">&nbsp;&nbsp;&nbsp;&nbsp;c&nbsp;=&nbsp;labels[i][d1][d2]...[dk]</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_26">26</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_26">26</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_27">27</td><td nowrap="nowrap">Finally,&nbsp;L&nbsp;is&nbsp;optionally&nbsp;reduced:</td><td class="diff_next"></td><td class="diff_header" id="to217_27">27</td><td nowrap="nowrap">Finally,&nbsp;L&nbsp;is&nbsp;optionally&nbsp;reduced:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_28">28</td><td nowrap="nowrap">If&nbsp;reduction&nbsp;=&nbsp;'none',&nbsp;the&nbsp;output&nbsp;is&nbsp;L&nbsp;with&nbsp;shape&nbsp;(N,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk).</td><td class="diff_next"></td><td class="diff_header" id="to217_28">28</td><td nowrap="nowrap">If&nbsp;reduction&nbsp;=&nbsp;'none',&nbsp;the&nbsp;output&nbsp;is&nbsp;L&nbsp;with&nbsp;shape&nbsp;(N,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_29">29</td><td nowrap="nowrap">If&nbsp;reduction&nbsp;=&nbsp;'sum',&nbsp;the&nbsp;output&nbsp;is&nbsp;scalar:&nbsp;Sum(L).</td><td class="diff_next"></td><td class="diff_header" id="to217_29">29</td><td nowrap="nowrap">If&nbsp;reduction&nbsp;=&nbsp;'sum',&nbsp;the&nbsp;output&nbsp;is&nbsp;scalar:&nbsp;Sum(L).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_30">30</td><td nowrap="nowrap">If&nbsp;reduction&nbsp;=&nbsp;'mean',&nbsp;the&nbsp;output&nbsp;is&nbsp;scalar:&nbsp;ReduceMean(L),&nbsp;or&nbsp;if&nbsp;weight&nbsp;is&nbsp;provided:&nbsp;ReduceSum(L)&nbsp;/&nbsp;ReduceSum(W),</td><td class="diff_next"></td><td class="diff_header" id="to217_30">30</td><td nowrap="nowrap">If&nbsp;reduction&nbsp;=&nbsp;'mean',&nbsp;the&nbsp;output&nbsp;is&nbsp;scalar:&nbsp;ReduceMean(L),&nbsp;or&nbsp;if&nbsp;weight&nbsp;is&nbsp;provided:&nbsp;ReduceSum(L)&nbsp;/&nbsp;ReduceSum(W),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_31">31</td><td nowrap="nowrap">where&nbsp;tensor&nbsp;W&nbsp;is&nbsp;of&nbsp;shape&nbsp;(N,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk)&nbsp;and&nbsp;W[n][d1][d2]...[dk]&nbsp;=&nbsp;weights[labels[i][d1][d2]...[dk]].</td><td class="diff_next"></td><td class="diff_header" id="to217_31">31</td><td nowrap="nowrap">where&nbsp;tensor&nbsp;W&nbsp;is&nbsp;of&nbsp;shape&nbsp;(N,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk)&nbsp;and&nbsp;W[n][d1][d2]...[dk]&nbsp;=&nbsp;weights[labels[i][d1][d2]...[dk]].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_32">32</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_32">32</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_33">33</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to217_33">33</td><td nowrap="nowrap">**Attributes**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_34">34</td><td nowrap="nowrap">*&nbsp;**ignore_index**:</td><td class="diff_next"></td><td class="diff_header" id="to217_34">34</td><td nowrap="nowrap">*&nbsp;**ignore_index**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_35">35</td><td nowrap="nowrap">&nbsp;&nbsp;Specifies&nbsp;a&nbsp;target&nbsp;value&nbsp;that&nbsp;is&nbsp;ignored&nbsp;and&nbsp;does&nbsp;not&nbsp;contribute&nbsp;to</td><td class="diff_next"></td><td class="diff_header" id="to217_35">35</td><td nowrap="nowrap">&nbsp;&nbsp;Specifies&nbsp;a&nbsp;target&nbsp;value&nbsp;that&nbsp;is&nbsp;ignored&nbsp;and&nbsp;does&nbsp;not&nbsp;contribute&nbsp;to</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;the&nbsp;input&nbsp;gradient.&nbsp;It's&nbsp;an&nbsp;optional&nbsp;value.</td><td class="diff_next"></td><td class="diff_header" id="to217_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;the&nbsp;input&nbsp;gradient.&nbsp;It's&nbsp;an&nbsp;optional&nbsp;value.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_37">37</td><td nowrap="nowrap">*&nbsp;**reduction**:</td><td class="diff_next"></td><td class="diff_header" id="to217_37">37</td><td nowrap="nowrap">*&nbsp;**reduction**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Type&nbsp;of&nbsp;reduction&nbsp;to&nbsp;apply&nbsp;to&nbsp;loss:&nbsp;none,&nbsp;sum,&nbsp;mean(default).</td><td class="diff_next"></td><td class="diff_header" id="to217_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Type&nbsp;of&nbsp;reduction&nbsp;to&nbsp;apply&nbsp;to&nbsp;loss:&nbsp;none,&nbsp;sum,&nbsp;mean(default).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_39">39</td><td nowrap="nowrap">&nbsp;&nbsp;'none':&nbsp;no&nbsp;reduction&nbsp;will&nbsp;be&nbsp;applied,&nbsp;'sum':&nbsp;the&nbsp;output&nbsp;will&nbsp;be</td><td class="diff_next"></td><td class="diff_header" id="to217_39">39</td><td nowrap="nowrap">&nbsp;&nbsp;'none':&nbsp;no&nbsp;reduction&nbsp;will&nbsp;be&nbsp;applied,&nbsp;'sum':&nbsp;the&nbsp;output&nbsp;will&nbsp;be</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;summed.&nbsp;'mean':&nbsp;the&nbsp;sum&nbsp;of&nbsp;the&nbsp;output&nbsp;will&nbsp;be&nbsp;divided&nbsp;by&nbsp;the&nbsp;number</td><td class="diff_next"></td><td class="diff_header" id="to217_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;summed.&nbsp;'mean':&nbsp;the&nbsp;sum&nbsp;of&nbsp;the&nbsp;output&nbsp;will&nbsp;be&nbsp;divided&nbsp;by&nbsp;the&nbsp;number</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_41">41</td><td nowrap="nowrap">&nbsp;&nbsp;of&nbsp;elements&nbsp;in&nbsp;the&nbsp;output.</td><td class="diff_next"></td><td class="diff_header" id="to217_41">41</td><td nowrap="nowrap">&nbsp;&nbsp;of&nbsp;elements&nbsp;in&nbsp;the&nbsp;output.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_42">42</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_42">42</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_43">43</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to217_43">43</td><td nowrap="nowrap">**Inputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_44">44</td><td nowrap="nowrap">Between&nbsp;2&nbsp;and&nbsp;3&nbsp;inputs.</td><td class="diff_next"></td><td class="diff_header" id="to217_44">44</td><td nowrap="nowrap">Between&nbsp;2&nbsp;and&nbsp;3&nbsp;inputs.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_45">45</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_45">45</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_46">46</td><td nowrap="nowrap">*&nbsp;**scores**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to217_46">46</td><td nowrap="nowrap">*&nbsp;**scores**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_47">47</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;predicted&nbsp;outputs&nbsp;with&nbsp;shape&nbsp;[batch_size,&nbsp;class_size],&nbsp;or</td><td class="diff_next"></td><td class="diff_header" id="to217_47">47</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;predicted&nbsp;outputs&nbsp;with&nbsp;shape&nbsp;[batch_size,&nbsp;class_size],&nbsp;or</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;[batch_size,&nbsp;class_size,&nbsp;D1,&nbsp;D2&nbsp;,&nbsp;...,&nbsp;Dk],&nbsp;where&nbsp;K&nbsp;is&nbsp;the&nbsp;number&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to217_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;[batch_size,&nbsp;class_size,&nbsp;D1,&nbsp;D2&nbsp;,&nbsp;...,&nbsp;Dk],&nbsp;where&nbsp;K&nbsp;is&nbsp;the&nbsp;number&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_49">49</td><td nowrap="nowrap">&nbsp;&nbsp;dimensions.</td><td class="diff_next"></td><td class="diff_header" id="to217_49">49</td><td nowrap="nowrap">&nbsp;&nbsp;dimensions.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_50">50</td><td nowrap="nowrap">*&nbsp;**labels**&nbsp;(heterogeneous)&nbsp;-&nbsp;**Tind**:</td><td class="diff_next"></td><td class="diff_header" id="to217_50">50</td><td nowrap="nowrap">*&nbsp;**labels**&nbsp;(heterogeneous)&nbsp;-&nbsp;**Tind**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_51">51</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;ground&nbsp;truth&nbsp;output&nbsp;tensor,&nbsp;with&nbsp;shape&nbsp;[batch_size],&nbsp;or</td><td class="diff_next"></td><td class="diff_header" id="to217_51">51</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;ground&nbsp;truth&nbsp;output&nbsp;tensor,&nbsp;with&nbsp;shape&nbsp;[batch_size],&nbsp;or</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_52">52</td><td nowrap="nowrap">&nbsp;&nbsp;[batch_size,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk],&nbsp;where&nbsp;K&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;dimensions.</td><td class="diff_next"></td><td class="diff_header" id="to217_52">52</td><td nowrap="nowrap">&nbsp;&nbsp;[batch_size,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk],&nbsp;where&nbsp;K&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;dimensions.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_53">53</td><td nowrap="nowrap">&nbsp;&nbsp;Labels&nbsp;element&nbsp;value&nbsp;shall&nbsp;be&nbsp;in&nbsp;range&nbsp;of&nbsp;[0,&nbsp;C).&nbsp;If&nbsp;ignore_index&nbsp;is</td><td class="diff_next"></td><td class="diff_header" id="to217_53">53</td><td nowrap="nowrap">&nbsp;&nbsp;Labels&nbsp;element&nbsp;value&nbsp;shall&nbsp;be&nbsp;in&nbsp;range&nbsp;of&nbsp;[0,&nbsp;C).&nbsp;If&nbsp;ignore_index&nbsp;is</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_54">54</td><td nowrap="nowrap">&nbsp;&nbsp;specified,&nbsp;it&nbsp;may&nbsp;have&nbsp;a&nbsp;value&nbsp;outside&nbsp;[0,&nbsp;C)&nbsp;and&nbsp;the&nbsp;label&nbsp;values</td><td class="diff_next"></td><td class="diff_header" id="to217_54">54</td><td nowrap="nowrap">&nbsp;&nbsp;specified,&nbsp;it&nbsp;may&nbsp;have&nbsp;a&nbsp;value&nbsp;outside&nbsp;[0,&nbsp;C)&nbsp;and&nbsp;the&nbsp;label&nbsp;values</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_55">55</td><td nowrap="nowrap">&nbsp;&nbsp;should&nbsp;either&nbsp;be&nbsp;in&nbsp;the&nbsp;range&nbsp;[0,&nbsp;C)&nbsp;or&nbsp;have&nbsp;the&nbsp;value&nbsp;ignore_index.</td><td class="diff_next"></td><td class="diff_header" id="to217_55">55</td><td nowrap="nowrap">&nbsp;&nbsp;should&nbsp;either&nbsp;be&nbsp;in&nbsp;the&nbsp;range&nbsp;[0,&nbsp;C)&nbsp;or&nbsp;have&nbsp;the&nbsp;value&nbsp;ignore_index.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_56">56</td><td nowrap="nowrap">*&nbsp;**weights**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to217_56">56</td><td nowrap="nowrap">*&nbsp;**weights**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_57">57</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;manual&nbsp;rescaling&nbsp;weight&nbsp;given&nbsp;to&nbsp;each&nbsp;class.&nbsp;If&nbsp;given,&nbsp;it&nbsp;has&nbsp;to</td><td class="diff_next"></td><td class="diff_header" id="to217_57">57</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;manual&nbsp;rescaling&nbsp;weight&nbsp;given&nbsp;to&nbsp;each&nbsp;class.&nbsp;If&nbsp;given,&nbsp;it&nbsp;has&nbsp;to</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_58">58</td><td nowrap="nowrap">&nbsp;&nbsp;be&nbsp;a&nbsp;1D&nbsp;Tensor&nbsp;assigning&nbsp;weight&nbsp;to&nbsp;each&nbsp;of&nbsp;the&nbsp;classes.&nbsp;Otherwise,</td><td class="diff_next"></td><td class="diff_header" id="to217_58">58</td><td nowrap="nowrap">&nbsp;&nbsp;be&nbsp;a&nbsp;1D&nbsp;Tensor&nbsp;assigning&nbsp;weight&nbsp;to&nbsp;each&nbsp;of&nbsp;the&nbsp;classes.&nbsp;Otherwise,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_59">59</td><td nowrap="nowrap">&nbsp;&nbsp;it&nbsp;is&nbsp;treated&nbsp;as&nbsp;if&nbsp;having&nbsp;all&nbsp;ones.</td><td class="diff_next"></td><td class="diff_header" id="to217_59">59</td><td nowrap="nowrap">&nbsp;&nbsp;it&nbsp;is&nbsp;treated&nbsp;as&nbsp;if&nbsp;having&nbsp;all&nbsp;ones.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_60">60</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_60">60</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_61">61</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to217_61">61</td><td nowrap="nowrap">**Outputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_62">62</td><td nowrap="nowrap">Between&nbsp;1&nbsp;and&nbsp;2&nbsp;outputs.</td><td class="diff_next"></td><td class="diff_header" id="to217_62">62</td><td nowrap="nowrap">Between&nbsp;1&nbsp;and&nbsp;2&nbsp;outputs.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_63">63</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_63">63</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_64">64</td><td nowrap="nowrap">*&nbsp;**output**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to217_64">64</td><td nowrap="nowrap">*&nbsp;**output**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_65">65</td><td nowrap="nowrap">&nbsp;&nbsp;Weighted&nbsp;loss&nbsp;float&nbsp;Tensor.&nbsp;If&nbsp;reduction&nbsp;is&nbsp;'none',&nbsp;this&nbsp;has&nbsp;the</td><td class="diff_next"></td><td class="diff_header" id="to217_65">65</td><td nowrap="nowrap">&nbsp;&nbsp;Weighted&nbsp;loss&nbsp;float&nbsp;Tensor.&nbsp;If&nbsp;reduction&nbsp;is&nbsp;'none',&nbsp;this&nbsp;has&nbsp;the</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_66">66</td><td nowrap="nowrap">&nbsp;&nbsp;shape&nbsp;of&nbsp;[batch_size],&nbsp;or&nbsp;[batch_size,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk]&nbsp;in&nbsp;case&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to217_66">66</td><td nowrap="nowrap">&nbsp;&nbsp;shape&nbsp;of&nbsp;[batch_size],&nbsp;or&nbsp;[batch_size,&nbsp;D1,&nbsp;D2,&nbsp;...,&nbsp;Dk]&nbsp;in&nbsp;case&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_67">67</td><td nowrap="nowrap">&nbsp;&nbsp;K-dimensional&nbsp;loss.&nbsp;Otherwise,&nbsp;it&nbsp;is&nbsp;a&nbsp;scalar.</td><td class="diff_next"></td><td class="diff_header" id="to217_67">67</td><td nowrap="nowrap">&nbsp;&nbsp;K-dimensional&nbsp;loss.&nbsp;Otherwise,&nbsp;it&nbsp;is&nbsp;a&nbsp;scalar.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_68">68</td><td nowrap="nowrap">*&nbsp;**log_prob**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to217_68">68</td><td nowrap="nowrap">*&nbsp;**log_prob**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to217__0"></td><td class="diff_header" id="from217_69">69</td><td nowrap="nowrap">&nbsp;&nbsp;Log&nbsp;probability&nbsp;tensor.&nbsp;If&nbsp;the&nbsp;output&nbsp;of&nbsp;softmax&nbsp;is&nbsp;prob,&nbsp;its&nbsp;value</td><td class="diff_next"></td><td class="diff_header" id="to217_69">69</td><td nowrap="nowrap">&nbsp;&nbsp;Log&nbsp;probability&nbsp;tensor.&nbsp;If&nbsp;the&nbsp;output&nbsp;of&nbsp;softmax&nbsp;is&nbsp;prob,&nbsp;its&nbsp;value</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_70">70</td><td nowrap="nowrap">&nbsp;&nbsp;is&nbsp;log(prob).</td><td class="diff_next"></td><td class="diff_header" id="to217_70">70</td><td nowrap="nowrap">&nbsp;&nbsp;is&nbsp;log(prob).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_71">71</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to217_71">71</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_72">72</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to217_72">72</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_73">73</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to217_73">73</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to217__top">t</a></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"><a href="#difflib_chg_to217__top">t</a></td><td class="diff_header" id="to217_74">74</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;tensor(bfloat16),</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_74">74</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to217_75">75</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_75">75</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to217_76">76</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_76">76</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to217_77">77</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_77">77</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to217_78">78</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_78">78</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to217_79">79</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_79">79</td><td nowrap="nowrap">*&nbsp;**Tind**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to217_80">80</td><td nowrap="nowrap">*&nbsp;**Tind**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_80">80</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32),</td><td class="diff_next"></td><td class="diff_header" id="to217_81">81</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_81">81</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int64)</td><td class="diff_next"></td><td class="diff_header" id="to217_82">82</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int64)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_82">82</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to217_83">83</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from217_83">83</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;target&nbsp;to&nbsp;integer&nbsp;types</td><td class="diff_next"></td><td class="diff_header" id="to217_84">84</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;target&nbsp;to&nbsp;integer&nbsp;types</td></tr>
            </tbody>
        </table>

.. _l-onnx-op-softmaxcrossentropyloss-12:
SoftmaxCrossEntropyLoss - 12
============================
**Version**
* **name**: `SoftmaxCrossEntropyLoss (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#SoftmaxCrossEntropyLoss>`_
* **domain**: **main**
* **since_version**: **12**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 12**.

**Summary**
Loss function that measures the softmax cross entropy
between 'scores' and 'labels'.
This operator first computes a loss tensor whose shape is identical to the labels input.
If the input is 2-D with shape (N, C), the loss tensor may be a N-element vector L = (l_1, l_2, ..., l_N).
If the input is N-D tensor with shape (N, C, D1, D2, ..., Dk),
the loss tensor L may have (N, D1, D2, ..., Dk) as its shape and L[i,][j_1][j_2]...[j_k] denotes a scalar element in L.
After L is available, this operator can optionally do a reduction operator.

shape(scores): (N, C) where C is the number of classes, or (N, C, D1, D2,..., Dk),
        with K >= 1 in case of K-dimensional loss.
shape(labels): (N) where each value is 0 <= labels[i] <= C-1, or (N, D1, D2,..., Dk),
        with K >= 1 in case of K-dimensional loss.

The loss for one sample, l_i, can caculated as follows:
    l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk], where i is the index of classes.
or
    l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk] * weights[c], if 'weights' is provided.

loss is zero for the case when label-value equals ignore_index.
    l[i][d1][d2]...[dk]  = 0, when labels[n][d1][d2]...[dk] = ignore_index

where:
    p = Softmax(scores)
    y = Log(p)
    c = labels[i][d1][d2]...[dk]

Finally, L is optionally reduced:
If reduction = 'none', the output is L with shape (N, D1, D2, ..., Dk).
If reduction = 'sum', the output is scalar: Sum(L).
If reduction = 'mean', the output is scalar: ReduceMean(L), or if weight is provided: ReduceSum(L) / ReduceSum(W),
where tensor W is of shape (N, D1, D2, ..., Dk) and W[n][d1][d2]...[dk] = weights[labels[i][d1][d2]...[dk]].

**Attributes**
* **ignore_index**:
  Specifies a target value that is ignored and does not contribute to
  the input gradient. It's an optional value.
* **reduction**:
  Type of reduction to apply to loss: none, sum, mean(default).
  'none': no reduction will be applied, 'sum': the output will be
  summed. 'mean': the sum of the output will be divided by the number
  of elements in the output.

**Inputs**
Between 2 and 3 inputs.

* **scores** (heterogeneous) - **T**:
  The predicted outputs with shape [batch_size, class_size], or
  [batch_size, class_size, D1, D2 , ..., Dk], where K is the number of
  dimensions.
* **labels** (heterogeneous) - **Tind**:
  The ground truth output tensor, with shape [batch_size], or
  [batch_size, D1, D2, ..., Dk], where K is the number of dimensions.
  Labels element value shall be in range of [0, C). If ignore_index is
  specified, it may have a value outside [0, C) and the label values
  should either be in the range [0, C) or have the value ignore_index.
* **weights** (optional, heterogeneous) - **T**:
  A manual rescaling weight given to each class. If given, it has to
  be a 1D Tensor assigning weight to each of the classes. Otherwise,
  it is treated as if having all ones.

**Outputs**
Between 1 and 2 outputs.

* **output** (heterogeneous) - **T**:
  Weighted loss float Tensor. If reduction is 'none', this has the
  shape of [batch_size], or [batch_size, D1, D2, ..., Dk] in case of
  K-dimensional loss. Otherwise, it is a scalar.
* **log_prob** (optional, heterogeneous) - **T**:
  Log probability tensor. If the output of softmax is prob, its value
  is log(prob).

**Type Constraints**
* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.
* **Tind** in (
  tensor(int32),
  tensor(int64)
  ):
  Constrain target to integer types
