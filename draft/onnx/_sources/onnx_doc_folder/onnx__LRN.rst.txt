
.. _l-onnx-doc-LRN:

===
LRN
===

.. contents::
    :local:


.. _l-onnx-op-lrn-13:

LRN - 13
========

**Version**

* **name**: `LRN (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#LRN>`_
* **domain**: **main**
* **since_version**: **13**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 13**.

**Summary**

Local Response Normalization proposed in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).
It normalizes over local input regions.
The local region is defined across the channels. For an element X[n, c, d1, ..., dk] in a tensor
of shape (N x C x D1 x D2, ..., Dk), its region is
{X[n, i, d1, ..., dk] | max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))}.

square_sum[n, c, d1, ..., dk] = sum(X[n, i, d1, ..., dk] ^ 2),
where max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2)).

Y[n, c, d1, ..., dk] = X[n, c, d1, ..., dk] / (bias + alpha / size * square_sum[n, c, d1, ..., dk] ) ^ beta

**Attributes**

* **alpha**:
  Scaling parameter.
* **beta**:
  The exponent.
* **bias**:

* **size** (required):
  The number of channels to sum over

**Inputs**

* **X** (heterogeneous) - **T**:
  Input data tensor from the previous operator; dimensions for image
  case are (N x C x H x W), where N is the batch size, C is the number
  of channels, and H and W are the height and the width of the data.
  For non image case, the dimensions are in the form of (N x C x D1 x
  D2 ... Dn), where N is the batch size. Optionally, if dimension
  denotation is in effect, the operation expects the input data tensor
  to arrive with the dimension denotation of [DATA_BATCH,
  DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...].

**Outputs**

* **Y** (heterogeneous) - **T**:
  Output tensor, which has the shape and type as input tensor

**Type Constraints**

* **T** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output  types to float tensors.

**Examples**

**default**

::

    alpha = 0.0002
    beta = 0.5
    bias = 2.0
    nsize = 3
    node = onnx.helper.make_node(
        "LRN",
        inputs=["x"],
        outputs=["y"],
        alpha=alpha,
        beta=beta,
        bias=bias,
        size=nsize,
    )
    x = np.random.randn(5, 5, 5, 5).astype(np.float32)
    square_sum = np.zeros((5, 5, 5, 5)).astype(np.float32)
    for n, c, h, w in np.ndindex(x.shape):
        square_sum[n, c, h, w] = sum(
            x[
                n,
                max(0, c - int(math.floor((nsize - 1) / 2))) : min(
                    5, c + int(math.ceil((nsize - 1) / 2)) + 1
                ),
                h,
                w,
            ]
            ** 2
        )
    y = x / ((bias + (alpha / nsize) * square_sum) ** beta)
    expect(node, inputs=[x], outputs=[y], name="test_lrn")

**_default**

::

    alpha = 0.0001
    beta = 0.75
    bias = 1.0
    nsize = 3
    node = onnx.helper.make_node("LRN", inputs=["x"], outputs=["y"], size=3)
    x = np.random.randn(5, 5, 5, 5).astype(np.float32)
    square_sum = np.zeros((5, 5, 5, 5)).astype(np.float32)
    for n, c, h, w in np.ndindex(x.shape):
        square_sum[n, c, h, w] = sum(
            x[
                n,
                max(0, c - int(math.floor((nsize - 1) / 2))) : min(
                    5, c + int(math.ceil((nsize - 1) / 2)) + 1
                ),
                h,
                w,
            ]
            ** 2
        )
    y = x / ((bias + (alpha / nsize) * square_sum) ** beta)
    expect(node, inputs=[x], outputs=[y], name="test_lrn_default")

**Differences**

.. raw:: html

        <table class="diff" id="difflib_chg_to95__top"
               cellspacing="0" cellpadding="0" rules="groups" >
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

            <tbody>
                <tr><td class="diff_next"><a href="#difflib_chg_to95__0">f</a></td><td class="diff_header" id="from95_1">1</td><td nowrap="nowrap">Local&nbsp;Response&nbsp;Normalization&nbsp;proposed&nbsp;in&nbsp;the&nbsp;[AlexNet&nbsp;paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</td><td class="diff_next"><a href="#difflib_chg_to95__0">f</a></td><td class="diff_header" id="to95_1">1</td><td nowrap="nowrap">Local&nbsp;Response&nbsp;Normalization&nbsp;proposed&nbsp;in&nbsp;the&nbsp;[AlexNet&nbsp;paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_2">2</td><td nowrap="nowrap">It&nbsp;normalizes&nbsp;over&nbsp;local&nbsp;input&nbsp;regions.</td><td class="diff_next"></td><td class="diff_header" id="to95_2">2</td><td nowrap="nowrap">It&nbsp;normalizes&nbsp;over&nbsp;local&nbsp;input&nbsp;regions.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_3">3</td><td nowrap="nowrap">The&nbsp;local&nbsp;region&nbsp;is&nbsp;defined&nbsp;across&nbsp;the&nbsp;channels.&nbsp;For&nbsp;an&nbsp;element&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;in&nbsp;a&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to95_3">3</td><td nowrap="nowrap">The&nbsp;local&nbsp;region&nbsp;is&nbsp;defined&nbsp;across&nbsp;the&nbsp;channels.&nbsp;For&nbsp;an&nbsp;element&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;in&nbsp;a&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_4">4</td><td nowrap="nowrap">of&nbsp;shape&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x&nbsp;D2,&nbsp;...,&nbsp;Dk),&nbsp;its&nbsp;region&nbsp;is</td><td class="diff_next"></td><td class="diff_header" id="to95_4">4</td><td nowrap="nowrap">of&nbsp;shape&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x&nbsp;D2,&nbsp;...,&nbsp;Dk),&nbsp;its&nbsp;region&nbsp;is</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_5">5</td><td nowrap="nowrap">{X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;|&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))}.</td><td class="diff_next"></td><td class="diff_header" id="to95_5">5</td><td nowrap="nowrap">{X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;|&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))}.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_6">6</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_6">6</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_7">7</td><td nowrap="nowrap">square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;sum(X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;^&nbsp;2),</td><td class="diff_next"></td><td class="diff_header" id="to95_7">7</td><td nowrap="nowrap">square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;sum(X[n,&nbsp;i,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;^&nbsp;2),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_8">8</td><td nowrap="nowrap">where&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2)).</td><td class="diff_next"></td><td class="diff_header" id="to95_8">8</td><td nowrap="nowrap">where&nbsp;max(0,&nbsp;c&nbsp;-&nbsp;floor((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2))&nbsp;&lt;=&nbsp;i&nbsp;&lt;=&nbsp;min(C&nbsp;-&nbsp;1,&nbsp;c&nbsp;+&nbsp;ceil((size&nbsp;-&nbsp;1)&nbsp;/&nbsp;2)).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_9">9</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_9">9</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_10">10</td><td nowrap="nowrap">Y[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;/&nbsp;(bias&nbsp;+&nbsp;alpha&nbsp;/&nbsp;size&nbsp;*&nbsp;square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;)&nbsp;^&nbsp;beta</td><td class="diff_next"></td><td class="diff_header" id="to95_10">10</td><td nowrap="nowrap">Y[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;=&nbsp;X[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;/&nbsp;(bias&nbsp;+&nbsp;alpha&nbsp;/&nbsp;size&nbsp;*&nbsp;square_sum[n,&nbsp;c,&nbsp;d1,&nbsp;...,&nbsp;dk]&nbsp;)&nbsp;^&nbsp;beta</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_11">11</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_11">11</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_12">12</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to95_12">12</td><td nowrap="nowrap">**Attributes**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_13">13</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_13">13</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_14">14</td><td nowrap="nowrap">*&nbsp;**alpha**:</td><td class="diff_next"></td><td class="diff_header" id="to95_14">14</td><td nowrap="nowrap">*&nbsp;**alpha**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;Scaling&nbsp;parameter.</td><td class="diff_next"></td><td class="diff_header" id="to95_15">15</td><td nowrap="nowrap">&nbsp;&nbsp;Scaling&nbsp;parameter.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_16">16</td><td nowrap="nowrap">*&nbsp;**beta**:</td><td class="diff_next"></td><td class="diff_header" id="to95_16">16</td><td nowrap="nowrap">*&nbsp;**beta**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_17">17</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;exponent.</td><td class="diff_next"></td><td class="diff_header" id="to95_17">17</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;exponent.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_18">18</td><td nowrap="nowrap">*&nbsp;**bias**:</td><td class="diff_next"></td><td class="diff_header" id="to95_18">18</td><td nowrap="nowrap">*&nbsp;**bias**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_19">19</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_19">19</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_20">20</td><td nowrap="nowrap">*&nbsp;**size**&nbsp;(required):</td><td class="diff_next"></td><td class="diff_header" id="to95_20">20</td><td nowrap="nowrap">*&nbsp;**size**&nbsp;(required):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_21">21</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;channels&nbsp;to&nbsp;sum&nbsp;over</td><td class="diff_next"></td><td class="diff_header" id="to95_21">21</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;channels&nbsp;to&nbsp;sum&nbsp;over</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_22">22</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_22">22</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_23">23</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to95_23">23</td><td nowrap="nowrap">**Inputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_24">24</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_24">24</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_25">25</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to95_25">25</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_26">26</td><td nowrap="nowrap">&nbsp;&nbsp;Input&nbsp;data&nbsp;tensor&nbsp;from&nbsp;the&nbsp;previous&nbsp;operator;&nbsp;dimensions&nbsp;for&nbsp;image</td><td class="diff_next"></td><td class="diff_header" id="to95_26">26</td><td nowrap="nowrap">&nbsp;&nbsp;Input&nbsp;data&nbsp;tensor&nbsp;from&nbsp;the&nbsp;previous&nbsp;operator;&nbsp;dimensions&nbsp;for&nbsp;image</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_27">27</td><td nowrap="nowrap">&nbsp;&nbsp;case&nbsp;are&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;H&nbsp;x&nbsp;W),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size,&nbsp;C&nbsp;is&nbsp;the&nbsp;number</td><td class="diff_next"></td><td class="diff_header" id="to95_27">27</td><td nowrap="nowrap">&nbsp;&nbsp;case&nbsp;are&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;H&nbsp;x&nbsp;W),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size,&nbsp;C&nbsp;is&nbsp;the&nbsp;number</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_28">28</td><td nowrap="nowrap">&nbsp;&nbsp;of&nbsp;channels,&nbsp;and&nbsp;H&nbsp;and&nbsp;W&nbsp;are&nbsp;the&nbsp;height&nbsp;and&nbsp;the&nbsp;width&nbsp;of&nbsp;the&nbsp;data.</td><td class="diff_next"></td><td class="diff_header" id="to95_28">28</td><td nowrap="nowrap">&nbsp;&nbsp;of&nbsp;channels,&nbsp;and&nbsp;H&nbsp;and&nbsp;W&nbsp;are&nbsp;the&nbsp;height&nbsp;and&nbsp;the&nbsp;width&nbsp;of&nbsp;the&nbsp;data.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;For&nbsp;non&nbsp;image&nbsp;case,&nbsp;the&nbsp;dimensions&nbsp;are&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x</td><td class="diff_next"></td><td class="diff_header" id="to95_29">29</td><td nowrap="nowrap">&nbsp;&nbsp;For&nbsp;non&nbsp;image&nbsp;case,&nbsp;the&nbsp;dimensions&nbsp;are&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;(N&nbsp;x&nbsp;C&nbsp;x&nbsp;D1&nbsp;x</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;D2&nbsp;...&nbsp;Dn),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size.&nbsp;Optionally,&nbsp;if&nbsp;dimension</td><td class="diff_next"></td><td class="diff_header" id="to95_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;D2&nbsp;...&nbsp;Dn),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;batch&nbsp;size.&nbsp;Optionally,&nbsp;if&nbsp;dimension</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;denotation&nbsp;is&nbsp;in&nbsp;effect,&nbsp;the&nbsp;operation&nbsp;expects&nbsp;the&nbsp;input&nbsp;data&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to95_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;denotation&nbsp;is&nbsp;in&nbsp;effect,&nbsp;the&nbsp;operation&nbsp;expects&nbsp;the&nbsp;input&nbsp;data&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;to&nbsp;arrive&nbsp;with&nbsp;the&nbsp;dimension&nbsp;denotation&nbsp;of&nbsp;[DATA_BATCH,</td><td class="diff_next"></td><td class="diff_header" id="to95_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;to&nbsp;arrive&nbsp;with&nbsp;the&nbsp;dimension&nbsp;denotation&nbsp;of&nbsp;[DATA_BATCH,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;DATA_CHANNEL,&nbsp;DATA_FEATURE,&nbsp;DATA_FEATURE&nbsp;...].</td><td class="diff_next"></td><td class="diff_header" id="to95_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;DATA_CHANNEL,&nbsp;DATA_FEATURE,&nbsp;DATA_FEATURE&nbsp;...].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_34">34</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_34">34</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_35">35</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to95_35">35</td><td nowrap="nowrap">**Outputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_36">36</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_36">36</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_37">37</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to95_37">37</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to95__0"></td><td class="diff_header" id="from95_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Output&nbsp;tensor,&nbsp;which&nbsp;has&nbsp;the&nbsp;shape&nbsp;and&nbsp;type&nbsp;as&nbsp;input&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to95_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Output&nbsp;tensor,&nbsp;which&nbsp;has&nbsp;the&nbsp;shape&nbsp;and&nbsp;type&nbsp;as&nbsp;input&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_39">39</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_39">39</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_40">40</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to95_40">40</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_41">41</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to95_41">41</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_42">42</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to95_42">42</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to95__top">t</a></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"><a href="#difflib_chg_to95__top">t</a></td><td class="diff_header" id="to95_43">43</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;tensor(bfloat16),</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_43">43</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to95_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to95_45">45</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_45">45</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to95_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to95_47">47</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from95_47">47</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to95_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
            </tbody>
        </table>

.. _l-onnx-op-lrn-1:

LRN - 1
=======

**Version**

* **name**: `LRN (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#LRN>`_
* **domain**: **main**
* **since_version**: **1**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 1**.

**Summary**

Local Response Normalization proposed in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).
It normalizes over local input regions.
The local region is defined across the channels. For an element X[n, c, d1, ..., dk] in a tensor
of shape (N x C x D1 x D2, ..., Dk), its region is
{X[n, i, d1, ..., dk] | max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))}.

square_sum[n, c, d1, ..., dk] = sum(X[n, i, d1, ..., dk] ^ 2),
where max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2)).

Y[n, c, d1, ..., dk] = X[n, c, d1, ..., dk] / (bias + alpha / size * square_sum[n, c, d1, ..., dk] ) ^ beta

**Attributes**

* **alpha**:
  Scaling parameter.
* **beta**:
  The exponent.
* **bias**:

* **size** (required):
  The number of channels to sum over

**Inputs**

* **X** (heterogeneous) - **T**:
  Input data tensor from the previous operator; dimensions for image
  case are (N x C x H x W), where N is the batch size, C is the number
  of channels, and H and W are the height and the width of the data.
  For non image case, the dimensions are in the form of (N x C x D1 x
  D2 ... Dn), where N is the batch size. Optionally, if dimension
  denotation is in effect, the operation expects the input data tensor
  to arrive with the dimension denotation of [DATA_BATCH,
  DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...].

**Outputs**

* **Y** (heterogeneous) - **T**:
  Output tensor, which has the shape and type as input tensor

**Type Constraints**

* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output  types to float tensors.
