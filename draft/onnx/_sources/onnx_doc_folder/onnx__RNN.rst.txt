
.. _l-onnx-doc-RNN:

===
RNN
===

.. contents::
    :local:


.. _l-onnx-op-rnn-14:
RNN - 14
========
**Version**
* **name**: `RNN (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#RNN>`_
* **domain**: **main**
* **since_version**: **14**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 14**.

**Summary**

Computes an one-layer simple RNN. This operator is usually supported
via some custom implementation such as CuDNN.

Notations:

`X` - input tensor

`i` - input gate

`t` - time step (t-1 means previous time step)

`Wi` - W parameter weight matrix for input gate

`Ri` - R recurrence weight matrix for input gate

`Wbi` - W parameter bias vector for input gate

`Rbi` - R parameter bias vector for input gate

`WBi` - W parameter weight matrix for backward input gate

`RBi` - R recurrence weight matrix for backward input gate

`WBbi` - WR bias vectors for backward input gate

`RBbi` - RR bias vectors for backward input gate

`H` - Hidden state

`num_directions` - 2 if direction == bidirectional else 1

Activation functions:

  Relu(x)                - max(0, x)

  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})

  Sigmoid(x)             - 1/(1 + e^{-x})

  (NOTE: Below are optional)

  Affine(x)              - alpha*x + beta

  LeakyRelu(x)           - x if x >= 0 else alpha * x

  ThresholdedRelu(x)     - x if x >= alpha else 0

  ScaledTanh(x)          - alpha*Tanh(beta*x)

  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)

  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)

  Softsign(x)            - x/(1 + |x|)

  Softplus(x)            - log(1 + e^x)

Equations (Default: f=Tanh):

  - Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)
This operator has **optional** inputs/outputs. See `ONNX <https://github.com/onnx/onnx/blob/master/docs/IR.md>`_ for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.

**Attributes**
* **activation_alpha**:
  Optional scaling values used by some activation functions. The
  values are consumed in the order of activation functions, for
  example (f, g, h) in LSTM. Default values are the same as of
  corresponding ONNX operators.For example with LeakyRelu, the default
  alpha is 0.01.
* **activation_beta**:
  Optional scaling values used by some activation functions. The
  values are consumed in the order of activation functions, for
  example (f, g, h) in LSTM. Default values are the same as of
  corresponding ONNX operators.
* **activations**:
  One (or two if bidirectional) activation function for input gate.
  The activation function must be one of the activation functions
  specified above. Optional: Default `Tanh` if not specified.
* **clip**:
  Cell clip threshold. Clipping bounds the elements of a tensor in the
  range of [-threshold, +threshold] and is applied to the input of
  activations. No clip if not specified.
* **direction**:
  Specify if the RNN is forward, reverse, or bidirectional. Must be
  one of forward (default), reverse, or bidirectional.
* **hidden_size**:
  Number of neurons in the hidden layer
* **layout**:
  The shape format of inputs X, initial_h and outputs Y, Y_h. If 0,
  the following shapes are expected: X.shape = [seq_length,
  batch_size, input_size], Y.shape = [seq_length, num_directions,
  batch_size, hidden_size], initial_h.shape = Y_h.shape =
  [num_directions, batch_size, hidden_size]. If 1, the following
  shapes are expected: X.shape = [batch_size, seq_length, input_size],
  Y.shape = [batch_size, seq_length, num_directions, hidden_size],
  initial_h.shape = Y_h.shape = [batch_size, num_directions,
  hidden_size].

**Inputs**
Between 3 and 6 inputs.

* **X** (heterogeneous) - **T**:
  The input sequences packed (and potentially padded) into one 3-D
  tensor with the shape of `[seq_length, batch_size, input_size]`.
* **W** (heterogeneous) - **T**:
  The weight tensor for input gate. Concatenation of `Wi` and `WBi`
  (if bidirectional). The tensor has shape `[num_directions,
  hidden_size, input_size]`.
* **R** (heterogeneous) - **T**:
  The recurrence weight tensor. Concatenation of `Ri` and `RBi` (if
  bidirectional). The tensor has shape `[num_directions, hidden_size,
  hidden_size]`.
* **B** (optional, heterogeneous) - **T**:
  The bias tensor for input gate. Concatenation of `[Wbi, Rbi]` and
  `[WBbi, RBbi]` (if bidirectional). The tensor has shape
  `[num_directions, 2*hidden_size]`. Optional: If not specified -
  assumed to be 0.
* **sequence_lens** (optional, heterogeneous) - **T1**:
  Optional tensor specifying lengths of the sequences in a batch. If
  not specified - assumed all sequences in the batch to have length
  `seq_length`. It has shape `[batch_size]`.
* **initial_h** (optional, heterogeneous) - **T**:
  Optional initial value of the hidden. If not specified - assumed to
  be 0. It has shape `[num_directions, batch_size, hidden_size]`.

**Outputs**
Between 0 and 2 outputs.

* **Y** (optional, heterogeneous) - **T**:
  A tensor that concats all the intermediate output values of the
  hidden. It has shape `[seq_length, num_directions, batch_size,
  hidden_size]`.
* **Y_h** (optional, heterogeneous) - **T**:
  The last output value of the hidden. It has shape `[num_directions,
  batch_size, hidden_size]`.

**Type Constraints**
* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.
* **T1** in (
  tensor(int32)
  ):
  Constrain seq_lens to integer tensor.

**Examples**

**_defaults**
::
    input = np.array([[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]]).astype(np.float32)

    input_size = 2
    hidden_size = 4
    weight_scale = 0.1

    node = onnx.helper.make_node(
        "RNN", inputs=["X", "W", "R"], outputs=["", "Y_h"], hidden_size=hidden_size
    )

    W = weight_scale * np.ones((1, hidden_size, input_size)).astype(np.float32)
    R = weight_scale * np.ones((1, hidden_size, hidden_size)).astype(np.float32)

    rnn = RNN_Helper(X=input, W=W, R=R)
    _, Y_h = rnn.step()
    expect(
        node,
        inputs=[input, W, R],
        outputs=[Y_h.astype(np.float32)],
        name="test_simple_rnn_defaults",
    )

**_initial_bias**
::
    input = np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]).astype(
        np.float32
    )

    input_size = 3
    hidden_size = 5
    custom_bias = 0.1
    weight_scale = 0.1

    node = onnx.helper.make_node(
        "RNN",
        inputs=["X", "W", "R", "B"],
        outputs=["", "Y_h"],
        hidden_size=hidden_size,
    )

    W = weight_scale * np.ones((1, hidden_size, input_size)).astype(np.float32)
    R = weight_scale * np.ones((1, hidden_size, hidden_size)).astype(np.float32)

    # Adding custom bias
    W_B = custom_bias * np.ones((1, hidden_size)).astype(np.float32)
    R_B = np.zeros((1, hidden_size)).astype(np.float32)
    B = np.concatenate((W_B, R_B), axis=1)

    rnn = RNN_Helper(X=input, W=W, R=R, B=B)
    _, Y_h = rnn.step()
    expect(
        node,
        inputs=[input, W, R, B],
        outputs=[Y_h.astype(np.float32)],
        name="test_simple_rnn_with_initial_bias",
    )

**_seq_length**
::
    input = np.array(
        [
            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],
            [[10.0, 11.0, 12.0], [13.0, 14.0, 15.0], [16.0, 17.0, 18.0]],
        ]
    ).astype(np.float32)

    input_size = 3
    hidden_size = 5

    node = onnx.helper.make_node(
        "RNN",
        inputs=["X", "W", "R", "B"],
        outputs=["", "Y_h"],
        hidden_size=hidden_size,
    )

    W = np.random.randn(1, hidden_size, input_size).astype(np.float32)
    R = np.random.randn(1, hidden_size, hidden_size).astype(np.float32)

    # Adding custom bias
    W_B = np.random.randn(1, hidden_size).astype(np.float32)
    R_B = np.random.randn(1, hidden_size).astype(np.float32)
    B = np.concatenate((W_B, R_B), axis=1)

    rnn = RNN_Helper(X=input, W=W, R=R, B=B)
    _, Y_h = rnn.step()
    expect(
        node,
        inputs=[input, W, R, B],
        outputs=[Y_h.astype(np.float32)],
        name="test_rnn_seq_length",
    )

**_batchwise**
::
    input = np.array([[[1.0, 2.0]], [[3.0, 4.0]], [[5.0, 6.0]]]).astype(np.float32)

    input_size = 2
    hidden_size = 4
    weight_scale = 0.5
    layout = 1

    node = onnx.helper.make_node(
        "RNN",
        inputs=["X", "W", "R"],
        outputs=["Y", "Y_h"],
        hidden_size=hidden_size,
        layout=layout,
    )

    W = weight_scale * np.ones((1, hidden_size, input_size)).astype(np.float32)
    R = weight_scale * np.ones((1, hidden_size, hidden_size)).astype(np.float32)

    rnn = RNN_Helper(X=input, W=W, R=R, layout=layout)
    Y, Y_h = rnn.step()
    expect(
        node,
        inputs=[input, W, R],
        outputs=[Y.astype(np.float32), Y_h.astype(np.float32)],
        name="test_simple_rnn_batchwise",
    )

**Differences**

.. raw:: html

        <table class="diff" id="difflib_chg_to159__top"
               cellspacing="0" cellpadding="0" rules="groups" >
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

            <tbody>
                <tr><td class="diff_next"><a href="#difflib_chg_to159__0">f</a></td><td class="diff_header" id="from159_1">1</td><td nowrap="nowrap">Computes&nbsp;an&nbsp;one-layer&nbsp;simple&nbsp;RNN.&nbsp;This&nbsp;operator&nbsp;is&nbsp;usually&nbsp;supported</td><td class="diff_next"><a href="#difflib_chg_to159__0">f</a></td><td class="diff_header" id="to159_1">1</td><td nowrap="nowrap">Computes&nbsp;an&nbsp;one-layer&nbsp;simple&nbsp;RNN.&nbsp;This&nbsp;operator&nbsp;is&nbsp;usually&nbsp;supported</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_2">2</td><td nowrap="nowrap">via&nbsp;some&nbsp;custom&nbsp;implementation&nbsp;such&nbsp;as&nbsp;CuDNN.</td><td class="diff_next"></td><td class="diff_header" id="to159_2">2</td><td nowrap="nowrap">via&nbsp;some&nbsp;custom&nbsp;implementation&nbsp;such&nbsp;as&nbsp;CuDNN.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_3">3</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_3">3</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_4">4</td><td nowrap="nowrap">Notations:</td><td class="diff_next"></td><td class="diff_header" id="to159_4">4</td><td nowrap="nowrap">Notations:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_5">5</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_5">5</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_6">6</td><td nowrap="nowrap">X&nbsp;-&nbsp;input&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to159_6">6</td><td nowrap="nowrap">X&nbsp;-&nbsp;input&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_7">7</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_7">7</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_8">8</td><td nowrap="nowrap">i&nbsp;-&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_8">8</td><td nowrap="nowrap">i&nbsp;-&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_9">9</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_9">9</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_10">10</td><td nowrap="nowrap">t&nbsp;-&nbsp;time&nbsp;step&nbsp;(t-1&nbsp;means&nbsp;previous&nbsp;time&nbsp;step)</td><td class="diff_next"></td><td class="diff_header" id="to159_10">10</td><td nowrap="nowrap">t&nbsp;-&nbsp;time&nbsp;step&nbsp;(t-1&nbsp;means&nbsp;previous&nbsp;time&nbsp;step)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_11">11</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_11">11</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_12">12</td><td nowrap="nowrap">Wi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_12">12</td><td nowrap="nowrap">Wi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_13">13</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_13">13</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_14">14</td><td nowrap="nowrap">Ri&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_14">14</td><td nowrap="nowrap">Ri&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_15">15</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_15">15</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_16">16</td><td nowrap="nowrap">Wbi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_16">16</td><td nowrap="nowrap">Wbi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_17">17</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_17">17</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_18">18</td><td nowrap="nowrap">Rbi&nbsp;-&nbsp;R&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_18">18</td><td nowrap="nowrap">Rbi&nbsp;-&nbsp;R&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_19">19</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_19">19</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_20">20</td><td nowrap="nowrap">WBi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_20">20</td><td nowrap="nowrap">WBi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_21">21</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_21">21</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_22">22</td><td nowrap="nowrap">RBi&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_22">22</td><td nowrap="nowrap">RBi&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_23">23</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_23">23</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_24">24</td><td nowrap="nowrap">WBbi&nbsp;-&nbsp;WR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_24">24</td><td nowrap="nowrap">WBbi&nbsp;-&nbsp;WR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_25">25</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_25">25</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_26">26</td><td nowrap="nowrap">RBbi&nbsp;-&nbsp;RR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to159_26">26</td><td nowrap="nowrap">RBbi&nbsp;-&nbsp;RR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_27">27</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_27">27</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_28">28</td><td nowrap="nowrap">H&nbsp;-&nbsp;Hidden&nbsp;state</td><td class="diff_next"></td><td class="diff_header" id="to159_28">28</td><td nowrap="nowrap">H&nbsp;-&nbsp;Hidden&nbsp;state</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_29">29</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_29">29</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_30">30</td><td nowrap="nowrap">num_directions&nbsp;-&nbsp;2&nbsp;if&nbsp;direction&nbsp;==&nbsp;bidirectional&nbsp;else&nbsp;1</td><td class="diff_next"></td><td class="diff_header" id="to159_30">30</td><td nowrap="nowrap">num_directions&nbsp;-&nbsp;2&nbsp;if&nbsp;direction&nbsp;==&nbsp;bidirectional&nbsp;else&nbsp;1</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_31">31</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_31">31</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_32">32</td><td nowrap="nowrap">Activation&nbsp;functions:</td><td class="diff_next"></td><td class="diff_header" id="to159_32">32</td><td nowrap="nowrap">Activation&nbsp;functions:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_33">33</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_33">33</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;Relu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;max(0,&nbsp;x)</td><td class="diff_next"></td><td class="diff_header" id="to159_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;Relu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;max(0,&nbsp;x)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_35">35</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_35">35</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;Tanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;(1&nbsp;-&nbsp;e^{-2x})/(1&nbsp;+&nbsp;e^{-2x})</td><td class="diff_next"></td><td class="diff_header" id="to159_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;Tanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;(1&nbsp;-&nbsp;e^{-2x})/(1&nbsp;+&nbsp;e^{-2x})</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_37">37</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_37">37</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Sigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;1/(1&nbsp;+&nbsp;e^{-x})</td><td class="diff_next"></td><td class="diff_header" id="to159_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Sigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;1/(1&nbsp;+&nbsp;e^{-x})</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_39">39</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_39">39</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;(NOTE:&nbsp;Below&nbsp;are&nbsp;optional)</td><td class="diff_next"></td><td class="diff_header" id="to159_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;(NOTE:&nbsp;Below&nbsp;are&nbsp;optional)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_41">41</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_41">41</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;Affine(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*x&nbsp;+&nbsp;beta</td><td class="diff_next"></td><td class="diff_header" id="to159_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;Affine(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*x&nbsp;+&nbsp;beta</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_43">43</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_43">43</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;LeakyRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha&nbsp;*&nbsp;x</td><td class="diff_next"></td><td class="diff_header" id="to159_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;LeakyRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha&nbsp;*&nbsp;x</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_45">45</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_45">45</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;ThresholdedRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;alpha&nbsp;else&nbsp;0</td><td class="diff_next"></td><td class="diff_header" id="to159_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;ThresholdedRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;alpha&nbsp;else&nbsp;0</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_47">47</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_47">47</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;ScaledTanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*Tanh(beta*x)</td><td class="diff_next"></td><td class="diff_header" id="to159_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;ScaledTanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*Tanh(beta*x)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_49">49</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_49">49</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_50">50</td><td nowrap="nowrap">&nbsp;&nbsp;HardSigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;min(max(alpha*x&nbsp;+&nbsp;beta,&nbsp;0),&nbsp;1)</td><td class="diff_next"></td><td class="diff_header" id="to159_50">50</td><td nowrap="nowrap">&nbsp;&nbsp;HardSigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;min(max(alpha*x&nbsp;+&nbsp;beta,&nbsp;0),&nbsp;1)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_51">51</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_51">51</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_52">52</td><td nowrap="nowrap">&nbsp;&nbsp;Elu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha*(e^x&nbsp;-&nbsp;1)</td><td class="diff_next"></td><td class="diff_header" id="to159_52">52</td><td nowrap="nowrap">&nbsp;&nbsp;Elu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha*(e^x&nbsp;-&nbsp;1)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_53">53</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_53">53</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_54">54</td><td nowrap="nowrap">&nbsp;&nbsp;Softsign(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x/(1&nbsp;+&nbsp;|x|)</td><td class="diff_next"></td><td class="diff_header" id="to159_54">54</td><td nowrap="nowrap">&nbsp;&nbsp;Softsign(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x/(1&nbsp;+&nbsp;|x|)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_55">55</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_55">55</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_56">56</td><td nowrap="nowrap">&nbsp;&nbsp;Softplus(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;log(1&nbsp;+&nbsp;e^x)</td><td class="diff_next"></td><td class="diff_header" id="to159_56">56</td><td nowrap="nowrap">&nbsp;&nbsp;Softplus(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;log(1&nbsp;+&nbsp;e^x)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_57">57</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_57">57</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_58">58</td><td nowrap="nowrap">Equations&nbsp;(Default:&nbsp;f=Tanh):</td><td class="diff_next"></td><td class="diff_header" id="to159_58">58</td><td nowrap="nowrap">Equations&nbsp;(Default:&nbsp;f=Tanh):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_59">59</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_59">59</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_60">60</td><td nowrap="nowrap">&nbsp;&nbsp;-&nbsp;Ht&nbsp;=&nbsp;f(Xt*(Wi^T)&nbsp;+&nbsp;Ht-1*(Ri^T)&nbsp;+&nbsp;Wbi&nbsp;+&nbsp;Rbi)</td><td class="diff_next"></td><td class="diff_header" id="to159_60">60</td><td nowrap="nowrap">&nbsp;&nbsp;-&nbsp;Ht&nbsp;=&nbsp;f(Xt*(Wi^T)&nbsp;+&nbsp;Ht-1*(Ri^T)&nbsp;+&nbsp;Wbi&nbsp;+&nbsp;Rbi)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_61">61</td><td nowrap="nowrap">This&nbsp;operator&nbsp;has&nbsp;**optional**&nbsp;inputs/outputs.&nbsp;See&nbsp;ONNX&nbsp;&lt;https://github.com/onnx/onnx/blob/master/docs/IR.md&gt;_&nbsp;for&nbsp;more&nbsp;details&nbsp;about&nbsp;the&nbsp;representation&nbsp;of&nbsp;optional&nbsp;arguments.&nbsp;An&nbsp;empty&nbsp;string&nbsp;may&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;place&nbsp;of&nbsp;an&nbsp;actual&nbsp;argument's&nbsp;name&nbsp;to&nbsp;indicate&nbsp;a&nbsp;missing&nbsp;argument.&nbsp;Trailing&nbsp;optional&nbsp;arguments&nbsp;(those&nbsp;not&nbsp;followed&nbsp;by&nbsp;an&nbsp;argument&nbsp;that&nbsp;is&nbsp;present)&nbsp;may&nbsp;also&nbsp;be&nbsp;simply&nbsp;omitted.</td><td class="diff_next"></td><td class="diff_header" id="to159_61">61</td><td nowrap="nowrap">This&nbsp;operator&nbsp;has&nbsp;**optional**&nbsp;inputs/outputs.&nbsp;See&nbsp;ONNX&nbsp;&lt;https://github.com/onnx/onnx/blob/master/docs/IR.md&gt;_&nbsp;for&nbsp;more&nbsp;details&nbsp;about&nbsp;the&nbsp;representation&nbsp;of&nbsp;optional&nbsp;arguments.&nbsp;An&nbsp;empty&nbsp;string&nbsp;may&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;place&nbsp;of&nbsp;an&nbsp;actual&nbsp;argument's&nbsp;name&nbsp;to&nbsp;indicate&nbsp;a&nbsp;missing&nbsp;argument.&nbsp;Trailing&nbsp;optional&nbsp;arguments&nbsp;(those&nbsp;not&nbsp;followed&nbsp;by&nbsp;an&nbsp;argument&nbsp;that&nbsp;is&nbsp;present)&nbsp;may&nbsp;also&nbsp;be&nbsp;simply&nbsp;omitted.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_62">62</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_62">62</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_63">63</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to159_63">63</td><td nowrap="nowrap">**Attributes**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_64">64</td><td nowrap="nowrap">*&nbsp;**activation_alpha**:</td><td class="diff_next"></td><td class="diff_header" id="to159_64">64</td><td nowrap="nowrap">*&nbsp;**activation_alpha**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_65">65</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td><td class="diff_next"></td><td class="diff_header" id="to159_65">65</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_66">66</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td><td class="diff_next"></td><td class="diff_header" id="to159_66">66</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_67">67</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to159_67">67</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_68">68</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.For&nbsp;example&nbsp;with&nbsp;LeakyRelu,&nbsp;the&nbsp;default</td><td class="diff_next"></td><td class="diff_header" id="to159_68">68</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.For&nbsp;example&nbsp;with&nbsp;LeakyRelu,&nbsp;the&nbsp;default</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_69">69</td><td nowrap="nowrap">&nbsp;&nbsp;alpha&nbsp;is&nbsp;0.01.</td><td class="diff_next"></td><td class="diff_header" id="to159_69">69</td><td nowrap="nowrap">&nbsp;&nbsp;alpha&nbsp;is&nbsp;0.01.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_70">70</td><td nowrap="nowrap">*&nbsp;**activation_beta**:</td><td class="diff_next"></td><td class="diff_header" id="to159_70">70</td><td nowrap="nowrap">*&nbsp;**activation_beta**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_71">71</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td><td class="diff_next"></td><td class="diff_header" id="to159_71">71</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_72">72</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td><td class="diff_next"></td><td class="diff_header" id="to159_72">72</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_73">73</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to159_73">73</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_74">74</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.</td><td class="diff_next"></td><td class="diff_header" id="to159_74">74</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_75">75</td><td nowrap="nowrap">*&nbsp;**activations**:</td><td class="diff_next"></td><td class="diff_header" id="to159_75">75</td><td nowrap="nowrap">*&nbsp;**activations**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_76">76</td><td nowrap="nowrap">&nbsp;&nbsp;One&nbsp;(or&nbsp;two&nbsp;if&nbsp;bidirectional)&nbsp;activation&nbsp;function&nbsp;for&nbsp;input&nbsp;gate.</td><td class="diff_next"></td><td class="diff_header" id="to159_76">76</td><td nowrap="nowrap">&nbsp;&nbsp;One&nbsp;(or&nbsp;two&nbsp;if&nbsp;bidirectional)&nbsp;activation&nbsp;function&nbsp;for&nbsp;input&nbsp;gate.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_77">77</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;activation&nbsp;function&nbsp;must&nbsp;be&nbsp;one&nbsp;of&nbsp;the&nbsp;activation&nbsp;functions</td><td class="diff_next"></td><td class="diff_header" id="to159_77">77</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;activation&nbsp;function&nbsp;must&nbsp;be&nbsp;one&nbsp;of&nbsp;the&nbsp;activation&nbsp;functions</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_78">78</td><td nowrap="nowrap">&nbsp;&nbsp;specified&nbsp;above.&nbsp;Optional:&nbsp;Default&nbsp;Tanh&nbsp;if&nbsp;not&nbsp;specified.</td><td class="diff_next"></td><td class="diff_header" id="to159_78">78</td><td nowrap="nowrap">&nbsp;&nbsp;specified&nbsp;above.&nbsp;Optional:&nbsp;Default&nbsp;Tanh&nbsp;if&nbsp;not&nbsp;specified.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_79">79</td><td nowrap="nowrap">*&nbsp;**clip**:</td><td class="diff_next"></td><td class="diff_header" id="to159_79">79</td><td nowrap="nowrap">*&nbsp;**clip**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_80">80</td><td nowrap="nowrap">&nbsp;&nbsp;Cell&nbsp;clip&nbsp;threshold.&nbsp;Clipping&nbsp;bounds&nbsp;the&nbsp;elements&nbsp;of&nbsp;a&nbsp;tensor&nbsp;in&nbsp;the</td><td class="diff_next"></td><td class="diff_header" id="to159_80">80</td><td nowrap="nowrap">&nbsp;&nbsp;Cell&nbsp;clip&nbsp;threshold.&nbsp;Clipping&nbsp;bounds&nbsp;the&nbsp;elements&nbsp;of&nbsp;a&nbsp;tensor&nbsp;in&nbsp;the</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_81">81</td><td nowrap="nowrap">&nbsp;&nbsp;range&nbsp;of&nbsp;[-threshold,&nbsp;+threshold]&nbsp;and&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;input&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to159_81">81</td><td nowrap="nowrap">&nbsp;&nbsp;range&nbsp;of&nbsp;[-threshold,&nbsp;+threshold]&nbsp;and&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;input&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_82">82</td><td nowrap="nowrap">&nbsp;&nbsp;activations.&nbsp;No&nbsp;clip&nbsp;if&nbsp;not&nbsp;specified.</td><td class="diff_next"></td><td class="diff_header" id="to159_82">82</td><td nowrap="nowrap">&nbsp;&nbsp;activations.&nbsp;No&nbsp;clip&nbsp;if&nbsp;not&nbsp;specified.</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to159__0"></td><td class="diff_header" id="from159_83">83</td><td nowrap="nowrap">*&nbsp;**direction**:</td><td class="diff_next"></td><td class="diff_header" id="to159_83">83</td><td nowrap="nowrap">*&nbsp;**direction**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_84">84</td><td nowrap="nowrap">&nbsp;&nbsp;Specify&nbsp;if&nbsp;the&nbsp;RNN&nbsp;is&nbsp;forward,&nbsp;reverse,&nbsp;or&nbsp;bidirectional.&nbsp;Must&nbsp;be</td><td class="diff_next"></td><td class="diff_header" id="to159_84">84</td><td nowrap="nowrap">&nbsp;&nbsp;Specify&nbsp;if&nbsp;the&nbsp;RNN&nbsp;is&nbsp;forward,&nbsp;reverse,&nbsp;or&nbsp;bidirectional.&nbsp;Must&nbsp;be</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_85">85</td><td nowrap="nowrap">&nbsp;&nbsp;one&nbsp;of&nbsp;forward&nbsp;(default),&nbsp;reverse,&nbsp;or&nbsp;bidirectional.</td><td class="diff_next"></td><td class="diff_header" id="to159_85">85</td><td nowrap="nowrap">&nbsp;&nbsp;one&nbsp;of&nbsp;forward&nbsp;(default),&nbsp;reverse,&nbsp;or&nbsp;bidirectional.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_86">86</td><td nowrap="nowrap">*&nbsp;**hidden_size**:</td><td class="diff_next"></td><td class="diff_header" id="to159_86">86</td><td nowrap="nowrap">*&nbsp;**hidden_size**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_87">87</td><td nowrap="nowrap">&nbsp;&nbsp;Number&nbsp;of&nbsp;neurons&nbsp;in&nbsp;the&nbsp;hidden&nbsp;layer</td><td class="diff_next"></td><td class="diff_header" id="to159_87">87</td><td nowrap="nowrap">&nbsp;&nbsp;Number&nbsp;of&nbsp;neurons&nbsp;in&nbsp;the&nbsp;hidden&nbsp;layer</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to159__top">t</a></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"><a href="#difflib_chg_to159__top">t</a></td><td class="diff_header" id="to159_88">88</td><td nowrap="nowrap"><span class="diff_add">*&nbsp;**layout**:</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_89">89</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;The&nbsp;shape&nbsp;format&nbsp;of&nbsp;inputs&nbsp;X,&nbsp;initial_h&nbsp;and&nbsp;outputs&nbsp;Y,&nbsp;Y_h.&nbsp;If&nbsp;0,</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_90">90</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;the&nbsp;following&nbsp;shapes&nbsp;are&nbsp;expected:&nbsp;X.shape&nbsp;=&nbsp;[seq_length,</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_91">91</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;batch_size,&nbsp;input_size],&nbsp;Y.shape&nbsp;=&nbsp;[seq_length,&nbsp;num_directions,</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_92">92</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;batch_size,&nbsp;hidden_size],&nbsp;initial_h.shape&nbsp;=&nbsp;Y_h.shape&nbsp;=</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_93">93</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;[num_directions,&nbsp;batch_size,&nbsp;hidden_size].&nbsp;If&nbsp;1,&nbsp;the&nbsp;following</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_94">94</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;shapes&nbsp;are&nbsp;expected:&nbsp;X.shape&nbsp;=&nbsp;[batch_size,&nbsp;seq_length,&nbsp;input_size],</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_95">95</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;Y.shape&nbsp;=&nbsp;[batch_size,&nbsp;seq_length,&nbsp;num_directions,&nbsp;hidden_size],</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_96">96</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;initial_h.shape&nbsp;=&nbsp;Y_h.shape&nbsp;=&nbsp;[batch_size,&nbsp;num_directions,</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_97">97</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;hidden_size].</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_88">88</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_98">98</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_89">89</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to159_99">99</td><td nowrap="nowrap">**Inputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_90">90</td><td nowrap="nowrap">Between&nbsp;3&nbsp;and&nbsp;6&nbsp;inputs.</td><td class="diff_next"></td><td class="diff_header" id="to159_100">100</td><td nowrap="nowrap">Between&nbsp;3&nbsp;and&nbsp;6&nbsp;inputs.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_91">91</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_101">101</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_92">92</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_102">102</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_93">93</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;input&nbsp;sequences&nbsp;packed&nbsp;(and&nbsp;potentially&nbsp;padded)&nbsp;into&nbsp;one&nbsp;3-D</td><td class="diff_next"></td><td class="diff_header" id="to159_103">103</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;input&nbsp;sequences&nbsp;packed&nbsp;(and&nbsp;potentially&nbsp;padded)&nbsp;into&nbsp;one&nbsp;3-D</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_94">94</td><td nowrap="nowrap">&nbsp;&nbsp;tensor&nbsp;with&nbsp;the&nbsp;shape&nbsp;of&nbsp;[seq_length,&nbsp;batch_size,&nbsp;input_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_104">104</td><td nowrap="nowrap">&nbsp;&nbsp;tensor&nbsp;with&nbsp;the&nbsp;shape&nbsp;of&nbsp;[seq_length,&nbsp;batch_size,&nbsp;input_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_95">95</td><td nowrap="nowrap">*&nbsp;**W**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_105">105</td><td nowrap="nowrap">*&nbsp;**W**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_96">96</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;weight&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;Wi&nbsp;and&nbsp;WBi</td><td class="diff_next"></td><td class="diff_header" id="to159_106">106</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;weight&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;Wi&nbsp;and&nbsp;WBi</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_97">97</td><td nowrap="nowrap">&nbsp;&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,</td><td class="diff_next"></td><td class="diff_header" id="to159_107">107</td><td nowrap="nowrap">&nbsp;&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_98">98</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size,&nbsp;input_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_108">108</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size,&nbsp;input_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_99">99</td><td nowrap="nowrap">*&nbsp;**R**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_109">109</td><td nowrap="nowrap">*&nbsp;**R**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_100">100</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;recurrence&nbsp;weight&nbsp;tensor.&nbsp;Concatenation&nbsp;of&nbsp;Ri&nbsp;and&nbsp;RBi&nbsp;(if</td><td class="diff_next"></td><td class="diff_header" id="to159_110">110</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;recurrence&nbsp;weight&nbsp;tensor.&nbsp;Concatenation&nbsp;of&nbsp;Ri&nbsp;and&nbsp;RBi&nbsp;(if</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_101">101</td><td nowrap="nowrap">&nbsp;&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;hidden_size,</td><td class="diff_next"></td><td class="diff_header" id="to159_111">111</td><td nowrap="nowrap">&nbsp;&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;hidden_size,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_102">102</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_112">112</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_103">103</td><td nowrap="nowrap">*&nbsp;**B**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_113">113</td><td nowrap="nowrap">*&nbsp;**B**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_104">104</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;bias&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;[Wbi,&nbsp;Rbi]&nbsp;and</td><td class="diff_next"></td><td class="diff_header" id="to159_114">114</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;bias&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;[Wbi,&nbsp;Rbi]&nbsp;and</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_105">105</td><td nowrap="nowrap">&nbsp;&nbsp;[WBbi,&nbsp;RBbi]&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape</td><td class="diff_next"></td><td class="diff_header" id="to159_115">115</td><td nowrap="nowrap">&nbsp;&nbsp;[WBbi,&nbsp;RBbi]&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_106">106</td><td nowrap="nowrap">&nbsp;&nbsp;[num_directions,&nbsp;2*hidden_size].&nbsp;Optional:&nbsp;If&nbsp;not&nbsp;specified&nbsp;-</td><td class="diff_next"></td><td class="diff_header" id="to159_116">116</td><td nowrap="nowrap">&nbsp;&nbsp;[num_directions,&nbsp;2*hidden_size].&nbsp;Optional:&nbsp;If&nbsp;not&nbsp;specified&nbsp;-</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_107">107</td><td nowrap="nowrap">&nbsp;&nbsp;assumed&nbsp;to&nbsp;be&nbsp;0.</td><td class="diff_next"></td><td class="diff_header" id="to159_117">117</td><td nowrap="nowrap">&nbsp;&nbsp;assumed&nbsp;to&nbsp;be&nbsp;0.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_108">108</td><td nowrap="nowrap">*&nbsp;**sequence_lens**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T1**:</td><td class="diff_next"></td><td class="diff_header" id="to159_118">118</td><td nowrap="nowrap">*&nbsp;**sequence_lens**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T1**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_109">109</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;tensor&nbsp;specifying&nbsp;lengths&nbsp;of&nbsp;the&nbsp;sequences&nbsp;in&nbsp;a&nbsp;batch.&nbsp;If</td><td class="diff_next"></td><td class="diff_header" id="to159_119">119</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;tensor&nbsp;specifying&nbsp;lengths&nbsp;of&nbsp;the&nbsp;sequences&nbsp;in&nbsp;a&nbsp;batch.&nbsp;If</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_110">110</td><td nowrap="nowrap">&nbsp;&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;all&nbsp;sequences&nbsp;in&nbsp;the&nbsp;batch&nbsp;to&nbsp;have&nbsp;length</td><td class="diff_next"></td><td class="diff_header" id="to159_120">120</td><td nowrap="nowrap">&nbsp;&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;all&nbsp;sequences&nbsp;in&nbsp;the&nbsp;batch&nbsp;to&nbsp;have&nbsp;length</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_111">111</td><td nowrap="nowrap">&nbsp;&nbsp;seq_length.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[batch_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_121">121</td><td nowrap="nowrap">&nbsp;&nbsp;seq_length.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[batch_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_112">112</td><td nowrap="nowrap">*&nbsp;**initial_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_122">122</td><td nowrap="nowrap">*&nbsp;**initial_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_113">113</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;initial&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;If&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;to</td><td class="diff_next"></td><td class="diff_header" id="to159_123">123</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;initial&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;If&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;to</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_114">114</td><td nowrap="nowrap">&nbsp;&nbsp;be&nbsp;0.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;batch_size,&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_124">124</td><td nowrap="nowrap">&nbsp;&nbsp;be&nbsp;0.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;batch_size,&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_115">115</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_125">125</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_116">116</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to159_126">126</td><td nowrap="nowrap">**Outputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_117">117</td><td nowrap="nowrap">Between&nbsp;0&nbsp;and&nbsp;2&nbsp;outputs.</td><td class="diff_next"></td><td class="diff_header" id="to159_127">127</td><td nowrap="nowrap">Between&nbsp;0&nbsp;and&nbsp;2&nbsp;outputs.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_118">118</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_128">128</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_119">119</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_129">129</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_120">120</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;tensor&nbsp;that&nbsp;concats&nbsp;all&nbsp;the&nbsp;intermediate&nbsp;output&nbsp;values&nbsp;of&nbsp;the</td><td class="diff_next"></td><td class="diff_header" id="to159_130">130</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;tensor&nbsp;that&nbsp;concats&nbsp;all&nbsp;the&nbsp;intermediate&nbsp;output&nbsp;values&nbsp;of&nbsp;the</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_121">121</td><td nowrap="nowrap">&nbsp;&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[seq_length,&nbsp;num_directions,&nbsp;batch_size,</td><td class="diff_next"></td><td class="diff_header" id="to159_131">131</td><td nowrap="nowrap">&nbsp;&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[seq_length,&nbsp;num_directions,&nbsp;batch_size,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_122">122</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_132">132</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_123">123</td><td nowrap="nowrap">*&nbsp;**Y_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to159_133">133</td><td nowrap="nowrap">*&nbsp;**Y_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_124">124</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;last&nbsp;output&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,</td><td class="diff_next"></td><td class="diff_header" id="to159_134">134</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;last&nbsp;output&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_125">125</td><td nowrap="nowrap">&nbsp;&nbsp;batch_size,&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to159_135">135</td><td nowrap="nowrap">&nbsp;&nbsp;batch_size,&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_126">126</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to159_136">136</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_127">127</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to159_137">137</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_128">128</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to159_138">138</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_129">129</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to159_139">139</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_130">130</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to159_140">140</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_131">131</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to159_141">141</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_132">132</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to159_142">142</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_133">133</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to159_143">143</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_134">134</td><td nowrap="nowrap">*&nbsp;**T1**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to159_144">144</td><td nowrap="nowrap">*&nbsp;**T1**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_135">135</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32)</td><td class="diff_next"></td><td class="diff_header" id="to159_145">145</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_136">136</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to159_146">146</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from159_137">137</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;seq_lens&nbsp;to&nbsp;integer&nbsp;tensor.</td><td class="diff_next"></td><td class="diff_header" id="to159_147">147</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;seq_lens&nbsp;to&nbsp;integer&nbsp;tensor.</td></tr>
            </tbody>
        </table>

.. _l-onnx-op-rnn-7:
RNN - 7
=======
**Version**
* **name**: `RNN (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#RNN>`_
* **domain**: **main**
* **since_version**: **7**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 7**.

**Summary**

Computes an one-layer simple RNN. This operator is usually supported
via some custom implementation such as CuDNN.

Notations:

`X` - input tensor

`i` - input gate

`t` - time step (t-1 means previous time step)

`Wi` - W parameter weight matrix for input gate

`Ri` - R recurrence weight matrix for input gate

`Wbi` - W parameter bias vector for input gate

`Rbi` - R parameter bias vector for input gate

`WBi` - W parameter weight matrix for backward input gate

`RBi` - R recurrence weight matrix for backward input gate

`WBbi` - WR bias vectors for backward input gate

`RBbi` - RR bias vectors for backward input gate

`H` - Hidden state

`num_directions` - 2 if direction == bidirectional else 1

Activation functions:

  Relu(x)                - max(0, x)

  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})

  Sigmoid(x)             - 1/(1 + e^{-x})

  (NOTE: Below are optional)

  Affine(x)              - alpha*x + beta

  LeakyRelu(x)           - x if x >= 0 else alpha * x

  ThresholdedRelu(x)     - x if x >= alpha else 0

  ScaledTanh(x)          - alpha*Tanh(beta*x)

  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)

  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)

  Softsign(x)            - x/(1 + |x|)

  Softplus(x)            - log(1 + e^x)

Equations (Default: f=Tanh):

  - Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)
This operator has **optional** inputs/outputs. See `ONNX <https://github.com/onnx/onnx/blob/master/docs/IR.md>`_ for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.

**Attributes**
* **activation_alpha**:
  Optional scaling values used by some activation functions. The
  values are consumed in the order of activation functions, for
  example (f, g, h) in LSTM. Default values are the same as of
  corresponding ONNX operators.For example with LeakyRelu, the default
  alpha is 0.01.
* **activation_beta**:
  Optional scaling values used by some activation functions. The
  values are consumed in the order of activation functions, for
  example (f, g, h) in LSTM. Default values are the same as of
  corresponding ONNX operators.
* **activations**:
  One (or two if bidirectional) activation function for input gate.
  The activation function must be one of the activation functions
  specified above. Optional: Default `Tanh` if not specified.
* **clip**:
  Cell clip threshold. Clipping bounds the elements of a tensor in the
  range of [-threshold, +threshold] and is applied to the input of
  activations. No clip if not specified.
* **direction**:
  Specify if the RNN is forward, reverse, or bidirectional. Must be
  one of forward (default), reverse, or bidirectional.
* **hidden_size**:
  Number of neurons in the hidden layer

**Inputs**
Between 3 and 6 inputs.

* **X** (heterogeneous) - **T**:
  The input sequences packed (and potentially padded) into one 3-D
  tensor with the shape of `[seq_length, batch_size, input_size]`.
* **W** (heterogeneous) - **T**:
  The weight tensor for input gate. Concatenation of `Wi` and `WBi`
  (if bidirectional). The tensor has shape `[num_directions,
  hidden_size, input_size]`.
* **R** (heterogeneous) - **T**:
  The recurrence weight tensor. Concatenation of `Ri` and `RBi` (if
  bidirectional). The tensor has shape `[num_directions, hidden_size,
  hidden_size]`.
* **B** (optional, heterogeneous) - **T**:
  The bias tensor for input gate. Concatenation of `[Wbi, Rbi]` and
  `[WBbi, RBbi]` (if bidirectional). The tensor has shape
  `[num_directions, 2*hidden_size]`. Optional: If not specified -
  assumed to be 0.
* **sequence_lens** (optional, heterogeneous) - **T1**:
  Optional tensor specifying lengths of the sequences in a batch. If
  not specified - assumed all sequences in the batch to have length
  `seq_length`. It has shape `[batch_size]`.
* **initial_h** (optional, heterogeneous) - **T**:
  Optional initial value of the hidden. If not specified - assumed to
  be 0. It has shape `[num_directions, batch_size, hidden_size]`.

**Outputs**
Between 0 and 2 outputs.

* **Y** (optional, heterogeneous) - **T**:
  A tensor that concats all the intermediate output values of the
  hidden. It has shape `[seq_length, num_directions, batch_size,
  hidden_size]`.
* **Y_h** (optional, heterogeneous) - **T**:
  The last output value of the hidden. It has shape `[num_directions,
  batch_size, hidden_size]`.

**Type Constraints**
* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.
* **T1** in (
  tensor(int32)
  ):
  Constrain seq_lens to integer tensor.

**Differences**

.. raw:: html

        <table class="diff" id="difflib_chg_to160__top"
               cellspacing="0" cellpadding="0" rules="groups" >
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

            <tbody>
                <tr><td class="diff_next"><a href="#difflib_chg_to160__0">f</a></td><td class="diff_header" id="from160_1">1</td><td nowrap="nowrap">Computes&nbsp;an&nbsp;one-layer&nbsp;simple&nbsp;RNN.&nbsp;This&nbsp;operator&nbsp;is&nbsp;usually&nbsp;supported</td><td class="diff_next"><a href="#difflib_chg_to160__0">f</a></td><td class="diff_header" id="to160_1">1</td><td nowrap="nowrap">Computes&nbsp;an&nbsp;one-layer&nbsp;simple&nbsp;RNN.&nbsp;This&nbsp;operator&nbsp;is&nbsp;usually&nbsp;supported</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_2">2</td><td nowrap="nowrap">via&nbsp;some&nbsp;custom&nbsp;implementation&nbsp;such&nbsp;as&nbsp;CuDNN.</td><td class="diff_next"></td><td class="diff_header" id="to160_2">2</td><td nowrap="nowrap">via&nbsp;some&nbsp;custom&nbsp;implementation&nbsp;such&nbsp;as&nbsp;CuDNN.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_3">3</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_3">3</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_4">4</td><td nowrap="nowrap">Notations:</td><td class="diff_next"></td><td class="diff_header" id="to160_4">4</td><td nowrap="nowrap">Notations:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_5">5</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_5">5</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_6">6</td><td nowrap="nowrap">X&nbsp;-&nbsp;input&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to160_6">6</td><td nowrap="nowrap">X&nbsp;-&nbsp;input&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_7">7</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_7">7</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_8">8</td><td nowrap="nowrap">i&nbsp;-&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_8">8</td><td nowrap="nowrap">i&nbsp;-&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_9">9</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_9">9</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_10">10</td><td nowrap="nowrap">t&nbsp;-&nbsp;time&nbsp;step&nbsp;(t-1&nbsp;means&nbsp;previous&nbsp;time&nbsp;step)</td><td class="diff_next"></td><td class="diff_header" id="to160_10">10</td><td nowrap="nowrap">t&nbsp;-&nbsp;time&nbsp;step&nbsp;(t-1&nbsp;means&nbsp;previous&nbsp;time&nbsp;step)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_11">11</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_11">11</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_12">12</td><td nowrap="nowrap">Wi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_12">12</td><td nowrap="nowrap">Wi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_13">13</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_13">13</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_14">14</td><td nowrap="nowrap">Ri&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_14">14</td><td nowrap="nowrap">Ri&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_15">15</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_15">15</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_16">16</td><td nowrap="nowrap">Wbi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_16">16</td><td nowrap="nowrap">Wbi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_17">17</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_17">17</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_18">18</td><td nowrap="nowrap">Rbi&nbsp;-&nbsp;R&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_18">18</td><td nowrap="nowrap">Rbi&nbsp;-&nbsp;R&nbsp;parameter&nbsp;bias&nbsp;vector&nbsp;for&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_19">19</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_19">19</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_20">20</td><td nowrap="nowrap">WBi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_20">20</td><td nowrap="nowrap">WBi&nbsp;-&nbsp;W&nbsp;parameter&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_21">21</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_21">21</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_22">22</td><td nowrap="nowrap">RBi&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_22">22</td><td nowrap="nowrap">RBi&nbsp;-&nbsp;R&nbsp;recurrence&nbsp;weight&nbsp;matrix&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_23">23</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_23">23</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_24">24</td><td nowrap="nowrap">WBbi&nbsp;-&nbsp;WR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_24">24</td><td nowrap="nowrap">WBbi&nbsp;-&nbsp;WR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_25">25</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_25">25</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_26">26</td><td nowrap="nowrap">RBbi&nbsp;-&nbsp;RR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td><td class="diff_next"></td><td class="diff_header" id="to160_26">26</td><td nowrap="nowrap">RBbi&nbsp;-&nbsp;RR&nbsp;bias&nbsp;vectors&nbsp;for&nbsp;backward&nbsp;input&nbsp;gate</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_27">27</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_27">27</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_28">28</td><td nowrap="nowrap">H&nbsp;-&nbsp;Hidden&nbsp;state</td><td class="diff_next"></td><td class="diff_header" id="to160_28">28</td><td nowrap="nowrap">H&nbsp;-&nbsp;Hidden&nbsp;state</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_29">29</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_29">29</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_30">30</td><td nowrap="nowrap">num_directions&nbsp;-&nbsp;2&nbsp;if&nbsp;direction&nbsp;==&nbsp;bidirectional&nbsp;else&nbsp;1</td><td class="diff_next"></td><td class="diff_header" id="to160_30">30</td><td nowrap="nowrap">num_directions&nbsp;-&nbsp;2&nbsp;if&nbsp;direction&nbsp;==&nbsp;bidirectional&nbsp;else&nbsp;1</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_31">31</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_31">31</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_32">32</td><td nowrap="nowrap">Activation&nbsp;functions:</td><td class="diff_next"></td><td class="diff_header" id="to160_32">32</td><td nowrap="nowrap">Activation&nbsp;functions:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_33">33</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_33">33</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;Relu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;max(0,&nbsp;x)</td><td class="diff_next"></td><td class="diff_header" id="to160_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;Relu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;max(0,&nbsp;x)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_35">35</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_35">35</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;Tanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;(1&nbsp;-&nbsp;e^{-2x})/(1&nbsp;+&nbsp;e^{-2x})</td><td class="diff_next"></td><td class="diff_header" id="to160_36">36</td><td nowrap="nowrap">&nbsp;&nbsp;Tanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;(1&nbsp;-&nbsp;e^{-2x})/(1&nbsp;+&nbsp;e^{-2x})</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_37">37</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_37">37</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Sigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;1/(1&nbsp;+&nbsp;e^{-x})</td><td class="diff_next"></td><td class="diff_header" id="to160_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;Sigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;1/(1&nbsp;+&nbsp;e^{-x})</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_39">39</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_39">39</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;(NOTE:&nbsp;Below&nbsp;are&nbsp;optional)</td><td class="diff_next"></td><td class="diff_header" id="to160_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;(NOTE:&nbsp;Below&nbsp;are&nbsp;optional)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_41">41</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_41">41</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;Affine(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*x&nbsp;+&nbsp;beta</td><td class="diff_next"></td><td class="diff_header" id="to160_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;Affine(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*x&nbsp;+&nbsp;beta</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_43">43</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_43">43</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;LeakyRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha&nbsp;*&nbsp;x</td><td class="diff_next"></td><td class="diff_header" id="to160_44">44</td><td nowrap="nowrap">&nbsp;&nbsp;LeakyRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha&nbsp;*&nbsp;x</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_45">45</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_45">45</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;ThresholdedRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;alpha&nbsp;else&nbsp;0</td><td class="diff_next"></td><td class="diff_header" id="to160_46">46</td><td nowrap="nowrap">&nbsp;&nbsp;ThresholdedRelu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;alpha&nbsp;else&nbsp;0</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_47">47</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_47">47</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;ScaledTanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*Tanh(beta*x)</td><td class="diff_next"></td><td class="diff_header" id="to160_48">48</td><td nowrap="nowrap">&nbsp;&nbsp;ScaledTanh(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;alpha*Tanh(beta*x)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_49">49</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_49">49</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_50">50</td><td nowrap="nowrap">&nbsp;&nbsp;HardSigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;min(max(alpha*x&nbsp;+&nbsp;beta,&nbsp;0),&nbsp;1)</td><td class="diff_next"></td><td class="diff_header" id="to160_50">50</td><td nowrap="nowrap">&nbsp;&nbsp;HardSigmoid(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;min(max(alpha*x&nbsp;+&nbsp;beta,&nbsp;0),&nbsp;1)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_51">51</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_51">51</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_52">52</td><td nowrap="nowrap">&nbsp;&nbsp;Elu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha*(e^x&nbsp;-&nbsp;1)</td><td class="diff_next"></td><td class="diff_header" id="to160_52">52</td><td nowrap="nowrap">&nbsp;&nbsp;Elu(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x&nbsp;if&nbsp;x&nbsp;&gt;=&nbsp;0&nbsp;else&nbsp;alpha*(e^x&nbsp;-&nbsp;1)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_53">53</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_53">53</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_54">54</td><td nowrap="nowrap">&nbsp;&nbsp;Softsign(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x/(1&nbsp;+&nbsp;|x|)</td><td class="diff_next"></td><td class="diff_header" id="to160_54">54</td><td nowrap="nowrap">&nbsp;&nbsp;Softsign(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;x/(1&nbsp;+&nbsp;|x|)</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to160__0"></td><td class="diff_header" id="from160_55">55</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_55">55</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_56">56</td><td nowrap="nowrap">&nbsp;&nbsp;Softplus(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;log(1&nbsp;+&nbsp;e^x)</td><td class="diff_next"></td><td class="diff_header" id="to160_56">56</td><td nowrap="nowrap">&nbsp;&nbsp;Softplus(x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;log(1&nbsp;+&nbsp;e^x)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_57">57</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_57">57</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_58">58</td><td nowrap="nowrap">Equations&nbsp;(Default:&nbsp;f=Tanh):</td><td class="diff_next"></td><td class="diff_header" id="to160_58">58</td><td nowrap="nowrap">Equations&nbsp;(Default:&nbsp;f=Tanh):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_59">59</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_59">59</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to160__1">n</a></td><td class="diff_header" id="from160_60">60</td><td nowrap="nowrap">&nbsp;&nbsp;-&nbsp;Ht&nbsp;=&nbsp;f(Xt*(Wi^T)&nbsp;+&nbsp;Ht-1*Ri&nbsp;+&nbsp;Wbi&nbsp;+&nbsp;Rbi)</td><td class="diff_next"><a href="#difflib_chg_to160__1">n</a></td><td class="diff_header" id="to160_60">60</td><td nowrap="nowrap">&nbsp;&nbsp;-&nbsp;Ht&nbsp;=&nbsp;f(Xt*(Wi^T)&nbsp;+&nbsp;Ht-1*<span class="diff_add">(</span>Ri<span class="diff_add">^T)</span>&nbsp;+&nbsp;Wbi&nbsp;+&nbsp;Rbi)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_61">61</td><td nowrap="nowrap"><span class="diff_add">This&nbsp;operator&nbsp;has&nbsp;**optional**&nbsp;inputs/outputs.&nbsp;See&nbsp;ONNX&nbsp;&lt;https://github.com/onnx/onnx/blob/master/docs/IR.md&gt;_&nbsp;for&nbsp;more&nbsp;details&nbsp;about&nbsp;the&nbsp;representation&nbsp;of&nbsp;optional&nbsp;arguments.&nbsp;An&nbsp;empty&nbsp;string&nbsp;may&nbsp;be&nbsp;used&nbsp;in&nbsp;the&nbsp;place&nbsp;of&nbsp;an&nbsp;actual&nbsp;argument's&nbsp;name&nbsp;to&nbsp;indicate&nbsp;a&nbsp;missing&nbsp;argument.&nbsp;Trailing&nbsp;optional&nbsp;arguments&nbsp;(those&nbsp;not&nbsp;followed&nbsp;by&nbsp;an&nbsp;argument&nbsp;that&nbsp;is&nbsp;present)&nbsp;may&nbsp;also&nbsp;be&nbsp;simply&nbsp;omitted.</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_61">61</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_62">62</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_62">62</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to160_63">63</td><td nowrap="nowrap">**Attributes**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_63">63</td><td nowrap="nowrap">*&nbsp;**activation_alpha**:</td><td class="diff_next"></td><td class="diff_header" id="to160_64">64</td><td nowrap="nowrap">*&nbsp;**activation_alpha**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_64">64</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td><td class="diff_next"></td><td class="diff_header" id="to160_65">65</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_65">65</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td><td class="diff_next"></td><td class="diff_header" id="to160_66">66</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_66">66</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to160_67">67</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_67">67</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.For&nbsp;example&nbsp;with&nbsp;LeakyRelu,&nbsp;the&nbsp;default</td><td class="diff_next"></td><td class="diff_header" id="to160_68">68</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.For&nbsp;example&nbsp;with&nbsp;LeakyRelu,&nbsp;the&nbsp;default</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_68">68</td><td nowrap="nowrap">&nbsp;&nbsp;alpha&nbsp;is&nbsp;0.01.</td><td class="diff_next"></td><td class="diff_header" id="to160_69">69</td><td nowrap="nowrap">&nbsp;&nbsp;alpha&nbsp;is&nbsp;0.01.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_69">69</td><td nowrap="nowrap">*&nbsp;**activation_beta**:</td><td class="diff_next"></td><td class="diff_header" id="to160_70">70</td><td nowrap="nowrap">*&nbsp;**activation_beta**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_70">70</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td><td class="diff_next"></td><td class="diff_header" id="to160_71">71</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;scaling&nbsp;values&nbsp;used&nbsp;by&nbsp;some&nbsp;activation&nbsp;functions.&nbsp;The</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_71">71</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td><td class="diff_next"></td><td class="diff_header" id="to160_72">72</td><td nowrap="nowrap">&nbsp;&nbsp;values&nbsp;are&nbsp;consumed&nbsp;in&nbsp;the&nbsp;order&nbsp;of&nbsp;activation&nbsp;functions,&nbsp;for</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_72">72</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to160_73">73</td><td nowrap="nowrap">&nbsp;&nbsp;example&nbsp;(f,&nbsp;g,&nbsp;h)&nbsp;in&nbsp;LSTM.&nbsp;Default&nbsp;values&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_73">73</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.</td><td class="diff_next"></td><td class="diff_header" id="to160_74">74</td><td nowrap="nowrap">&nbsp;&nbsp;corresponding&nbsp;ONNX&nbsp;operators.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_74">74</td><td nowrap="nowrap">*&nbsp;**activations**:</td><td class="diff_next"></td><td class="diff_header" id="to160_75">75</td><td nowrap="nowrap">*&nbsp;**activations**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_75">75</td><td nowrap="nowrap">&nbsp;&nbsp;One&nbsp;(or&nbsp;two&nbsp;if&nbsp;bidirectional)&nbsp;activation&nbsp;function&nbsp;for&nbsp;input&nbsp;gate.</td><td class="diff_next"></td><td class="diff_header" id="to160_76">76</td><td nowrap="nowrap">&nbsp;&nbsp;One&nbsp;(or&nbsp;two&nbsp;if&nbsp;bidirectional)&nbsp;activation&nbsp;function&nbsp;for&nbsp;input&nbsp;gate.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_76">76</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;activation&nbsp;function&nbsp;must&nbsp;be&nbsp;one&nbsp;of&nbsp;the&nbsp;activation&nbsp;functions</td><td class="diff_next"></td><td class="diff_header" id="to160_77">77</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;activation&nbsp;function&nbsp;must&nbsp;be&nbsp;one&nbsp;of&nbsp;the&nbsp;activation&nbsp;functions</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_77">77</td><td nowrap="nowrap">&nbsp;&nbsp;specified&nbsp;above.&nbsp;Optional:&nbsp;Default&nbsp;Tanh&nbsp;if&nbsp;not&nbsp;specified.</td><td class="diff_next"></td><td class="diff_header" id="to160_78">78</td><td nowrap="nowrap">&nbsp;&nbsp;specified&nbsp;above.&nbsp;Optional:&nbsp;Default&nbsp;Tanh&nbsp;if&nbsp;not&nbsp;specified.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_78">78</td><td nowrap="nowrap">*&nbsp;**clip**:</td><td class="diff_next"></td><td class="diff_header" id="to160_79">79</td><td nowrap="nowrap">*&nbsp;**clip**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_79">79</td><td nowrap="nowrap">&nbsp;&nbsp;Cell&nbsp;clip&nbsp;threshold.&nbsp;Clipping&nbsp;bounds&nbsp;the&nbsp;elements&nbsp;of&nbsp;a&nbsp;tensor&nbsp;in&nbsp;the</td><td class="diff_next"></td><td class="diff_header" id="to160_80">80</td><td nowrap="nowrap">&nbsp;&nbsp;Cell&nbsp;clip&nbsp;threshold.&nbsp;Clipping&nbsp;bounds&nbsp;the&nbsp;elements&nbsp;of&nbsp;a&nbsp;tensor&nbsp;in&nbsp;the</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_80">80</td><td nowrap="nowrap">&nbsp;&nbsp;range&nbsp;of&nbsp;[-threshold,&nbsp;+threshold]&nbsp;and&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;input&nbsp;of</td><td class="diff_next"></td><td class="diff_header" id="to160_81">81</td><td nowrap="nowrap">&nbsp;&nbsp;range&nbsp;of&nbsp;[-threshold,&nbsp;+threshold]&nbsp;and&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;input&nbsp;of</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_81">81</td><td nowrap="nowrap">&nbsp;&nbsp;activations.&nbsp;No&nbsp;clip&nbsp;if&nbsp;not&nbsp;specified.</td><td class="diff_next"></td><td class="diff_header" id="to160_82">82</td><td nowrap="nowrap">&nbsp;&nbsp;activations.&nbsp;No&nbsp;clip&nbsp;if&nbsp;not&nbsp;specified.</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to160__1"></td><td class="diff_header" id="from160_82">82</td><td nowrap="nowrap">*&nbsp;**direction**:</td><td class="diff_next"></td><td class="diff_header" id="to160_83">83</td><td nowrap="nowrap">*&nbsp;**direction**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_83">83</td><td nowrap="nowrap">&nbsp;&nbsp;Specify&nbsp;if&nbsp;the&nbsp;RNN&nbsp;is&nbsp;forward,&nbsp;reverse,&nbsp;or&nbsp;bidirectional.&nbsp;Must&nbsp;be</td><td class="diff_next"></td><td class="diff_header" id="to160_84">84</td><td nowrap="nowrap">&nbsp;&nbsp;Specify&nbsp;if&nbsp;the&nbsp;RNN&nbsp;is&nbsp;forward,&nbsp;reverse,&nbsp;or&nbsp;bidirectional.&nbsp;Must&nbsp;be</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_84">84</td><td nowrap="nowrap">&nbsp;&nbsp;one&nbsp;of&nbsp;forward&nbsp;(default),&nbsp;reverse,&nbsp;or&nbsp;bidirectional.</td><td class="diff_next"></td><td class="diff_header" id="to160_85">85</td><td nowrap="nowrap">&nbsp;&nbsp;one&nbsp;of&nbsp;forward&nbsp;(default),&nbsp;reverse,&nbsp;or&nbsp;bidirectional.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_85">85</td><td nowrap="nowrap">*&nbsp;**hidden_size**:</td><td class="diff_next"></td><td class="diff_header" id="to160_86">86</td><td nowrap="nowrap">*&nbsp;**hidden_size**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_86">86</td><td nowrap="nowrap">&nbsp;&nbsp;Number&nbsp;of&nbsp;neurons&nbsp;in&nbsp;the&nbsp;hidden&nbsp;layer</td><td class="diff_next"></td><td class="diff_header" id="to160_87">87</td><td nowrap="nowrap">&nbsp;&nbsp;Number&nbsp;of&nbsp;neurons&nbsp;in&nbsp;the&nbsp;hidden&nbsp;layer</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to160__2">n</a></td><td class="diff_header" id="from160_87">87</td><td nowrap="nowrap"><span class="diff_sub">*&nbsp;**output_sequence**:</span></td><td class="diff_next"><a href="#difflib_chg_to160__2">n</a></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_88">88</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;The&nbsp;sequence&nbsp;output&nbsp;for&nbsp;the&nbsp;hidden&nbsp;is&nbsp;optional&nbsp;if&nbsp;0.&nbsp;Default&nbsp;0.</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_89">89</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_88">88</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_90">90</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to160_89">89</td><td nowrap="nowrap">**Inputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_91">91</td><td nowrap="nowrap">Between&nbsp;3&nbsp;and&nbsp;6&nbsp;inputs.</td><td class="diff_next"></td><td class="diff_header" id="to160_90">90</td><td nowrap="nowrap">Between&nbsp;3&nbsp;and&nbsp;6&nbsp;inputs.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_92">92</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_91">91</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_93">93</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_92">92</td><td nowrap="nowrap">*&nbsp;**X**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_94">94</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;input&nbsp;sequences&nbsp;packed&nbsp;(and&nbsp;potentially&nbsp;padded)&nbsp;into&nbsp;one&nbsp;3-D</td><td class="diff_next"></td><td class="diff_header" id="to160_93">93</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;input&nbsp;sequences&nbsp;packed&nbsp;(and&nbsp;potentially&nbsp;padded)&nbsp;into&nbsp;one&nbsp;3-D</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_95">95</td><td nowrap="nowrap">&nbsp;&nbsp;tensor&nbsp;with&nbsp;the&nbsp;shape&nbsp;of&nbsp;[seq_length,&nbsp;batch_size,&nbsp;input_size].</td><td class="diff_next"></td><td class="diff_header" id="to160_94">94</td><td nowrap="nowrap">&nbsp;&nbsp;tensor&nbsp;with&nbsp;the&nbsp;shape&nbsp;of&nbsp;[seq_length,&nbsp;batch_size,&nbsp;input_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_96">96</td><td nowrap="nowrap">*&nbsp;**W**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_95">95</td><td nowrap="nowrap">*&nbsp;**W**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_97">97</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;weight&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;Wi&nbsp;and&nbsp;WBi</td><td class="diff_next"></td><td class="diff_header" id="to160_96">96</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;weight&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;Wi&nbsp;and&nbsp;WBi</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_98">98</td><td nowrap="nowrap">&nbsp;&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,</td><td class="diff_next"></td><td class="diff_header" id="to160_97">97</td><td nowrap="nowrap">&nbsp;&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_99">99</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size,&nbsp;input_size].</td><td class="diff_next"></td><td class="diff_header" id="to160_98">98</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size,&nbsp;input_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_100">100</td><td nowrap="nowrap">*&nbsp;**R**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_99">99</td><td nowrap="nowrap">*&nbsp;**R**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_101">101</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;recurrence&nbsp;weight&nbsp;tensor.&nbsp;Concatenation&nbsp;of&nbsp;Ri&nbsp;and&nbsp;RBi&nbsp;(if</td><td class="diff_next"></td><td class="diff_header" id="to160_100">100</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;recurrence&nbsp;weight&nbsp;tensor.&nbsp;Concatenation&nbsp;of&nbsp;Ri&nbsp;and&nbsp;RBi&nbsp;(if</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_102">102</td><td nowrap="nowrap">&nbsp;&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;hidden_size,</td><td class="diff_next"></td><td class="diff_header" id="to160_101">101</td><td nowrap="nowrap">&nbsp;&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;hidden_size,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_103">103</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to160_102">102</td><td nowrap="nowrap">&nbsp;&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_104">104</td><td nowrap="nowrap">*&nbsp;**B**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_103">103</td><td nowrap="nowrap">*&nbsp;**B**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_105">105</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;bias&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;[Wbi,&nbsp;Rbi]&nbsp;and</td><td class="diff_next"></td><td class="diff_header" id="to160_104">104</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;bias&nbsp;tensor&nbsp;for&nbsp;input&nbsp;gate.&nbsp;Concatenation&nbsp;of&nbsp;[Wbi,&nbsp;Rbi]&nbsp;and</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_106">106</td><td nowrap="nowrap">&nbsp;&nbsp;[WBbi,&nbsp;RBbi]&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape</td><td class="diff_next"></td><td class="diff_header" id="to160_105">105</td><td nowrap="nowrap">&nbsp;&nbsp;[WBbi,&nbsp;RBbi]&nbsp;(if&nbsp;bidirectional).&nbsp;The&nbsp;tensor&nbsp;has&nbsp;shape</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_107">107</td><td nowrap="nowrap">&nbsp;&nbsp;[num_directions,&nbsp;2*hidden_size].&nbsp;Optional:&nbsp;If&nbsp;not&nbsp;specified&nbsp;-</td><td class="diff_next"></td><td class="diff_header" id="to160_106">106</td><td nowrap="nowrap">&nbsp;&nbsp;[num_directions,&nbsp;2*hidden_size].&nbsp;Optional:&nbsp;If&nbsp;not&nbsp;specified&nbsp;-</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_108">108</td><td nowrap="nowrap">&nbsp;&nbsp;assumed&nbsp;to&nbsp;be&nbsp;0.</td><td class="diff_next"></td><td class="diff_header" id="to160_107">107</td><td nowrap="nowrap">&nbsp;&nbsp;assumed&nbsp;to&nbsp;be&nbsp;0.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_109">109</td><td nowrap="nowrap">*&nbsp;**sequence_lens**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T1**:</td><td class="diff_next"></td><td class="diff_header" id="to160_108">108</td><td nowrap="nowrap">*&nbsp;**sequence_lens**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T1**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_110">110</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;tensor&nbsp;specifying&nbsp;lengths&nbsp;of&nbsp;the&nbsp;sequences&nbsp;in&nbsp;a&nbsp;batch.&nbsp;If</td><td class="diff_next"></td><td class="diff_header" id="to160_109">109</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;tensor&nbsp;specifying&nbsp;lengths&nbsp;of&nbsp;the&nbsp;sequences&nbsp;in&nbsp;a&nbsp;batch.&nbsp;If</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_111">111</td><td nowrap="nowrap">&nbsp;&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;all&nbsp;sequences&nbsp;in&nbsp;the&nbsp;batch&nbsp;to&nbsp;have&nbsp;length</td><td class="diff_next"></td><td class="diff_header" id="to160_110">110</td><td nowrap="nowrap">&nbsp;&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;all&nbsp;sequences&nbsp;in&nbsp;the&nbsp;batch&nbsp;to&nbsp;have&nbsp;length</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_112">112</td><td nowrap="nowrap">&nbsp;&nbsp;seq_length.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[batch_size].</td><td class="diff_next"></td><td class="diff_header" id="to160_111">111</td><td nowrap="nowrap">&nbsp;&nbsp;seq_length.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[batch_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_113">113</td><td nowrap="nowrap">*&nbsp;**initial_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_112">112</td><td nowrap="nowrap">*&nbsp;**initial_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_114">114</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;initial&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;If&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;to</td><td class="diff_next"></td><td class="diff_header" id="to160_113">113</td><td nowrap="nowrap">&nbsp;&nbsp;Optional&nbsp;initial&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;If&nbsp;not&nbsp;specified&nbsp;-&nbsp;assumed&nbsp;to</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_115">115</td><td nowrap="nowrap">&nbsp;&nbsp;be&nbsp;0.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;batch_size,&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to160_114">114</td><td nowrap="nowrap">&nbsp;&nbsp;be&nbsp;0.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,&nbsp;batch_size,&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_116">116</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_115">115</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_117">117</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to160_116">116</td><td nowrap="nowrap">**Outputs**</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to160__2"></td><td class="diff_header" id="from160_118">118</td><td nowrap="nowrap">Between&nbsp;0&nbsp;and&nbsp;2&nbsp;outputs.</td><td class="diff_next"></td><td class="diff_header" id="to160_117">117</td><td nowrap="nowrap">Between&nbsp;0&nbsp;and&nbsp;2&nbsp;outputs.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_119">119</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_118">118</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_120">120</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_119">119</td><td nowrap="nowrap">*&nbsp;**Y**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_121">121</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;tensor&nbsp;that&nbsp;concats&nbsp;all&nbsp;the&nbsp;intermediate&nbsp;output&nbsp;values&nbsp;of&nbsp;the</td><td class="diff_next"></td><td class="diff_header" id="to160_120">120</td><td nowrap="nowrap">&nbsp;&nbsp;A&nbsp;tensor&nbsp;that&nbsp;concats&nbsp;all&nbsp;the&nbsp;intermediate&nbsp;output&nbsp;values&nbsp;of&nbsp;the</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_122">122</td><td nowrap="nowrap">&nbsp;&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[seq_length,&nbsp;num_directions,&nbsp;batch_size,</td><td class="diff_next"></td><td class="diff_header" id="to160_121">121</td><td nowrap="nowrap">&nbsp;&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[seq_length,&nbsp;num_directions,&nbsp;batch_size,</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to160__top">t</a></td><td class="diff_header" id="from160_123">123</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;hidden_size].&nbsp;It&nbsp;is&nbsp;optional&nbsp;if&nbsp;output_sequence&nbsp;is&nbsp;0.</span></td><td class="diff_next"><a href="#difflib_chg_to160__top">t</a></td><td class="diff_header" id="to160_122">122</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;hidden_size].</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_124">124</td><td nowrap="nowrap">*&nbsp;**Y_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to160_123">123</td><td nowrap="nowrap">*&nbsp;**Y_h**&nbsp;(optional,&nbsp;heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_125">125</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;last&nbsp;output&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,</td><td class="diff_next"></td><td class="diff_header" id="to160_124">124</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;last&nbsp;output&nbsp;value&nbsp;of&nbsp;the&nbsp;hidden.&nbsp;It&nbsp;has&nbsp;shape&nbsp;[num_directions,</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_126">126</td><td nowrap="nowrap">&nbsp;&nbsp;batch_size,&nbsp;hidden_size].</td><td class="diff_next"></td><td class="diff_header" id="to160_125">125</td><td nowrap="nowrap">&nbsp;&nbsp;batch_size,&nbsp;hidden_size].</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_127">127</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to160_126">126</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_128">128</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to160_127">127</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_129">129</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to160_128">128</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_130">130</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to160_129">129</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_131">131</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to160_130">130</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_132">132</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to160_131">131</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_133">133</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to160_132">132</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_134">134</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to160_133">133</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_135">135</td><td nowrap="nowrap">*&nbsp;**T1**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to160_134">134</td><td nowrap="nowrap">*&nbsp;**T1**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_136">136</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32)</td><td class="diff_next"></td><td class="diff_header" id="to160_135">135</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(int32)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_137">137</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to160_136">136</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from160_138">138</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;seq_lens&nbsp;to&nbsp;integer&nbsp;tensor.</td><td class="diff_next"></td><td class="diff_header" id="to160_137">137</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;seq_lens&nbsp;to&nbsp;integer&nbsp;tensor.</td></tr>
            </tbody>
        </table>

.. _l-onnx-op-rnn-1:
RNN - 1
=======
**Version**
* **name**: `RNN (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#RNN>`_
* **domain**: **main**
* **since_version**: **1**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 1**.

**Summary**

Computes an one-layer simple RNN. This operator is usually supported
via some custom implementation such as CuDNN.

Notations:

`X` - input tensor

`i` - input gate

`t` - time step (t-1 means previous time step)

`Wi` - W parameter weight matrix for input gate

`Ri` - R recurrence weight matrix for input gate

`Wbi` - W parameter bias vector for input gate

`Rbi` - R parameter bias vector for input gate

`WBi` - W parameter weight matrix for backward input gate

`RBi` - R recurrence weight matrix for backward input gate

`WBbi` - WR bias vectors for backward input gate

`RBbi` - RR bias vectors for backward input gate

`H` - Hidden state

`num_directions` - 2 if direction == bidirectional else 1

Activation functions:

  Relu(x)                - max(0, x)

  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})

  Sigmoid(x)             - 1/(1 + e^{-x})

  (NOTE: Below are optional)

  Affine(x)              - alpha*x + beta

  LeakyRelu(x)           - x if x >= 0 else alpha * x

  ThresholdedRelu(x)     - x if x >= alpha else 0

  ScaledTanh(x)          - alpha*Tanh(beta*x)

  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)

  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)

  Softsign(x)            - x/(1 + |x|)

  Softplus(x)            - log(1 + e^x)

Equations (Default: f=Tanh):

  - Ht = f(Xt*(Wi^T) + Ht-1*Ri + Wbi + Rbi)

**Attributes**
* **activation_alpha**:
  Optional scaling values used by some activation functions. The
  values are consumed in the order of activation functions, for
  example (f, g, h) in LSTM. Default values are the same as of
  corresponding ONNX operators.For example with LeakyRelu, the default
  alpha is 0.01.
* **activation_beta**:
  Optional scaling values used by some activation functions. The
  values are consumed in the order of activation functions, for
  example (f, g, h) in LSTM. Default values are the same as of
  corresponding ONNX operators.
* **activations**:
  One (or two if bidirectional) activation function for input gate.
  The activation function must be one of the activation functions
  specified above. Optional: Default `Tanh` if not specified.
* **clip**:
  Cell clip threshold. Clipping bounds the elements of a tensor in the
  range of [-threshold, +threshold] and is applied to the input of
  activations. No clip if not specified.
* **direction**:
  Specify if the RNN is forward, reverse, or bidirectional. Must be
  one of forward (default), reverse, or bidirectional.
* **hidden_size**:
  Number of neurons in the hidden layer
* **output_sequence**:
  The sequence output for the hidden is optional if 0. Default 0.

**Inputs**
Between 3 and 6 inputs.

* **X** (heterogeneous) - **T**:
  The input sequences packed (and potentially padded) into one 3-D
  tensor with the shape of `[seq_length, batch_size, input_size]`.
* **W** (heterogeneous) - **T**:
  The weight tensor for input gate. Concatenation of `Wi` and `WBi`
  (if bidirectional). The tensor has shape `[num_directions,
  hidden_size, input_size]`.
* **R** (heterogeneous) - **T**:
  The recurrence weight tensor. Concatenation of `Ri` and `RBi` (if
  bidirectional). The tensor has shape `[num_directions, hidden_size,
  hidden_size]`.
* **B** (optional, heterogeneous) - **T**:
  The bias tensor for input gate. Concatenation of `[Wbi, Rbi]` and
  `[WBbi, RBbi]` (if bidirectional). The tensor has shape
  `[num_directions, 2*hidden_size]`. Optional: If not specified -
  assumed to be 0.
* **sequence_lens** (optional, heterogeneous) - **T1**:
  Optional tensor specifying lengths of the sequences in a batch. If
  not specified - assumed all sequences in the batch to have length
  `seq_length`. It has shape `[batch_size]`.
* **initial_h** (optional, heterogeneous) - **T**:
  Optional initial value of the hidden. If not specified - assumed to
  be 0. It has shape `[num_directions, batch_size, hidden_size]`.

**Outputs**
Between 0 and 2 outputs.

* **Y** (optional, heterogeneous) - **T**:
  A tensor that concats all the intermediate output values of the
  hidden. It has shape `[seq_length, num_directions, batch_size,
  hidden_size]`. It is optional if `output_sequence` is 0.
* **Y_h** (optional, heterogeneous) - **T**:
  The last output value of the hidden. It has shape `[num_directions,
  batch_size, hidden_size]`.

**Type Constraints**
* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.
* **T1** in (
  tensor(int32)
  ):
  Constrain seq_lens to integer tensor.
