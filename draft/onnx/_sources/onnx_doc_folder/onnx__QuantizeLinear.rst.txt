
.. _l-onnx-doc-QuantizeLinear:

==============
QuantizeLinear
==============

.. contents::
    :local:


.. _l-onnx-op-quantizelinear-13:

QuantizeLinear - 13
===================

**Version**

* **name**: `QuantizeLinear (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#QuantizeLinear>`_
* **domain**: **main**
* **since_version**: **13**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 13**.

**Summary**

The linear quantization operator. It consumes a high precision tensor, a scale, and a zero point to compute the low precision / quantized tensor.
The scale factor and zero point must have same shape, and can be either a scalar for per-tensor / per layer quantization, or a 1-D tensor for per-axis quantization.
The quantization formula is y = saturate ((x / y_scale) + y_zero_point).
For saturation, it saturates to [0, 255] if it&#39;s uint8, or [-128, 127] if it&#39;s int8.
For (x / y_scale), it&#39;s rounding to nearest ties to even. Refer to https://en.wikipedia.org/wiki/Rounding for details. &#39;y_zero_point&#39; and &#39;y&#39; must have same type.

**Attributes**

* **axis**:
  (Optional) The axis of the quantization dimension of the input
  tensor. Ignored for per-tensor quantization. Negative value means
  counting dimensions from the back. Accepted range is [-r, r-1] where
  r = rank(input).

**Inputs**

Between 2 and 3 inputs.

* **x** (heterogeneous) - **T1**:
  N-D full precision Input tensor to be quantized.
* **y_scale** (heterogeneous) - **tensor(float)**:
  Scale for doing quantization to get &#39;y&#39;. It can be a scalar, which
  means per-tensor/layer quantization, or a 1-D Tensor for per-axis
  quantization.
* **y_zero_point** (optional, heterogeneous) - **T2**:
  Zero point for doing quantization to get &#39;y&#39;. Shape must match
  y_scale. Default is uint8 with zero point of 0 if it&#39;s not
  specified.

**Outputs**

* **y** (heterogeneous) - **T2**:
  N-D quantized output tensor. It has same shape as input &#39;x&#39;.

**Type Constraints**

* **T1** in (
  tensor(float),
  tensor(int32)
  ):
  Constrain &#39;x&#39; to float or int32 tensor.
* **T2** in (
  tensor(int8),
  tensor(uint8)
  ):
  Constrain &#39;y_zero_point&#39; and &#39;y&#39; to 8-bit integer tensor.

**Examples**

**default**

::

    import numpy as np
    import onnx

    node = onnx.helper.make_node(
        &#34;QuantizeLinear&#34;,
        inputs=[&#34;x&#34;, &#34;y_scale&#34;, &#34;y_zero_point&#34;],
        outputs=[&#34;y&#34;],
    )

    x = np.array([0, 2, 3, 1000, -254, -1000]).astype(np.float32)
    y_scale = np.float32(2)
    y_zero_point = np.uint8(128)
    y = np.array([128, 129, 130, 255, 1, 0]).astype(np.uint8)

    expect(
        node,
        inputs=[x, y_scale, y_zero_point],
        outputs=[y],
        name=&#34;test_quantizelinear&#34;,
    )

**_axis**

::

    import numpy as np
    import onnx

    node = onnx.helper.make_node(
        &#34;QuantizeLinear&#34;,
        inputs=[&#34;x&#34;, &#34;y_scale&#34;, &#34;y_zero_point&#34;],
        outputs=[&#34;y&#34;],
    )

    x = np.array(
        [
            [
                [[-162, 10], [-100, 232], [-20, -50]],
                [[-76, 0], [0, 252], [32, -44]],
                [[245, -485], [-960, -270], [-375, -470]],
            ],
        ],
        dtype=np.float32,
    )
    y_scale = np.array([2, 4, 5], dtype=np.float32)
    y_zero_point = np.array([84, 24, 196], dtype=np.uint8)
    y = (x / y_scale.reshape(1, 3, 1, 1) + y_zero_point.reshape(1, 3, 1, 1)).astype(
        np.uint8
    )

    expect(
        node,
        inputs=[x, y_scale, y_zero_point],
        outputs=[y],
        name=&#34;test_quantizelinear_axis&#34;,
    )

.. toctree::

    text_diff_QuantizeLinear_10_13

.. _l-onnx-op-quantizelinear-10:

QuantizeLinear - 10
===================

**Version**

* **name**: `QuantizeLinear (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#QuantizeLinear>`_
* **domain**: **main**
* **since_version**: **10**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 10**.

**Summary**

The linear per-tensor/layer quantization operator. It consumes a high precision tensor, a scale, a zero point to compute the low precision / quantized tensor.
The quantization formula is y = saturate ((x / y_scale) + y_zero_point). For saturation, it saturates to [0, 255] if it&#39;s uint8, or [-128, 127] if it&#39;s int8.
For (x / y_scale), it&#39;s rounding to nearest ties to even. Refer to https://en.wikipedia.org/wiki/Rounding for details. &#39;y_zero_point&#39; and &#39;y&#39; must have same type.

**Inputs**

Between 2 and 3 inputs.

* **x** (heterogeneous) - **T1**:
  N-D full precision Input tensor to be quantized.
* **y_scale** (heterogeneous) - **tensor(float)**:
  Scale for doing quantization to get &#39;y&#39;. It&#39;s a scalar, which means
  a per-tensor/layer quantization.
* **y_zero_point** (optional, heterogeneous) - **T2**:
  Zero point for doing quantization to get &#39;y&#39;. It&#39;s a scalar, which
  means a per-tensor/layer quantization. Default value is uint8 typed
  0 if it&#39;s not specified.

**Outputs**

* **y** (heterogeneous) - **T2**:
  N-D quantized output tensor. It has same shape as input &#39;x&#39;.

**Type Constraints**

* **T1** in (
  tensor(float),
  tensor(int32)
  ):
  Constrain &#39;x&#39; to float or int32 tensor.
* **T2** in (
  tensor(int8),
  tensor(uint8)
  ):
  Constrain &#39;y_zero_point&#39; and &#39;y&#39; to 8-bit integer tensor.
