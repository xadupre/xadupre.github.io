
.. _l-onnx-doc-LogSoftmax:

==========
LogSoftmax
==========

.. contents::
    :local:


.. _l-onnx-op-logsoftmax-13:

LogSoftmax - 13
===============

**Version**

* **name**: `LogSoftmax (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#LogSoftmax>`_
* **domain**: **main**
* **since_version**: **13**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 13**.

**Summary**

The operator computes the log of softmax values for the given input:

 LogSoftmax(input, axis) = Log(Softmax(input, axis=axis))

The "axis" attribute indicates the dimension along which LogSoftmax
will be performed. The output tensor has the same shape
and contains the LogSoftmax values of the corresponding input.

**Attributes**

* **axis**:
   Describes the dimension LogSoftmax will be performed on. Negative
  value means counting dimensions from the back. Accepted range is
  [-r, r-1] where r = rank(input).

**Inputs**

* **input** (heterogeneous) - **T**:
  The input tensor of rank >= axis.

**Outputs**

* **output** (heterogeneous) - **T**:
  The output values with the same shape as the input tensor.

**Type Constraints**

* **T** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.

**Examples**

**default**

::

    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
    )
    x = np.array([[-1, 0, 1]]).astype(np.float32)
    # expected output
    # [[-2.4076061 -1.407606  -0.407606 ]]
    y = logsoftmax(x)
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_example_1")

**_logsoftmax_axis**

::

    x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)
    # expected output
    # [[-3.4401896  -2.4401896  -1.4401896  -0.44018966]
    # [-3.4401896  -2.4401896  -1.4401896  -0.44018966]]
    y = logsoftmax(x)

    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
    )
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_large_number")

    x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))
    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
        axis=0,
    )
    y = logsoftmax(x, axis=0)
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_axis_0")

    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
        axis=1,
    )
    y = logsoftmax(x, axis=1)
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_axis_1")

    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
        axis=2,
    )
    y = logsoftmax(x, axis=2)
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_axis_2")

    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
        axis=-1,
    )
    y = logsoftmax(x, axis=-1)
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_negative_axis")

    # default axis is -1
    node = onnx.helper.make_node(
        "LogSoftmax",
        inputs=["x"],
        outputs=["y"],
    )
    expect(node, inputs=[x], outputs=[y], name="test_logsoftmax_default_axis")

**Differences**

.. raw:: html

        <table class="diff" id="difflib_chg_to106__top"
               cellspacing="0" cellpadding="0" rules="groups" >
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

            <tbody>
                <tr><td class="diff_next" id="difflib_chg_to106__1"><a href="#difflib_chg_to106__1">n</a></td><td class="diff_header" id="from106_1">1</td><td nowrap="nowrap"><span class="diff_sub">The&nbsp;operator&nbsp;computes&nbsp;the&nbsp;logsoftmax&nbsp;(log&nbsp;of&nbsp;softmax)&nbsp;values&nbsp;for&nbsp;each&nbsp;layer&nbsp;in&nbsp;the&nbsp;batch</span></td><td class="diff_next"><a href="#difflib_chg_to106__1">n</a></td><td class="diff_header" id="to106_1">1</td><td nowrap="nowrap"><span class="diff_add">The&nbsp;operator&nbsp;computes&nbsp;the&nbsp;log&nbsp;of&nbsp;softmax&nbsp;values&nbsp;for&nbsp;the&nbsp;given&nbsp;input:</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_2">2</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;of&nbsp;the&nbsp;given&nbsp;input.</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_3">3</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_2">2</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to106__2">n</a></td><td class="diff_header" id="from106_4">4</td><td nowrap="nowrap"><span class="diff_sub">The&nbsp;input&nbsp;does&nbsp;not&nbsp;need&nbsp;to&nbsp;explicitly&nbsp;be&nbsp;a&nbsp;2D&nbsp;vector;&nbsp;rather,&nbsp;it&nbsp;will&nbsp;be</span></td><td class="diff_next"><a href="#difflib_chg_to106__2">n</a></td><td class="diff_header" id="to106_3">3</td><td nowrap="nowrap"><span class="diff_add">&nbsp;LogSoftmax(input,&nbsp;axis)&nbsp;=&nbsp;Log(Softmax(input,&nbsp;axis=axis))</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_5">5</td><td nowrap="nowrap"><span class="diff_sub">coerced&nbsp;into&nbsp;one.&nbsp;For&nbsp;an&nbsp;arbitrary&nbsp;n-dimensional&nbsp;tensor</span></td><td class="diff_next"></td><td class="diff_header" id="to106_4">4</td><td nowrap="nowrap"><span class="diff_add">&nbsp;</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_6">6</td><td nowrap="nowrap"><span class="diff_sub">input&nbsp;\in&nbsp;[a_0,&nbsp;a_1,&nbsp;...,&nbsp;a_{k-1},&nbsp;a_k,&nbsp;...,&nbsp;a_{n-1}]&nbsp;and&nbsp;k&nbsp;is</span></td><td class="diff_next"></td><td class="diff_header" id="to106_5">5</td><td nowrap="nowrap"><span class="diff_add">The&nbsp;"axis"&nbsp;attribute&nbsp;indicates&nbsp;the&nbsp;dimension&nbsp;along&nbsp;which&nbsp;LogSoftmax</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_7">7</td><td nowrap="nowrap"><span class="diff_sub">the&nbsp;axis&nbsp;provided,&nbsp;then&nbsp;input&nbsp;will&nbsp;be&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2-dimensional&nbsp;tensor&nbsp;with</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_8">8</td><td nowrap="nowrap"><span class="diff_sub">dimensions&nbsp;[a_0&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{k-1},&nbsp;a_k&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}].&nbsp;For&nbsp;the&nbsp;default</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_9">9</td><td nowrap="nowrap"><span class="diff_sub">case&nbsp;where&nbsp;axis=1,&nbsp;this&nbsp;means&nbsp;the&nbsp;input&nbsp;tensor&nbsp;will&nbsp;be&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2D&nbsp;tensor</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_10">10</td><td nowrap="nowrap"><span class="diff_sub">of&nbsp;dimensions&nbsp;[a_0,&nbsp;a_1&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}],&nbsp;where&nbsp;a_0&nbsp;is&nbsp;often&nbsp;the&nbsp;batch&nbsp;size.</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_11">11</td><td nowrap="nowrap"><span class="diff_sub">In&nbsp;this&nbsp;situation,&nbsp;we&nbsp;must&nbsp;have&nbsp;a_0&nbsp;=&nbsp;N&nbsp;and&nbsp;a_1&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}&nbsp;=&nbsp;D.</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_12">12</td><td nowrap="nowrap"><span class="diff_sub">Each&nbsp;of&nbsp;these&nbsp;dimensions&nbsp;must&nbsp;be&nbsp;matched&nbsp;correctly,&nbsp;or&nbsp;else&nbsp;the&nbsp;operator</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_13">13</td><td nowrap="nowrap">will&nbsp;<span class="diff_chg">throw&nbsp;</span>er<span class="diff_chg">r</span>or<span class="diff_chg">s</span>.&nbsp;The&nbsp;output&nbsp;tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;shape</td><td class="diff_next"></td><td class="diff_header" id="to106_6">6</td><td nowrap="nowrap">will&nbsp;<span class="diff_chg">be&nbsp;p</span>er<span class="diff_chg">f</span>or<span class="diff_chg">med</span>.&nbsp;The&nbsp;output&nbsp;tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;shape</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to106__2"></td><td class="diff_header" id="from106_14">14</td><td nowrap="nowrap">and&nbsp;contains&nbsp;the&nbsp;<span class="diff_chg">l</span>og<span class="diff_chg">s</span>oftmax&nbsp;values&nbsp;of&nbsp;the&nbsp;corresponding&nbsp;input.</td><td class="diff_next"></td><td class="diff_header" id="to106_7">7</td><td nowrap="nowrap">and&nbsp;contains&nbsp;the&nbsp;<span class="diff_chg">L</span>og<span class="diff_chg">S</span>oftmax&nbsp;values&nbsp;of&nbsp;the&nbsp;corresponding&nbsp;input.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_15">15</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_8">8</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_16">16</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to106_9">9</td><td nowrap="nowrap">**Attributes**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_17">17</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_10">10</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_18">18</td><td nowrap="nowrap">*&nbsp;**axis**:</td><td class="diff_next"></td><td class="diff_header" id="to106_11">11</td><td nowrap="nowrap">*&nbsp;**axis**:</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to106__3">n</a></td><td class="diff_header" id="from106_19">19</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;Describes&nbsp;the&nbsp;axis&nbsp;of&nbsp;the&nbsp;inputs&nbsp;when&nbsp;coerced&nbsp;to&nbsp;2D;&nbsp;defaults&nbsp;to&nbsp;one</span></td><td class="diff_next"><a href="#difflib_chg_to106__3">n</a></td><td class="diff_header" id="to106_12">12</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;&nbsp;Describes&nbsp;the&nbsp;dimension&nbsp;LogSoftmax&nbsp;will&nbsp;be&nbsp;performed&nbsp;on.&nbsp;Negative</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_20">20</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;because&nbsp;the&nbsp;0th&nbsp;axis&nbsp;most&nbsp;likely&nbsp;describes&nbsp;the&nbsp;batch_size.&nbsp;Negative</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_21">21</td><td nowrap="nowrap">&nbsp;&nbsp;value&nbsp;means&nbsp;counting&nbsp;dimensions&nbsp;from&nbsp;the&nbsp;back.&nbsp;Accepted&nbsp;range&nbsp;is</td><td class="diff_next"></td><td class="diff_header" id="to106_13">13</td><td nowrap="nowrap">&nbsp;&nbsp;value&nbsp;means&nbsp;counting&nbsp;dimensions&nbsp;from&nbsp;the&nbsp;back.&nbsp;Accepted&nbsp;range&nbsp;is</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to106__3"></td><td class="diff_header" id="from106_22">22</td><td nowrap="nowrap">&nbsp;&nbsp;[-r,&nbsp;r-1]&nbsp;where&nbsp;r&nbsp;=&nbsp;rank(input).</td><td class="diff_next"></td><td class="diff_header" id="to106_14">14</td><td nowrap="nowrap">&nbsp;&nbsp;[-r,&nbsp;r-1]&nbsp;where&nbsp;r&nbsp;=&nbsp;rank(input).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_23">23</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_15">15</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_24">24</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to106_16">16</td><td nowrap="nowrap">**Inputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_25">25</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_17">17</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_26">26</td><td nowrap="nowrap">*&nbsp;**input**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to106_18">18</td><td nowrap="nowrap">*&nbsp;**input**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to106__4">n</a></td><td class="diff_header" id="from106_27">27</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;The&nbsp;input&nbsp;tensor&nbsp;that's&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2D&nbsp;matrix&nbsp;of&nbsp;size&nbsp;(NxD)&nbsp;as</span></td><td class="diff_next"><a href="#difflib_chg_to106__4">n</a></td><td class="diff_header" id="to106_19">19</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;The&nbsp;input&nbsp;tensor&nbsp;of&nbsp;rank&nbsp;&gt;=&nbsp;axis.</span></td></tr>
                <tr><td class="diff_next" id="difflib_chg_to106__4"></td><td class="diff_header" id="from106_28">28</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;described&nbsp;above.</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_29">29</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_20">20</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_30">30</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to106_21">21</td><td nowrap="nowrap">**Outputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_31">31</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_22">22</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_32">32</td><td nowrap="nowrap">*&nbsp;**output**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to106_23">23</td><td nowrap="nowrap">*&nbsp;**output**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to106__5">n</a></td><td class="diff_header" id="from106_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;output&nbsp;values&nbsp;with&nbsp;the&nbsp;same&nbsp;shape&nbsp;as&nbsp;input&nbsp;tensor<span class="diff_chg">&nbsp;(the&nbsp;original</span></td><td class="diff_next"><a href="#difflib_chg_to106__5">n</a></td><td class="diff_header" id="to106_24">24</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;output&nbsp;values&nbsp;with&nbsp;the&nbsp;same&nbsp;shape&nbsp;as<span class="diff_add">&nbsp;the</span>&nbsp;input&nbsp;tensor<span class="diff_chg">.</span></td></tr>
                <tr><td class="diff_next" id="difflib_chg_to106__5"></td><td class="diff_header" id="from106_34">34</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;&nbsp;size&nbsp;without&nbsp;coercion).</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_35">35</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_25">25</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_36">36</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to106_26">26</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_37">37</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to106_27">27</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_38">38</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to106_28">28</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to106__top">t</a></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"><a href="#difflib_chg_to106__top">t</a></td><td class="diff_header" id="to106_29">29</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;tensor(bfloat16),</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_39">39</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to106_30">30</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to106_31">31</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_41">41</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to106_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to106_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from106_43">43</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to106_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
            </tbody>
        </table>

.. _l-onnx-op-logsoftmax-11:

LogSoftmax - 11
===============

**Version**

* **name**: `LogSoftmax (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#LogSoftmax>`_
* **domain**: **main**
* **since_version**: **11**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 11**.

**Summary**

The operator computes the logsoftmax (log of softmax) values for each layer in the batch
 of the given input.

The input does not need to explicitly be a 2D vector; rather, it will be
coerced into one. For an arbitrary n-dimensional tensor
input \in [a_0, a_1, ..., a_{k-1}, a_k, ..., a_{n-1}] and k is
the axis provided, then input will be coerced into a 2-dimensional tensor with
dimensions [a_0 * ... * a_{k-1}, a_k * ... * a_{n-1}]. For the default
case where axis=1, this means the input tensor will be coerced into a 2D tensor
of dimensions [a_0, a_1 * ... * a_{n-1}], where a_0 is often the batch size.
In this situation, we must have a_0 = N and a_1 * ... * a_{n-1} = D.
Each of these dimensions must be matched correctly, or else the operator
will throw errors. The output tensor has the same shape
and contains the logsoftmax values of the corresponding input.

**Attributes**

* **axis**:
  Describes the axis of the inputs when coerced to 2D; defaults to one
  because the 0th axis most likely describes the batch_size. Negative
  value means counting dimensions from the back. Accepted range is
  [-r, r-1] where r = rank(input).

**Inputs**

* **input** (heterogeneous) - **T**:
  The input tensor that's coerced into a 2D matrix of size (NxD) as
  described above.

**Outputs**

* **output** (heterogeneous) - **T**:
  The output values with the same shape as input tensor (the original
  size without coercion).

**Type Constraints**

* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.

**Differences**

.. raw:: html

        <table class="diff" id="difflib_chg_to107__top"
               cellspacing="0" cellpadding="0" rules="groups" >
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>
            <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>

            <tbody>
                <tr><td class="diff_next" id="difflib_chg_to107__1"><a href="#difflib_chg_to107__0">f</a></td><td class="diff_header" id="from107_1">1</td><td nowrap="nowrap">The&nbsp;operator&nbsp;computes&nbsp;the&nbsp;logsoftmax&nbsp;(log&nbsp;of&nbsp;softmax)&nbsp;values&nbsp;for&nbsp;each&nbsp;layer&nbsp;in&nbsp;the&nbsp;batch</td><td class="diff_next"><a href="#difflib_chg_to107__0">f</a></td><td class="diff_header" id="to107_1">1</td><td nowrap="nowrap">The&nbsp;operator&nbsp;computes&nbsp;the&nbsp;logsoftmax&nbsp;(log&nbsp;of&nbsp;softmax)&nbsp;values&nbsp;for&nbsp;each&nbsp;layer&nbsp;in&nbsp;the&nbsp;batch</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to107__1">n</a></td><td class="diff_header" id="from107_2">2</td><td nowrap="nowrap"><span class="diff_sub">&nbsp;of&nbsp;the&nbsp;given&nbsp;input.&nbsp;The&nbsp;input&nbsp;is&nbsp;a&nbsp;2-D&nbsp;tensor&nbsp;(Tensor&lt;float&gt;)&nbsp;of&nbsp;size</span></td><td class="diff_next"><a href="#difflib_chg_to107__1">n</a></td><td class="diff_header" id="to107_2">2</td><td nowrap="nowrap"><span class="diff_add">&nbsp;of&nbsp;the&nbsp;given&nbsp;input.</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_3">3</td><td nowrap="nowrap"><span class="diff_sub">(batch_size&nbsp;x&nbsp;input_feature_dimensions).&nbsp;The&nbsp;output&nbsp;tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;shape</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_4">4</td><td nowrap="nowrap"><span class="diff_sub">and&nbsp;contains&nbsp;the&nbsp;logsoftmax&nbsp;values&nbsp;of&nbsp;the&nbsp;corresponding&nbsp;input.</span></td><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_5">5</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_3">3</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to107__2">n</a></td><td class="diff_header" id="from107_6">6</td><td nowrap="nowrap"><span class="diff_chg">I</span>nput&nbsp;does&nbsp;not&nbsp;need&nbsp;to&nbsp;explicitly&nbsp;be&nbsp;a&nbsp;2D&nbsp;vector;&nbsp;rather,&nbsp;it&nbsp;will&nbsp;be</td><td class="diff_next"><a href="#difflib_chg_to107__2">n</a></td><td class="diff_header" id="to107_4">4</td><td nowrap="nowrap"><span class="diff_chg">The&nbsp;i</span>nput&nbsp;does&nbsp;not&nbsp;need&nbsp;to&nbsp;explicitly&nbsp;be&nbsp;a&nbsp;2D&nbsp;vector;&nbsp;rather,&nbsp;it&nbsp;will&nbsp;be</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_7">7</td><td nowrap="nowrap">coerced&nbsp;into&nbsp;one.&nbsp;For&nbsp;an&nbsp;arbitrary&nbsp;n-dimensional&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to107_5">5</td><td nowrap="nowrap">coerced&nbsp;into&nbsp;one.&nbsp;For&nbsp;an&nbsp;arbitrary&nbsp;n-dimensional&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_8">8</td><td nowrap="nowrap">input&nbsp;\in&nbsp;[a_0,&nbsp;a_1,&nbsp;...,&nbsp;a_{k-1},&nbsp;a_k,&nbsp;...,&nbsp;a_{n-1}]&nbsp;and&nbsp;k&nbsp;is</td><td class="diff_next"></td><td class="diff_header" id="to107_6">6</td><td nowrap="nowrap">input&nbsp;\in&nbsp;[a_0,&nbsp;a_1,&nbsp;...,&nbsp;a_{k-1},&nbsp;a_k,&nbsp;...,&nbsp;a_{n-1}]&nbsp;and&nbsp;k&nbsp;is</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_9">9</td><td nowrap="nowrap">the&nbsp;axis&nbsp;provided,&nbsp;then&nbsp;input&nbsp;will&nbsp;be&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2-dimensional&nbsp;tensor&nbsp;with</td><td class="diff_next"></td><td class="diff_header" id="to107_7">7</td><td nowrap="nowrap">the&nbsp;axis&nbsp;provided,&nbsp;then&nbsp;input&nbsp;will&nbsp;be&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2-dimensional&nbsp;tensor&nbsp;with</td></tr>
                <tr><td class="diff_next" id="difflib_chg_to107__2"></td><td class="diff_header" id="from107_10">10</td><td nowrap="nowrap">dimensions&nbsp;[a_0&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{k-1},&nbsp;a_k&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}].&nbsp;For&nbsp;the&nbsp;default</td><td class="diff_next"></td><td class="diff_header" id="to107_8">8</td><td nowrap="nowrap">dimensions&nbsp;[a_0&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{k-1},&nbsp;a_k&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}].&nbsp;For&nbsp;the&nbsp;default</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_11">11</td><td nowrap="nowrap">case&nbsp;where&nbsp;axis=1,&nbsp;this&nbsp;means&nbsp;the&nbsp;input&nbsp;tensor&nbsp;will&nbsp;be&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2D&nbsp;tensor</td><td class="diff_next"></td><td class="diff_header" id="to107_9">9</td><td nowrap="nowrap">case&nbsp;where&nbsp;axis=1,&nbsp;this&nbsp;means&nbsp;the&nbsp;input&nbsp;tensor&nbsp;will&nbsp;be&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2D&nbsp;tensor</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_12">12</td><td nowrap="nowrap">of&nbsp;dimensions&nbsp;[a_0,&nbsp;a_1&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}],&nbsp;where&nbsp;a_0&nbsp;is&nbsp;often&nbsp;the&nbsp;batch&nbsp;size.</td><td class="diff_next"></td><td class="diff_header" id="to107_10">10</td><td nowrap="nowrap">of&nbsp;dimensions&nbsp;[a_0,&nbsp;a_1&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}],&nbsp;where&nbsp;a_0&nbsp;is&nbsp;often&nbsp;the&nbsp;batch&nbsp;size.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_13">13</td><td nowrap="nowrap">In&nbsp;this&nbsp;situation,&nbsp;we&nbsp;must&nbsp;have&nbsp;a_0&nbsp;=&nbsp;N&nbsp;and&nbsp;a_1&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}&nbsp;=&nbsp;D.</td><td class="diff_next"></td><td class="diff_header" id="to107_11">11</td><td nowrap="nowrap">In&nbsp;this&nbsp;situation,&nbsp;we&nbsp;must&nbsp;have&nbsp;a_0&nbsp;=&nbsp;N&nbsp;and&nbsp;a_1&nbsp;*&nbsp;...&nbsp;*&nbsp;a_{n-1}&nbsp;=&nbsp;D.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_14">14</td><td nowrap="nowrap">Each&nbsp;of&nbsp;these&nbsp;dimensions&nbsp;must&nbsp;be&nbsp;matched&nbsp;correctly,&nbsp;or&nbsp;else&nbsp;the&nbsp;operator</td><td class="diff_next"></td><td class="diff_header" id="to107_12">12</td><td nowrap="nowrap">Each&nbsp;of&nbsp;these&nbsp;dimensions&nbsp;must&nbsp;be&nbsp;matched&nbsp;correctly,&nbsp;or&nbsp;else&nbsp;the&nbsp;operator</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to107__3">n</a></td><td class="diff_header" id="from107_15">15</td><td nowrap="nowrap"><span class="diff_sub">will&nbsp;throw&nbsp;errors.</span></td><td class="diff_next"><a href="#difflib_chg_to107__3">n</a></td><td class="diff_header" id="to107_13">13</td><td nowrap="nowrap"><span class="diff_add">will&nbsp;throw&nbsp;errors.&nbsp;The&nbsp;output&nbsp;tensor&nbsp;has&nbsp;the&nbsp;same&nbsp;shape</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_14">14</td><td nowrap="nowrap"><span class="diff_add">and&nbsp;contains&nbsp;the&nbsp;logsoftmax&nbsp;values&nbsp;of&nbsp;the&nbsp;corresponding&nbsp;input.</span></td></tr>
                <tr><td class="diff_next" id="difflib_chg_to107__3"></td><td class="diff_header" id="from107_16">16</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_15">15</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_17">17</td><td nowrap="nowrap">**Attributes**</td><td class="diff_next"></td><td class="diff_header" id="to107_16">16</td><td nowrap="nowrap">**Attributes**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_18">18</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_17">17</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_19">19</td><td nowrap="nowrap">*&nbsp;**axis**:</td><td class="diff_next"></td><td class="diff_header" id="to107_18">18</td><td nowrap="nowrap">*&nbsp;**axis**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;Describes&nbsp;the&nbsp;axis&nbsp;of&nbsp;the&nbsp;inputs&nbsp;when&nbsp;coerced&nbsp;to&nbsp;2D;&nbsp;defaults&nbsp;to&nbsp;one</td><td class="diff_next"></td><td class="diff_header" id="to107_19">19</td><td nowrap="nowrap">&nbsp;&nbsp;Describes&nbsp;the&nbsp;axis&nbsp;of&nbsp;the&nbsp;inputs&nbsp;when&nbsp;coerced&nbsp;to&nbsp;2D;&nbsp;defaults&nbsp;to&nbsp;one</td></tr>
                <tr><td class="diff_next"><a href="#difflib_chg_to107__top">t</a></td><td class="diff_header" id="from107_21">21</td><td nowrap="nowrap">&nbsp;&nbsp;because&nbsp;the&nbsp;0th&nbsp;axis&nbsp;most&nbsp;likely&nbsp;describes&nbsp;the&nbsp;batch_size</td><td class="diff_next"><a href="#difflib_chg_to107__top">t</a></td><td class="diff_header" id="to107_20">20</td><td nowrap="nowrap">&nbsp;&nbsp;because&nbsp;the&nbsp;0th&nbsp;axis&nbsp;most&nbsp;likely&nbsp;describes&nbsp;the&nbsp;batch_size<span class="diff_add">.&nbsp;Negative</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_21">21</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;value&nbsp;means&nbsp;counting&nbsp;dimensions&nbsp;from&nbsp;the&nbsp;back.&nbsp;Accepted&nbsp;range&nbsp;is</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header"></td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_22">22</td><td nowrap="nowrap"><span class="diff_add">&nbsp;&nbsp;[-r,&nbsp;r-1]&nbsp;where&nbsp;r&nbsp;=&nbsp;rank(input).</span></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_22">22</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_23">23</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_23">23</td><td nowrap="nowrap">**Inputs**</td><td class="diff_next"></td><td class="diff_header" id="to107_24">24</td><td nowrap="nowrap">**Inputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_24">24</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_25">25</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_25">25</td><td nowrap="nowrap">*&nbsp;**input**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to107_26">26</td><td nowrap="nowrap">*&nbsp;**input**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_26">26</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;input&nbsp;tensor&nbsp;that's&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2D&nbsp;matrix&nbsp;of&nbsp;size&nbsp;(NxD)&nbsp;as</td><td class="diff_next"></td><td class="diff_header" id="to107_27">27</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;input&nbsp;tensor&nbsp;that's&nbsp;coerced&nbsp;into&nbsp;a&nbsp;2D&nbsp;matrix&nbsp;of&nbsp;size&nbsp;(NxD)&nbsp;as</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_27">27</td><td nowrap="nowrap">&nbsp;&nbsp;described&nbsp;above.</td><td class="diff_next"></td><td class="diff_header" id="to107_28">28</td><td nowrap="nowrap">&nbsp;&nbsp;described&nbsp;above.</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_28">28</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_29">29</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_29">29</td><td nowrap="nowrap">**Outputs**</td><td class="diff_next"></td><td class="diff_header" id="to107_30">30</td><td nowrap="nowrap">**Outputs**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_30">30</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_31">31</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_31">31</td><td nowrap="nowrap">*&nbsp;**output**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td><td class="diff_next"></td><td class="diff_header" id="to107_32">32</td><td nowrap="nowrap">*&nbsp;**output**&nbsp;(heterogeneous)&nbsp;-&nbsp;**T**:</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_32">32</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;output&nbsp;values&nbsp;with&nbsp;the&nbsp;same&nbsp;shape&nbsp;as&nbsp;input&nbsp;tensor&nbsp;(the&nbsp;original</td><td class="diff_next"></td><td class="diff_header" id="to107_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;The&nbsp;output&nbsp;values&nbsp;with&nbsp;the&nbsp;same&nbsp;shape&nbsp;as&nbsp;input&nbsp;tensor&nbsp;(the&nbsp;original</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_33">33</td><td nowrap="nowrap">&nbsp;&nbsp;size&nbsp;without&nbsp;coercion).</td><td class="diff_next"></td><td class="diff_header" id="to107_34">34</td><td nowrap="nowrap">&nbsp;&nbsp;size&nbsp;without&nbsp;coercion).</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_34">34</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_35">35</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_35">35</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td><td class="diff_next"></td><td class="diff_header" id="to107_36">36</td><td nowrap="nowrap">**Type&nbsp;Constraints**</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_36">36</td><td nowrap="nowrap"></td><td class="diff_next"></td><td class="diff_header" id="to107_37">37</td><td nowrap="nowrap"></td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_37">37</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td><td class="diff_next"></td><td class="diff_header" id="to107_38">38</td><td nowrap="nowrap">*&nbsp;**T**&nbsp;in&nbsp;(</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_38">38</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td><td class="diff_next"></td><td class="diff_header" id="to107_39">39</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(double),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_39">39</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td><td class="diff_next"></td><td class="diff_header" id="to107_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float),</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_40">40</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td><td class="diff_next"></td><td class="diff_header" id="to107_41">41</td><td nowrap="nowrap">&nbsp;&nbsp;tensor(float16)</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_41">41</td><td nowrap="nowrap">&nbsp;&nbsp;):</td><td class="diff_next"></td><td class="diff_header" id="to107_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;):</td></tr>
                <tr><td class="diff_next"></td><td class="diff_header" id="from107_42">42</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td><td class="diff_next"></td><td class="diff_header" id="to107_43">43</td><td nowrap="nowrap">&nbsp;&nbsp;Constrain&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;float&nbsp;tensors.</td></tr>
            </tbody>
        </table>

.. _l-onnx-op-logsoftmax-1:

LogSoftmax - 1
==============

**Version**

* **name**: `LogSoftmax (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#LogSoftmax>`_
* **domain**: **main**
* **since_version**: **1**
* **function**: False
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 1**.

**Summary**

The operator computes the logsoftmax (log of softmax) values for each layer in the batch
 of the given input. The input is a 2-D tensor (Tensor<float>) of size
(batch_size x input_feature_dimensions). The output tensor has the same shape
and contains the logsoftmax values of the corresponding input.

Input does not need to explicitly be a 2D vector; rather, it will be
coerced into one. For an arbitrary n-dimensional tensor
input \in [a_0, a_1, ..., a_{k-1}, a_k, ..., a_{n-1}] and k is
the axis provided, then input will be coerced into a 2-dimensional tensor with
dimensions [a_0 * ... * a_{k-1}, a_k * ... * a_{n-1}]. For the default
case where axis=1, this means the input tensor will be coerced into a 2D tensor
of dimensions [a_0, a_1 * ... * a_{n-1}], where a_0 is often the batch size.
In this situation, we must have a_0 = N and a_1 * ... * a_{n-1} = D.
Each of these dimensions must be matched correctly, or else the operator
will throw errors.

**Attributes**

* **axis**:
  Describes the axis of the inputs when coerced to 2D; defaults to one
  because the 0th axis most likely describes the batch_size

**Inputs**

* **input** (heterogeneous) - **T**:
  The input tensor that's coerced into a 2D matrix of size (NxD) as
  described above.

**Outputs**

* **output** (heterogeneous) - **T**:
  The output values with the same shape as input tensor (the original
  size without coercion).

**Type Constraints**

* **T** in (
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input and output types to float tensors.
