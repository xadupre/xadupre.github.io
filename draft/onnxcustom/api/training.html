
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Training &#8212; onnxcustom</title>
    
    <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script src="../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training utilities" href="training_utils.html" />
    <link rel="prev" title="Data" href="data.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   onnxcustom API
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="utils.html">
     Utils
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="data.html">
     Data
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="training_utils.html">
     Training utilities
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="onnx_python/index.html">
   Summary of onnx API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="onnxruntime_python/index.html">
   Summary of onnxruntime and onnxruntime-training API
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#baseestimator">
   BaseEstimator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exceptions">
   Exceptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#first-api-loss-part-of-the-graph">
   First API: loss part of the graph
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#helpers">
     Helpers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ortgradientoptimizer">
     OrtGradientOptimizer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#second-api-loss-part-of-the-graph">
   Second API: loss part of the graph
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx">
     ONNX
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learningpenalty">
     LearningPenalty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learningrate">
     LearningRate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learningloss">
     LearningLoss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions">
     Loss functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ortgradientforwardbackward">
     OrtGradientForwardBackward
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="training">
<h1>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h1>
<p>There exists two APIs in <a class="reference external" href="https://microsoft.github.io/onnxruntime/">onnxruntime</a>. One assumes
the loss function is part of the graph to derive, the other
one assumes the users provides the derivative of the loss
against the output of the graph. With the first API,
the weights are automatically updated. In the second API,
the users has to do it. It is more complex but gives more
freedom.</p>
<p>Both API are wrapped into two classes,
<a class="reference internal" href="#l-api-prt-gradient-optimizer"><span class="std std-ref">OrtGradientOptimizer</span></a> for the first API,
<a class="reference internal" href="#l-api-prt-gradient-optimizer-fw"><span class="std std-ref">OrtGradientForwardBackward</span></a> for the second API.
Both classes make it easier to a user accustomed to
<a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> API to train any graph with a
stochastic gradient descent algorithm.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#baseestimator" id="id1">BaseEstimator</a></p></li>
<li><p><a class="reference internal" href="#exceptions" id="id2">Exceptions</a></p></li>
<li><p><a class="reference internal" href="#first-api-loss-part-of-the-graph" id="id3">First API: loss part of the graph</a></p>
<ul>
<li><p><a class="reference internal" href="#helpers" id="id4">Helpers</a></p></li>
<li><p><a class="reference internal" href="#ortgradientoptimizer" id="id5">OrtGradientOptimizer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#second-api-loss-part-of-the-graph" id="id6">Second API: loss part of the graph</a></p>
<ul>
<li><p><a class="reference internal" href="#onnx" id="id7">ONNX</a></p></li>
<li><p><a class="reference internal" href="#learningpenalty" id="id8">LearningPenalty</a></p></li>
<li><p><a class="reference internal" href="#learningrate" id="id9">LearningRate</a></p></li>
<li><p><a class="reference internal" href="#learningloss" id="id10">LearningLoss</a></p></li>
<li><p><a class="reference internal" href="#loss-functions" id="id11">Loss functions</a></p></li>
<li><p><a class="reference internal" href="#ortgradientforwardbackward" id="id12">OrtGradientForwardBackward</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="baseestimator">
<h2><a class="toc-backref" href="#id1">BaseEstimator</a><a class="headerlink" href="#baseestimator" title="Permalink to this headline">¶</a></h2>
<p>Ancestor to both classes wrapping <a class="reference external" href="https://microsoft.github.io/onnxruntime/">onnxruntime</a> API.</p>
<p><a class="reference internal" href="../onnxcustom/training/_base_estimator.html#onnxcustom.training._base_estimator.BaseEstimator" title="onnxcustom.training._base_estimator.BaseEstimator"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training._base_estimator.BaseEstimator</span></code></a> (<em>self</em>, <em>model_onnx</em>, <em>learning_rate</em>, <em>device</em>)</p>
<blockquote>
<div><p>Base class for optimizers.
Implements common methods such <cite>__repr__</cite>.</p>
<p><a class="reference internal" href="../onnxcustom/training/_base_estimator.html#onnxcustom.training._base_estimator.BaseEstimator.get_params" title="onnxcustom.training._base_estimator.BaseEstimator.get_params"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_params</span></code></a> (<em>self</em>, <em>deep</em> = <cite>False</cite>)</p>
<blockquote>
<div><p>Returns the list of parameters.
Parameter <em>deep</em> is unused.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/_base_estimator.html#onnxcustom.training._base_estimator.BaseEstimator.get_trained_onnx" title="onnxcustom.training._base_estimator.BaseEstimator.get_trained_onnx"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_trained_onnx</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Returns the trained onnx graph, the initial graph
modified by replacing the initializers with the trained
weights.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/_base_estimator.html#onnxcustom.training._base_estimator.BaseEstimator.set_params" title="onnxcustom.training._base_estimator.BaseEstimator.set_params"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_params</span></code></a> (<em>self</em>, <em>params</em>)</p>
<blockquote>
<div><p>Returns the list of parameters.
Parameter <em>deep</em> is unused.</p>
</div></blockquote>
</div></blockquote>
</section>
<section id="exceptions">
<h2><a class="toc-backref" href="#id2">Exceptions</a><a class="headerlink" href="#exceptions" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../onnxcustom/training/excs.html#onnxcustom.training.excs.ConvergenceError" title="onnxcustom.training.excs.ConvergenceError"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.ConvergenceError</span></code></a> (<em>self</em>, <em>args</em>, <em>kwargs</em>)</p>
<blockquote>
<div><p>Raised when a learning algorithm failed
to converge.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/excs.html#onnxcustom.training.excs.EvaluationError" title="onnxcustom.training.excs.EvaluationError"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.EvaluationError</span></code></a> (<em>self</em>, <em>args</em>, <em>kwargs</em>)</p>
<blockquote>
<div><p>Raised when an evaluation failed.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/excs.html#onnxcustom.training.excs.ProviderError" title="onnxcustom.training.excs.ProviderError"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.ProviderError</span></code></a> (<em>self</em>, <em>args</em>, <em>kwargs</em>)</p>
<blockquote>
<div><p>Raised when an input is not on the expected device (CPU, GPU).</p>
</div></blockquote>
</section>
<section id="first-api-loss-part-of-the-graph">
<h2><a class="toc-backref" href="#id3">First API: loss part of the graph</a><a class="headerlink" href="#first-api-loss-part-of-the-graph" title="Permalink to this headline">¶</a></h2>
<section id="helpers">
<h3><a class="toc-backref" href="#id4">Helpers</a><a class="headerlink" href="#helpers" title="Permalink to this headline">¶</a></h3>
<p>Function <cite>add_loss_output</cite> adds a loss function to the graph
if this loss is part of the a predefined list. It may
be combination of L1, L2 losses and L1, L2 penalties.</p>
<p><a class="reference internal" href="../onnxcustom/utils/orttraining_helper.html#onnxcustom.utils.orttraining_helper.add_loss_output" title="onnxcustom.utils.orttraining_helper.add_loss_output"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnxcustom.utils.orttraining_helper.add_loss_output</span></code></a> (<em>onx</em>, <em>score_name</em> = <cite>‘squared_error’</cite>, <em>loss_name</em> = <cite>‘loss’</cite>, <em>label_name</em> = <cite>‘label’</cite>, <em>weight_name</em> = <cite>None</cite>, <em>penalty</em> = <cite>None</cite>, <em>output_index</em> = <cite>None</cite>, <em>kwargs</em>)</p>
<blockquote>
<div><p>Modifies an ONNX graph to add operators to score and allow training.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/utils/orttraining_helper.html#onnxcustom.utils.orttraining_helper.get_train_initializer" title="onnxcustom.utils.orttraining_helper.get_train_initializer"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnxcustom.utils.orttraining_helper.get_train_initializer</span></code></a> (<em>onx</em>)</p>
<blockquote>
<div><p>Returns the list of initializers to train.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/utils/onnx_rewriter.html#onnxcustom.utils.onnx_rewriter.onnx_rewrite_operator" title="onnxcustom.utils.onnx_rewriter.onnx_rewrite_operator"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnxcustom.utils.onnx_rewriter.onnx_rewrite_operator</span></code></a> (<em>onx</em>, <em>op_type</em>, <em>sub_onx</em>, <em>recursive</em> = <cite>True</cite>, <em>debug_info</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Replaces one operator by an onnx graph.</p>
</div></blockquote>
</section>
<section id="ortgradientoptimizer">
<span id="l-api-prt-gradient-optimizer"></span><h3><a class="toc-backref" href="#id5">OrtGradientOptimizer</a><a class="headerlink" href="#ortgradientoptimizer" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../onnxcustom/training/optimizers.html#onnxcustom.training.optimizers.OrtGradientOptimizer" title="onnxcustom.training.optimizers.OrtGradientOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.optimizers.OrtGradientOptimizer</span></code></a> (<em>self</em>, <em>model_onnx</em>, <em>weights_to_train</em>, <em>loss_output_name</em> = <cite>‘loss’</cite>, <em>max_iter</em> = <cite>100</cite>, <em>training_optimizer_name</em> = <cite>‘SGDOptimizer’</cite>, <em>batch_size</em> = <cite>10</cite>, <em>learning_rate</em> = <cite>‘SGD’</cite>, <em>device</em> = <cite>‘cpu’</cite>, <em>warm_start</em> = <cite>False</cite>, <em>verbose</em> = <cite>0</cite>, <em>validation_every</em> = <cite>0.1</cite>, <em>saved_gradient</em> = <cite>None</cite>, <em>sample_weight_name</em> = <cite>‘weight’</cite>)</p>
<blockquote>
<div><p>Implements a simple <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent</a>
with <a class="reference external" href="https://github.com/microsoft/onnxruntime/tree/master/orttraining">onnxruntime-training</a>.</p>
<p><a class="reference internal" href="../onnxcustom/training/optimizers.html#onnxcustom.training.optimizers.OrtGradientOptimizer.fit" title="onnxcustom.training.optimizers.OrtGradientOptimizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit</span></code></a> (<em>self</em>, <em>X</em>, <em>y</em>, <em>sample_weight</em> = <cite>None</cite>, <em>X_val</em> = <cite>None</cite>, <em>y_val</em> = <cite>None</cite>, <em>use_numpy</em> = <cite>False</cite>)</p>
<blockquote>
<div><p>Trains the model.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers.html#onnxcustom.training.optimizers.OrtGradientOptimizer.get_state" title="onnxcustom.training.optimizers.OrtGradientOptimizer.get_state"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_state</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Returns the trained weights.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers.html#onnxcustom.training.optimizers.OrtGradientOptimizer.get_trained_onnx" title="onnxcustom.training.optimizers.OrtGradientOptimizer.get_trained_onnx"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_trained_onnx</span></code></a> (<em>self</em>, <em>model</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Returns the trained onnx graph, the initial graph
modified by replacing the initializers with the trained
weights. If model is not specified, it uses the model
given as an argument to this class. This graph outputs
the loss and not the predictions. Parameter <em>model</em>
can be used to use the graph before loss was added
and then the returned graph will produce the predictions.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers.html#onnxcustom.training.optimizers.OrtGradientOptimizer.set_state" title="onnxcustom.training.optimizers.OrtGradientOptimizer.set_state"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_state</span></code></a> (<em>self</em>, <em>state</em>)</p>
<blockquote>
<div><p>Changes the trained weights.</p>
</div></blockquote>
</div></blockquote>
</section>
</section>
<section id="second-api-loss-part-of-the-graph">
<h2><a class="toc-backref" href="#id6">Second API: loss part of the graph</a><a class="headerlink" href="#second-api-loss-part-of-the-graph" title="Permalink to this headline">¶</a></h2>
<section id="onnx">
<h3><a class="toc-backref" href="#id7">ONNX</a><a class="headerlink" href="#onnx" title="Permalink to this headline">¶</a></h3>
<p>Second API relies on class <a class="reference external" href="http://www.xavierdupre.fr/app/onnxcustom/helpsphinx/onnxmd/onnxruntime_python/training_partial.html#trainingagent">TrainingAgent</a>. It expects to find
the weight to train in alphabetical order. That’s usual not the case.
The following function does not change the order but renames all
of them to fulfil that requirement.</p>
<p><a class="reference internal" href="../onnxcustom/utils/onnx_helper.html#onnxcustom.utils.onnx_helper.onnx_rename_weights" title="onnxcustom.utils.onnx_helper.onnx_rename_weights"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnxcustom.utils.onnx_helper.onnx_rename_weights</span></code></a> (<em>onx</em>)</p>
<blockquote>
<div><p>Renames ONNX initializers to make sure their name
follows the alphabetical order. The model is
modified inplace. This function calls
<a class="reference external" href="http://www.xavierdupre.fr/app/mlprodict/helpsphinx/mlprodict/onnx_tools/onnx_manipulations.html#mlprodict.onnx_tools.onnx_manipulations.onnx_rename_names" title="(in mlprodict v0.8.1747)"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnx_rename_names</span></code></a>.</p>
</div></blockquote>
</section>
<section id="learningpenalty">
<h3><a class="toc-backref" href="#id8">LearningPenalty</a><a class="headerlink" href="#learningpenalty" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.NoLearningPenalty" title="onnxcustom.training.sgd_learning_penalty.NoLearningPenalty"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_penalty.NoLearningPenalty</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>No regularization.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.NoLearningPenalty.build_onnx_function" title="onnxcustom.training.sgd_learning_penalty.NoLearningPenalty.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>n_tensors</em>)</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.NoLearningPenalty.penalty_loss" title="onnxcustom.training.sgd_learning_penalty.NoLearningPenalty.penalty_loss"><code class="xref py py-meth docutils literal notranslate"><span class="pre">penalty_loss</span></code></a> (<em>self</em>, <em>device</em>, <em>loss</em>, <em>weights</em>)</p>
<blockquote>
<div><p>Returns the received loss. Updates the loss inplace.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.NoLearningPenalty.update_weights" title="onnxcustom.training.sgd_learning_penalty.NoLearningPenalty.update_weights"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_weights</span></code></a> (<em>self</em>, <em>n_bind</em>, <em>device</em>, <em>statei</em>)</p>
<blockquote>
<div><p>Returns the received loss. Updates the weight inplace.</p>
</div></blockquote>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty" title="onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty</span></code></a> (<em>self</em>, <em>l1</em> = <cite>0.5</cite>, <em>l2</em> = <cite>0.5</cite>)</p>
<blockquote>
<div><p>Implements a L1 or L2 regularization on weights.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty.build_onnx_function" title="onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>n_tensors</em>)</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty.penalty_loss" title="onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty.penalty_loss"><code class="xref py py-meth docutils literal notranslate"><span class="pre">penalty_loss</span></code></a> (<em>self</em>, <em>device</em>, <em>inputs</em>)</p>
<blockquote>
<div><p>Computes the penalty associated to every
weights and adds them up to the loss.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_penalty.html#onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty.update_weights" title="onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty.update_weights"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_weights</span></code></a> (<em>self</em>, <em>n_bind</em>, <em>device</em>, <em>statei</em>)</p>
</div></blockquote>
</section>
<section id="learningrate">
<h3><a class="toc-backref" href="#id9">LearningRate</a><a class="headerlink" href="#learningrate" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGD" title="onnxcustom.training.sgd_learning_rate.LearningRateSGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_rate.LearningRateSGD</span></code></a> (<em>self</em>, <em>eta0</em> = <cite>0.01</cite>, <em>alpha</em> = <cite>0.0001</cite>, <em>power_t</em> = <cite>0.25</cite>, <em>learning_rate</em> = <cite>‘invscaling’</cite>)</p>
<blockquote>
<div><p>Implements the learning the same way as
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="(in scikit-learn v1.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDRegressor</span></code></a>.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGD.build_onnx_function" title="onnxcustom.training.sgd_learning_rate.LearningRateSGD.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>n_tensors</em>)</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGD.init_learning_rate" title="onnxcustom.training.sgd_learning_rate.LearningRateSGD.init_learning_rate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">init_learning_rate</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Updates the learning rate at the end of an iteration.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGD.update_learning_rate" title="onnxcustom.training.sgd_learning_rate.LearningRateSGD.update_learning_rate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_learning_rate</span></code></a> (<em>self</em>, <em>t</em>)</p>
<blockquote>
<div><p>Updates the learning rate at the end of an iteration.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGD.update_weights" title="onnxcustom.training.sgd_learning_rate.LearningRateSGD.update_weights"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_weights</span></code></a> (<em>self</em>, <em>n_bind</em>, <em>device</em>, <em>statei</em>, <em>gradienti</em>, <em>batch_size</em>, <em>velocity</em> = <cite>None</cite>)</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov" title="onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov</span></code></a> (<em>self</em>, <em>eta0</em> = <cite>0.01</cite>, <em>alpha</em> = <cite>0.0001</cite>, <em>power_t</em> = <cite>0.25</cite>, <em>learning_rate</em> = <cite>‘invscaling’</cite>, <em>momentum</em> = <cite>0.9</cite>, <em>nesterov</em> = <cite>True</cite>)</p>
<blockquote>
<div><p>Implements the learning the same way as
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="(in scikit-learn v1.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDRegressor</span></code></a>.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.build_onnx_function" title="onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>n_tensors</em>)</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.init_learning_rate" title="onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.init_learning_rate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">init_learning_rate</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Updates the learning rate at the end of an iteration.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.update_learning_rate" title="onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.update_learning_rate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_learning_rate</span></code></a> (<em>self</em>, <em>t</em>)</p>
<blockquote>
<div><p>Updates the learning rate at the end of an iteration.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_rate.html#onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.update_weights" title="onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov.update_weights"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_weights</span></code></a> (<em>self</em>, <em>n_bind</em>, <em>device</em>, <em>statei</em>, <em>gradienti</em>, <em>batch_size</em>, <em>velocity</em> = <cite>None</cite>)</p>
</div></blockquote>
</section>
<section id="learningloss">
<h3><a class="toc-backref" href="#id10">LearningLoss</a><a class="headerlink" href="#learningloss" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.AbsoluteLearningLoss" title="onnxcustom.training.sgd_learning_loss.AbsoluteLearningLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_loss.AbsoluteLearningLoss</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Implements a square loss <img class="math" src="../_images/math/bd135635a9a0e36d9d8c7711cbde8ce640af3e73.svg" alt="|Y - Z|"/>
where <em>Y</em> is the output and <em>Z</em> the expected output.
See <a class="reference internal" href="../onnxcustom/utils/onnx_function.html#onnxcustom.utils.onnx_function._onnx_grad_loss_absolute_error" title="onnxcustom.utils.onnx_function._onnx_grad_loss_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">_onnx_grad_loss_absolute_error</span></code></a> for the ONNX
implementation.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.AbsoluteLearningLoss.build_onnx_function" title="onnxcustom.training.sgd_learning_loss.AbsoluteLearningLoss.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>weight_name</em>)</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.ElasticLearningLoss" title="onnxcustom.training.sgd_learning_loss.ElasticLearningLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_loss.ElasticLearningLoss</span></code></a> (<em>self</em>, <em>l1_weight</em> = <cite>0.5</cite>, <em>l2_weight</em> = <cite>0.5</cite>)</p>
<blockquote>
<div><p>Implements a square loss
<img class="math" src="../_images/math/bb8f2acc6d837f276f8f1496f1d17f69f6ad55d2.svg" alt="(Y - Z)^2 \alpha + |Y - Z| * \beta"/>
where <em>Y</em> is the output and <em>Z</em> the expected output,
<img class="math" src="../_images/math/35093c7ccc735c18e6452789bcb7ec50e0cb0c43.svg" alt="\alpha"/> is <em>l2_weight</em> and <img class="math" src="../_images/math/8fd96498a01957a1ce6cb102fc544d7e32ae2973.svg" alt="\beta"/>
is <em>l1_weight</em>.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.ElasticLearningLoss.build_onnx_function" title="onnxcustom.training.sgd_learning_loss.ElasticLearningLoss.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>weight_name</em>)</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.NegLogLearningLoss" title="onnxcustom.training.sgd_learning_loss.NegLogLearningLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_loss.NegLogLearningLoss</span></code></a> (<em>self</em>, <em>eps</em> = <cite>1e-05</cite>, <em>probability_function</em> = <cite>‘sigmoid’</cite>)</p>
<blockquote>
<div><p>Implements a negative log loss
<cite>‘log(yt, yp) = -(1-yt)log(1-yp) - ytlog(yp)</cite>,
this only works for a binary classification where <em>yp</em> is the
predicted probability, <em>yt</em> is the expected probability.
<em>yt</em> is expected to be binary, <em>yp</em> is a matrix with two
columns, the sum on every line is 1.
However, this loss is usually applied after a function softmax
and the gradient is directly computed from the loss to the
raw score before they are processed through the softmax function
(see class <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_sgd_fast.pyx#L236">Log</a>).</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.NegLogLearningLoss.build_onnx_function" title="onnxcustom.training.sgd_learning_loss.NegLogLearningLoss.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>weight_name</em>)</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.SquareLearningLoss" title="onnxcustom.training.sgd_learning_loss.SquareLearningLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.sgd_learning_loss.SquareLearningLoss</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Implements a square loss <img class="math" src="../_images/math/220ff2fdeb35e1fb7fb11d4f769a6e235346b32b.svg" alt="(Y - Z)^2"/>
where <em>Y</em> is the output and <em>Z</em> the expected output.
See <a class="reference internal" href="../onnxcustom/utils/onnx_function.html#onnxcustom.utils.onnx_function._onnx_grad_loss_square_error" title="onnxcustom.utils.onnx_function._onnx_grad_loss_square_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">_onnx_grad_loss_square_error</span></code></a> for the ONNX
implementation.</p>
<p><a class="reference internal" href="../onnxcustom/training/sgd_learning_loss.html#onnxcustom.training.sgd_learning_loss.SquareLearningLoss.build_onnx_function" title="onnxcustom.training.sgd_learning_loss.SquareLearningLoss.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>, <em>opset</em>, <em>device</em>, <em>weight_name</em>)</p>
</div></blockquote>
</section>
<section id="loss-functions">
<h3><a class="toc-backref" href="#id11">Loss functions</a><a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../onnxcustom/utils/onnx_function.html#onnxcustom.utils.onnx_function.function_onnx_graph" title="onnxcustom.utils.onnx_function.function_onnx_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">onnxcustom.utils.onnx_function.function_onnx_graph</span></code></a> (<em>name</em>, <em>target_opset</em> = <cite>None</cite>, <em>dtype</em> = <cite>&lt;class ‘numpy.float32’&gt;</cite>, <em>weight_name</em> = <cite>None</cite>, <em>kwargs</em>)</p>
<blockquote>
<div><p>Returns the ONNX graph corresponding to a function.</p>
</div></blockquote>
</section>
<section id="ortgradientforwardbackward">
<span id="l-api-prt-gradient-optimizer-fw"></span><h3><a class="toc-backref" href="#id12">OrtGradientForwardBackward</a><a class="headerlink" href="#ortgradientforwardbackward" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer</span></code></a> (<em>self</em>, <em>model_onnx</em>, <em>weights_to_train</em> = <cite>None</cite>, <em>loss_output_name</em> = <cite>‘loss’</cite>, <em>max_iter</em> = <cite>100</cite>, <em>training_optimizer_name</em> = <cite>‘SGDOptimizer’</cite>, <em>batch_size</em> = <cite>10</cite>, <em>learning_rate</em> = <cite>‘SGD’</cite>, <em>device</em> = <cite>‘cpu’</cite>, <em>warm_start</em> = <cite>False</cite>, <em>verbose</em> = <cite>0</cite>, <em>validation_every</em> = <cite>0.1</cite>, <em>learning_loss</em> = <cite>‘square_error’</cite>, <em>enable_logging</em> = <cite>False</cite>, <em>weight_name</em> = <cite>None</cite>, <em>learning_penalty</em> = <cite>None</cite>, <em>exc</em> = <cite>True</cite>)</p>
<blockquote>
<div><p>Implements a simple <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent</a>
with <a class="reference external" href="https://github.com/microsoft/onnxruntime/tree/master/orttraining">onnxruntime-training</a>. It leverages class
&#64;see class OrtGradientForwardBackward.</p>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.build_onnx_function" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.build_onnx_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_onnx_function</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Creates ONNX graph and <em>InferenceSession</em> related to
any operations applying on <em>OrtValue</em>.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.fit" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit</span></code></a> (<em>self</em>, <em>X</em>, <em>y</em>, <em>sample_weight</em> = <cite>None</cite>, <em>X_val</em> = <cite>None</cite>, <em>y_val</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Trains the model.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.get_full_state" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.get_full_state"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_full_state</span></code></a> (<em>self</em>, <em>kind</em> = <cite>‘weight’</cite>)</p>
<blockquote>
<div><p>Returns the trained weights and the inputs.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.get_state" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.get_state"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_state</span></code></a> (<em>self</em>, <em>kind</em> = <cite>‘weight’</cite>)</p>
<blockquote>
<div><p>Returns the trained weights.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.get_trained_onnx" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.get_trained_onnx"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_trained_onnx</span></code></a> (<em>self</em>, <em>model</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Returns the trained onnx graph, the initial graph
modified by replacing the initializers with the trained
weights.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.losses" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.losses"><code class="xref py py-meth docutils literal notranslate"><span class="pre">losses</span></code></a> (<em>self</em>, <em>X</em>, <em>y</em>, <em>sample_weight</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Returns the losses associated to every observation.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.score" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.score"><code class="xref py py-meth docutils literal notranslate"><span class="pre">score</span></code></a> (<em>self</em>, <em>X</em>, <em>y</em>, <em>sample_weight</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Return the whole score associated.</p>
</div></blockquote>
<p><a class="reference internal" href="../onnxcustom/training/optimizers_partial.html#onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.set_state" title="onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer.set_state"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_state</span></code></a> (<em>self</em>, <em>state</em>, <em>check_trained</em> = <cite>True</cite>, <em>kind</em> = <cite>‘weight’</cite>, <em>zero</em> = <cite>False</cite>)</p>
<blockquote>
<div><p>Changes the trained weights.</p>
</div></blockquote>
</div></blockquote>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="data.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Data</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="training_utils.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Training utilities</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>