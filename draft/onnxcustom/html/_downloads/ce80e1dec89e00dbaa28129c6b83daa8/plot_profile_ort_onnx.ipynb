{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Profiling of ONNX graph with onnxruntime\n\nThis example shows to profile the execution of an ONNX file\nwith :epkg:`onnxruntime` to find the operators which consume\nmost of the time. The script assumes the first dimension, if left\nunknown, is the batch dimension.\n\n## One ONNX file\n\nThis section creates an ONNX graph if there is not one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport json\nfrom collections import OrderedDict\nimport numpy\nimport onnx\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable\nimport pandas\nfrom onnxruntime import InferenceSession, SessionOptions, get_device\nfrom onnxruntime.capi._pybind_state import (  # pylint: disable=E0611\n    SessionIOBinding, OrtDevice as C_OrtDevice, OrtValue as C_OrtValue)\nfrom sklearn.neighbors import RadiusNeighborsRegressor\nfrom skl2onnx import to_onnx\nfrom tqdm import tqdm\nfrom mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nfrom mlprodict.onnxrt.ops_whole.session import OnnxWholeSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"onnx_to_profile.onnx\"\n\n\nif not os.path.exists(filename):\n    print(\"Generate a graph for %r.\" % filename)\n    X = numpy.random.randn(1000, 10).astype(numpy.float64)\n    y = X.sum(axis=1).reshape((-1, 1))\n\n    model = RadiusNeighborsRegressor()\n    model.fit(X, y)\n    onx = to_onnx(model, X, options={'optim': 'cdist'})\n\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions\n\nWe need to generate random inputs to test the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def random_input(typ, shape, batch):\n    if typ == 'tensor(double)':\n        dtype = numpy.float64\n    elif typ == 'tensor(float)':\n        dtype = numpy.float32\n    else:\n        raise NotImplementedError(\n            \"Unable to guess dtype from %r.\" % typ)\n\n    if len(shape) <= 1:\n        new_shape = shape\n    elif shape[0] is None:\n        new_shape = tuple([batch] + list(shape[1:]))\n    else:\n        new_shape = shape\n    return numpy.random.randn(*new_shape).astype(dtype)\n\n\ndef random_feed(sess, batch=10):\n    \"\"\"\n    Creates a dictionary of random inputs.\n\n    :param batch: dimension to use as batch dimension if unknown\n    :return: dictionary\n    \"\"\"\n    inputs = sess.get_inputs()\n    res = OrderedDict()\n    for inp in inputs:\n        name = inp.name\n        typ = inp.type\n        shape = inp.shape\n        res[name] = random_input(typ, shape, batch)\n    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling\n\nLet's choose the device available on this machine.\nbatch dimension is set to 10.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = 10\n\nif get_device().upper() == 'GPU':\n    ort_device = C_OrtDevice(\n        C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)\n    provider = 'CUDAExecutionProvider'\nelse:\n    ort_device = C_OrtDevice(\n        C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)\n    provider = 'CPUExecutionProvider'\n\nprint(\"provider = %r\" % provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(filename, 'rb') as f:\n    onx = onnx.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create of the session.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "so = SessionOptions()\nso.enable_profiling = True\nso.optimized_model_filepath = os.path.split(filename)[-1] + \".optimized.onnx\"\nsess = InferenceSession(onx.SerializeToString(), so,\n                        providers=[provider])\nbind = SessionIOBinding(sess._sess)\n\nprint(\"graph_optimization_level:\", so.graph_optimization_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates random data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feed = random_feed(sess, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "moving the data on CPU or GPU\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feed_ort_value = OrderedDict(\n    (name, (C_OrtValue.ortvalue_from_numpy(v, ort_device), v.dtype))\n    for name, v in feed.items())\noutputs = [o.name for o in sess.get_outputs()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A function which calls the API for any device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_with_iobinding(sess, bind, ort_device, feed_ort_value, outputs):\n    for name, (value, dtype) in feed_ort_value.items():\n        bind.bind_input(name, ort_device, dtype, value.shape(),\n                        value.data_ptr())\n    for out in outputs:\n        bind.bind_output(out, ort_device)\n    sess._sess.run_with_iobinding(bind, None)\n    ortvalues = bind.get_outputs()\n    return [o.numpy() for o in ortvalues]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The profiling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(0, 10)):\n    run_with_iobinding(sess, bind, ort_device, feed_ort_value, outputs)\n\nprof = sess.end_profiling()\nwith open(prof, \"r\") as f:\n    js = json.load(f)\ndf = pandas.DataFrame(OnnxWholeSession.process_profiling(js))\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First graph is by operator type.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gr_dur = df[['dur', \"args_op_name\"]].groupby(\n    \"args_op_name\").sum().sort_values('dur')\ntotal = gr_dur['dur'].sum()\ngr_dur /= total\ngr_n = df[['dur', \"args_op_name\"]].groupby(\n    \"args_op_name\").count().sort_values('dur')\ngr_n = gr_n.loc[gr_dur.index, :]\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\ngr_dur.plot.barh(ax=ax[0])\ngr_n.plot.barh(ax=ax[1])\nax[0].set_title(\"duration\")\nax[1].set_title(\"n occurences\")\nfig.suptitle(os.path.split(filename)[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second graph is by operator name.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gr_dur = df[['dur', \"args_op_name\", \"name\"]].groupby(\n    [\"args_op_name\", \"name\"]).sum().sort_values('dur')\ntotal = gr_dur['dur'].sum()\ngr_dur /= total\nif gr_dur.shape[0] > 30:\n    gr_dur = gr_dur.tail(n=30)\n\ngr_dur.head(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(8, gr_dur.shape[0] // 2))\ngr_dur.plot.barh(ax=ax)\nax.set_title(\"duration per node\")\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(7)\nmake_axes_area_auto_adjustable(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumsum is where the execution spends most of its time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}