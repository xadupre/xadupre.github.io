{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Train a linear regression with forward backward\n\nThis example rewrites `l-orttraining-linreg` with another\noptimizer :class:`OrtGradientForwardBackwardOptimizer\n<onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer>`.\nThis optimizer relies on class :epkg:`TrainingAgent` from\n:epkg:`onnxruntime-training`. In this case, the user does not have to\nmodify the graph to compute the error. The optimizer\nbuilds another graph which returns the gradient of every weights\nassuming the gradient on the output is known. Finally, the optimizer\nadds the gradients to the weights. To summarize, it starts from the following\ngraph:\n\n<img src=\"file://images/onnxfwbw1.png\">\n\nClass :class:`OrtGradientForwardBackwardOptimizer\n<onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer>`\nbuilds other ONNX graph to implement a gradient descent algorithm:\n\n<img src=\"file://images/onnxfwbw2.png\">\n\nThe blue node is built by class :epkg:`TrainingAgent`\n(from :epkg:`onnxruntime-training`). The green nodes are added by\nclass :class:`OrtGradientForwardBackwardOptimizer\n<onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer>`.\nThis implementation relies on ONNX to do the computation but it could\nbe replaced by any other framework such as :epkg:`pytorch`. This\ndesign gives more freedom to the user to implement his own training\nalgorithm.\n\n## A simple linear regression with scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\nimport numpy\nfrom pandas import DataFrame\nfrom onnxruntime import get_device\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom mlprodict.onnx_conv import to_onnx\nfrom onnxcustom.plotting.plotting_onnx import plot_onnxs\nfrom onnxcustom.utils.orttraining_helper import get_train_initializer\nfrom onnxcustom.training.optimizers_partial import (\n    OrtGradientForwardBackwardOptimizer)\n\nX, y = make_regression(n_features=2, bias=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a :class:`sklearn.neural_network.MLPRegressor`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = MLPRegressor(hidden_layer_sizes=tuple(),\n                  activation='identity', max_iter=50,\n                  batch_size=10, solver='sgd',\n                  alpha=0, learning_rate_init=1e-2,\n                  n_iter_no_change=200,\n                  momentum=0, nesterovs_momentum=False)\nlr.fit(X, y)\nprint(lr.predict(X[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trained coefficients are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"trained coefficients:\", lr.coefs_, lr.intercepts_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX graph\n\nTraining with :epkg:`onnxruntime-training` starts with an ONNX\ngraph which defines the model to learn. It is obtained by simply\nconverting the previous linear regression into ONNX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(lr, X_train[:1].astype(numpy.float32), target_opset=15,\n              black_op={'LinearRegressor'})\n\nplot_onnxs(onx, title=\"Linear Regression in ONNX\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights\n\nEvery initializer is a set of weights which can be trained\nand a gradient will be computed for it.\nHowever an initializer used to modify a shape or to\nextract a subpart of a tensor does not need training.\n:func:`get_train_initializer\n<onnxcustom.tools.orttraining_helper.get_train_initializer>`\nremoves them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inits = get_train_initializer(onx)\nweights = {k: v for k, v in inits.items() if k != \"shape_tensor\"}\npprint(list((k, v[0].shape) for k, v in weights.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train on CPU or GPU if available\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if get_device().upper() == 'GPU' else 'cpu'\nprint(\"device=%r get_device()=%r\" % (device, get_device()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Descent\n\nThe training logic is hidden in class\n:class:`OrtGradientForwardBackwardOptimizer\n<onnxcustom.training.optimizers_partial.OrtGradientForwardBackwardOptimizer>`\nIt follows :epkg:`scikit-learn` API (see `SGDRegressor\n<https://scikit-learn.org/stable/modules/\ngenerated/sklearn.linear_model.SGDRegressor.html>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_session = OrtGradientForwardBackwardOptimizer(\n    onx, list(weights), device=device, verbose=1, learning_rate=1e-2,\n    warm_start=False, max_iter=200, batch_size=10)\n\ntrain_session.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the trained coefficients are...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()\npprint([\"trained coefficients:\", state_tensors])\nprint(\"last_losses:\", train_session.train_losses_[-5:])\n\nmin_length = min(len(train_session.train_losses_), len(lr.loss_curve_))\ndf = DataFrame({'ort losses': train_session.train_losses_[:min_length],\n                'skl losses': lr.loss_curve_[:min_length]})\ndf.plot(title=\"Train loss against iterations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The convergence speed is almost the same.\n\n## Gradient Graph\n\nAs mentioned in this introduction, the computation relies\non a few more graphs than the initial graph.\nWhen the loss is needed but not the gradient, class\n:epkg:`TrainingAgent` creates another graph, faster,\nwith the trained initializers as additional inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_loss = train_session.train_session_.cls_type_._optimized_pre_grad_model\n\nplot_onnxs(onx, onx_loss, title=['regression', 'loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the gradient.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_gradient = train_session.train_session_.cls_type_._trained_onnx\n\nplot_onnxs(onx_loss, onx_gradient, title=['loss', 'gradient + loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last ONNX graphs are used to compute the gradient *dE/dY*\nand to update the weights. The first graph takes the labels and the\nexpected labels and returns the square loss and its gradient.\nThe second graph takes the weights and the learning rate as inputs\nand returns the updated weights. This graph works on tensors of any shape\nbut with the same element type.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_onnxs(train_session.learning_loss.loss_grad_onnx_,\n           train_session.learning_rate.axpy_onnx_,\n           title=['error gradient + loss', 'gradient update'])\n\n# import matplotlib.pyplot as plt\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}