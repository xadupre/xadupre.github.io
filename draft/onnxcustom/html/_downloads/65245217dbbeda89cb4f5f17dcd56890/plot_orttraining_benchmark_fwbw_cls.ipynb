{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark, comparison sklearn - forward-backward - classification\n\nThe benchmark compares the processing time between :epkg:`scikit-learn`\nand :epkg:`onnxruntime-training` on a logistic regression regression\nand a neural network for classification.\nIt replicates the benchmark implemented in `l-orttraining-benchmark-fwbw`.\n\n## First comparison: neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nimport time\nimport numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom onnxruntime import get_device\nfrom pyquickhelper.pycode.profiling import profile, profile2graph\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom mlprodict.onnx_conv import to_onnx\nfrom mlprodict.plotting.text_plot import onnx_simple_text_plot\nfrom mlprodict.onnx_tools.onnx_manipulations import select_model_inputs_outputs\nfrom onnxcustom.utils.onnx_helper import onnx_rename_weights\nfrom onnxcustom.training.optimizers_partial import (\n    OrtGradientForwardBackwardOptimizer)\nfrom onnxcustom.training.sgd_learning_rate import LearningRateSGDNesterov\nfrom onnxcustom.training.sgd_learning_loss import NegLogLearningLoss\nfrom onnxcustom.training.sgd_learning_penalty import ElasticLearningPenalty\n\n\nX, y = make_classification(1000, n_features=100, n_classes=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.int64)\nX_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark(X, y, skl_model, train_session, name, verbose=True):\n    \"\"\"\n    :param skl_model: model from scikit-learn\n    :param train_session: instance of OrtGradientForwardBackwardOptimizer\n    :param name: experiment name\n    :param verbose: to debug\n    \"\"\"\n    print(\"[benchmark] %s\" % name)\n    begin = time.perf_counter()\n    skl_model.fit(X, y)\n    duration_skl = time.perf_counter() - begin\n    length_skl = len(skl_model.loss_curve_)\n    print(\"[benchmark] skl=%r iterations - %r seconds\" % (\n        length_skl, duration_skl))\n\n    begin = time.perf_counter()\n    train_session.fit(X, y)\n    duration_ort = time.perf_counter() - begin\n    length_ort = len(train_session.train_losses_)\n    print(\"[benchmark] ort=%r iteration - %r seconds\" % (\n        length_ort, duration_ort))\n\n    return dict(skl=duration_skl, ort=duration_ort, name=name,\n                iter_skl=length_skl, iter_ort=length_ort,\n                losses_skl=skl_model.loss_curve_,\n                losses_ort=train_session.train_losses_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Common parameters and model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 15\nmax_iter = 100\n\nnn = MLPClassifier(hidden_layer_sizes=(50, 10), max_iter=max_iter,\n                   solver='sgd', learning_rate_init=1e-1, alpha=1e-4,\n                   n_iter_no_change=max_iter * 3, batch_size=batch_size,\n                   nesterovs_momentum=True, momentum=0.9,\n                   learning_rate=\"invscaling\")\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversion to ONNX and trainer initialization\nIt is slightly different from a regression model.\nProbabilities usually come from raw scores transformed\nthrough a function such as the sigmoid function.\nThe gradient of the loss is computed against the raw scores\nbecause it is easier to compute than to let onnxruntime\ndo it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(nn, X_train[:1].astype(numpy.float32), target_opset=15,\n              options={'zipmap': False})\n\ntry:\n    print(onnx_simple_text_plot(onx))\nexcept RuntimeError as e:\n    print(\"You should upgrade mlprodict.\")\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Raw scores are the input of operator *Sigmoid*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = select_model_inputs_outputs(\n    onx, outputs=[\"add_result2\"], infer_shapes=True)\nprint(onnx_simple_text_plot(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the names are renamed to have them follow the\nalphabetical order (see :class:`OrtGradientForwardBackward\n<onnxcustom.training.ortgradient.OrtGradientForwardBackward>`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = onnx_rename_weights(onx)\nprint(onnx_simple_text_plot(onx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We select the log loss (see :class:`NegLogLearningLoss\n<onnxcustom.training.sgd_learning_loss.NegLogLearningLoss>`,\na simple regularization defined with :class:`ElasticLearningPenalty\n<onnxcustom.training.sgd_learning_penalty.ElasticLearningPenalty>`,\nand the Nesterov algorithm to update the weights with\n`LearningRateSGDNesterov\n<onnxcustom.training.sgd_learning_rate.LearningRateSGDNesterov>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_session = OrtGradientForwardBackwardOptimizer(\n    onx, device='cpu', warm_start=False,\n    max_iter=max_iter, batch_size=batch_size,\n    learning_loss=NegLogLearningLoss(),\n    learning_rate=LearningRateSGDNesterov(\n        1e-7, nesterov=True, momentum=0.9),\n    learning_penalty=ElasticLearningPenalty(l1=0, l2=1e-4))\n\n\nbenches = [benchmark(X_train, y_train, nn, train_session, name='NN-CPU')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_name(text):\n    pos = text.find('onnxruntime')\n    if pos >= 0:\n        return text[pos:]\n    pos = text.find('sklearn')\n    if pos >= 0:\n        return text[pos:]\n    pos = text.find('onnxcustom')\n    if pos >= 0:\n        return text[pos:]\n    pos = text.find('site-packages')\n    if pos >= 0:\n        return text[pos:]\n    return text\n\n\nps = profile(lambda: benchmark(X_train, y_train,\n             nn, train_session, name='NN-CPU'))[0]\nroot, nodes = profile2graph(ps, clean_text=clean_name)\ntext = root.to_text()\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## if GPU is available\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if get_device().upper() == 'GPU':\n\n    train_session = OrtGradientForwardBackwardOptimizer(\n        onx, device='cuda', warm_start=False,\n        max_iter=max_iter, batch_size=batch_size,\n        learning_loss=NegLogLearningLoss(),\n        learning_rate=LearningRateSGDNesterov(\n            1e-7, nesterov=False, momentum=0.9),\n        learning_penalty=ElasticLearningPenalty(l1=0, l2=1e-4))\n\n    benches.append(benchmark(X_train, y_train, nn,\n                   train_session, name='NN-GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple linear layer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nn = MLPClassifier(hidden_layer_sizes=tuple(), max_iter=max_iter,\n                   solver='sgd', learning_rate_init=1e-1, alpha=1e-4,\n                   n_iter_no_change=max_iter * 3, batch_size=batch_size,\n                   nesterovs_momentum=True, momentum=0.9,\n                   learning_rate=\"invscaling\", activation='identity')\n\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    nn.fit(X_train, y_train)\n\nonx = to_onnx(nn, X_train[:1].astype(numpy.float32), target_opset=15,\n              options={'zipmap': False, 'nocl': True})\nonx = select_model_inputs_outputs(\n    onx, outputs=[\"add_result\"], infer_shapes=True)\nonx = onnx_rename_weights(onx)\n\ntry:\n    print(onnx_simple_text_plot(onx))\nexcept RuntimeError as e:\n    print(\"You should upgrade mlprodict.\")\n    print(e)\n\ntrain_session = OrtGradientForwardBackwardOptimizer(\n    onx, device='cpu', warm_start=False,\n    max_iter=max_iter, batch_size=batch_size,\n    learning_loss=NegLogLearningLoss(),\n    learning_rate=LearningRateSGDNesterov(\n        1e-5, nesterov=True, momentum=0.9),\n    learning_penalty=ElasticLearningPenalty(l1=0, l2=1e-4))\n\n\nbenches.append(benchmark(X_train, y_train, nn, train_session, name='LR-CPU'))\n\nif get_device().upper() == 'GPU':\n\n    train_session = OrtGradientForwardBackwardOptimizer(\n        onx, device='cuda', warm_start=False,\n        max_iter=max_iter, batch_size=batch_size,\n        learning_loss=NegLogLearningLoss(),\n        learning_rate=LearningRateSGDNesterov(\n            1e-5, nesterov=False, momentum=0.9),\n        learning_penalty=ElasticLearningPenalty(l1=0, l2=1e-4))\n\n    benches.append(benchmark(X_train, y_train, nn,\n                   train_session, name='LR-GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n\nDataframe first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(benches).set_index('name')\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "text output\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\ndf[['skl', 'ort']].plot.bar(title=\"Processing time\", ax=ax[0])\nax[0].tick_params(axis='x', rotation=30)\nfor bench in benches:\n    ax[1].plot(bench['losses_skl'][1:], label='skl-' + bench['name'])\n    ax[1].plot(bench['losses_ort'][1:], label='ort-' + bench['name'])\nax[1].set_yscale('log')\nax[1].set_title(\"Losses\")\nax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient update are not exactly the same.\nIt should be improved for a fair comprison.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}