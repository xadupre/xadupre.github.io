{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Forward backward on a neural network on GPU (Nesterov) and penalty\n\nThis example does the same as `l-orttraining-nn-gpu-fwbw`\nbut updates the weights using `Nesterov momentum\n<https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum>`_.\n\n## A neural network with scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nimport numpy\nimport onnx\nfrom pandas import DataFrame\nfrom onnxruntime import get_device\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom onnxcustom.plotting.plotting_onnx import plot_onnxs\nfrom mlprodict.onnx_conv import to_onnx\nfrom mlprodict.plotting.text_plot import onnx_simple_text_plot\nfrom onnxcustom.utils.orttraining_helper import get_train_initializer\nfrom onnxcustom.utils.onnx_helper import onnx_rename_weights\nfrom onnxcustom.training.optimizers_partial import (\n    OrtGradientForwardBackwardOptimizer)\nfrom onnxcustom.training.sgd_learning_rate import LearningRateSGDNesterov\nfrom onnxcustom.training.sgd_learning_penalty import ElasticLearningPenalty\n\n\nX, y = make_regression(1000, n_features=10, bias=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nnn = MLPRegressor(hidden_layer_sizes=(10, 10), max_iter=100,\n                  solver='sgd', learning_rate_init=5e-5,\n                  n_iter_no_change=1000, batch_size=10, alpha=0,\n                  momentum=0.9, nesterovs_momentum=True)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    nn.fit(X_train, y_train)\n\nprint(nn.loss_curve_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Score:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"mean_squared_error=%r\" % mean_squared_error(y_test, nn.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion to ONNX\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(nn, X_train[:1].astype(numpy.float32), target_opset=15)\nplot_onnxs(onx)\n\nweights = list(sorted(get_train_initializer(onx)))\nprint(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training graph with forward backward\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if get_device().upper() == 'GPU' else 'cpu'\n\nprint(\"device=%r get_device()=%r\" % (device, get_device()))\n\nonx = onnx_rename_weights(onx)\ntrain_session = OrtGradientForwardBackwardOptimizer(\n    onx, device=device, verbose=1,\n    learning_rate=LearningRateSGDNesterov(1e-4, nesterov=True, momentum=0.9),\n    warm_start=False, max_iter=100, batch_size=10)\ntrain_session.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(train_session.train_losses_)\n\ndf = DataFrame({'ort losses': train_session.train_losses_,\n                'skl losses:': nn.loss_curve_})\ndf.plot(title=\"Train loss against iterations (Nesterov)\", logy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The convergence rate is different but both classes\ndo not update the learning exactly the same way.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization\n\nDefault parameters for MLPRegressor suggest to penalize weights\nduring training: `alpha=1e-4`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nn = MLPRegressor(hidden_layer_sizes=(10, 10), max_iter=100,\n                  solver='sgd', learning_rate_init=5e-5,\n                  n_iter_no_change=1000, batch_size=10, alpha=1e-4,\n                  momentum=0.9, nesterovs_momentum=True)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    nn.fit(X_train, y_train)\n\nprint(nn.loss_curve_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do the same with onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_session = OrtGradientForwardBackwardOptimizer(\n    onx, device=device, verbose=1,\n    learning_rate=LearningRateSGDNesterov(1e-4, nesterov=True, momentum=0.9),\n    learning_penalty=ElasticLearningPenalty(l1=0, l2=1e-4),\n    warm_start=False, max_iter=100, batch_size=10)\ntrain_session.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(train_session.train_losses_)\n\ndf = DataFrame({'ort losses': train_session.train_losses_,\n                'skl losses:': nn.loss_curve_})\ndf.plot(title=\"Train loss against iterations (Nesterov + penalty)\", logy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All ONNX graphs\n\nMethod Method :meth:`save_onnx_graph\n<onnxcustom.training._base.BaseOnnxClass.save_onnx_graph>`\ncan export all the ONNX graph used by the model on disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_graph(d):\n    for k, v in sorted(d.items()):\n        if isinstance(v, dict):\n            print_graph(v)\n        else:\n            print(\"\\n++++++\", v.replace(\"\\\\\", \"/\"), \"\\n\")\n            with open(v, \"rb\") as f:\n                print(onnx_simple_text_plot(onnx.load(f)))\n\n\nall_files = train_session.save_onnx_graph('.')\nprint_graph(all_files)\n\n\n# import matplotlib.pyplot as plt\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}