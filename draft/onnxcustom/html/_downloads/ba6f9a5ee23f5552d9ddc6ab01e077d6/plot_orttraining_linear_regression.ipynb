{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Train a linear regression with onnxruntime-training\n\nThis example explores how :epkg:`onnxruntime-training` can be used to\ntrain a simple linear regression using a gradient descent.\nIt compares the results with those obtained by\n:class:`sklearn.linear_model.SGDRegressor`\n\n## A simple linear regression with scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\nimport numpy\nimport onnx\nfrom pandas import DataFrame\nfrom onnxruntime import (\n    InferenceSession, get_device)\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom mlprodict.onnx_conv import to_onnx\nfrom onnxcustom.plotting.plotting_onnx import plot_onnxs\nfrom onnxcustom.utils.orttraining_helper import (\n    add_loss_output, get_train_initializer)\nfrom onnxcustom.training.optimizers import OrtGradientOptimizer\n\nX, y = make_regression(n_features=2, bias=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nlr = SGDRegressor(l1_ratio=0, max_iter=200, eta0=5e-2)\nlr.fit(X, y)\nprint(lr.predict(X[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trained coefficients are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"trained coefficients:\", lr.coef_, lr.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However this model does not show the training curve.\nWe switch to a :class:`sklearn.neural_network.MLPRegressor`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = MLPRegressor(hidden_layer_sizes=tuple(),\n                  activation='identity', max_iter=200,\n                  batch_size=10, solver='sgd',\n                  alpha=0, learning_rate_init=1e-2,\n                  n_iter_no_change=200,\n                  momentum=0, nesterovs_momentum=False)\nlr.fit(X, y)\nprint(lr.predict(X[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trained coefficients are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"trained coefficients:\", lr.coefs_, lr.intercepts_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX graph\n\nTraining with :epkg:`onnxruntime-training` starts with an ONNX\ngraph which defines the model to learn. It is obtained by simply\nconverting the previous linear regression into ONNX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(lr, X_train[:1].astype(numpy.float32), target_opset=15,\n              black_op={'LinearRegressor'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing a loss\n\nThe training requires a loss function. By default, it\nis the square function but it could be the absolute error or\ninclude regularization. Function\n:func:`add_loss_output\n<onnxcustom.utils.orttraining_helper.add_loss_output>`\nappends the loss function to the ONNX graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_train = add_loss_output(onx)\n\nplot_onnxs(onx, onx_train,\n           title=['Linear Regression',\n                  'Linear Regression + Loss with ONNX'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check inference is working.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(onx_train.SerializeToString(),\n                        providers=['CPUExecutionProvider'])\nres = sess.run(None, {'X': X_test, 'label': y_test.reshape((-1, 1))})\nprint(\"onnx loss=%r\" % (res[0][0, 0] / X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights\n\nEvery initializer is a set of weights which can be trained\nand a gradient will be computed for it.\nHowever an initializer used to modify a shape or to\nextract a subpart of a tensor does not need training.\nLet's remove them from the list of initializer to train.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inits = get_train_initializer(onx)\nweights = {k: v for k, v in inits.items() if k != \"shape_tensor\"}\npprint(list((k, v[0].shape) for k, v in weights.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train on CPU or GPU if available\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if get_device().upper() == 'GPU' else 'cpu'\nprint(\"device=%r get_device()=%r\" % (device, get_device()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Descent\n\nThe training logic is hidden in class\n:class:`OrtGradientOptimizer\n<onnxcustom.training.optimizers.OrtGradientOptimizer>`.\nIt follows :epkg:`scikit-learn` API (see `SGDRegressor\n<https://scikit-learn.org/stable/modules/\ngenerated/sklearn.linear_model.SGDRegressor.html>`_.\nThe gradient graph is not available at this stage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_session = OrtGradientOptimizer(\n    onx_train, list(weights), device=device, verbose=1, learning_rate=1e-2,\n    warm_start=False, max_iter=200, batch_size=10,\n    saved_gradient=\"saved_gradient.onnx\")\n\ntrain_session.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the trained coefficient are...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()\npprint([\"trained coefficients:\", state_tensors])\nprint(\"last_losses:\", train_session.train_losses_[-5:])\n\nmin_length = min(len(train_session.train_losses_), len(lr.loss_curve_))\ndf = DataFrame({'ort losses': train_session.train_losses_[:min_length],\n                'skl losses': lr.loss_curve_[:min_length]})\ndf.plot(title=\"Train loss against iterations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the training graph looks like the following...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(\"saved_gradient.onnx.training.onnx\", \"rb\") as f:\n    graph = onnx.load(f)\n    for inode, node in enumerate(graph.graph.node):\n        if '' in node.output:\n            for i in range(len(node.output)):\n                if node.output[i] == \"\":\n                    node.output[i] = \"n%d-%d\" % (inode, i)\n\nplot_onnxs(graph, title='Training graph')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The convergence speed is not the same but both gradient descents\ndo not update the gradient multiplier the same way.\n:epkg:`onnxruntime-training` does not implement any gradient descent,\nit just computes the gradient.\nThat's the purpose of :class:`OrtGradientOptimizer\n<onnxcustom.training.optimizers.OrtGradientOptimizer>`. Next example\ndigs into the implementation details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}