{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Train a linear regression with onnxruntime-training in details\n\n:epkg:`onnxruntime-training` only computes the gradient values.\nA gradient descent can then use it to train a model.\nThis example goes step by step from the gradient computation\nto the gradient descent to get a trained linear regression.\n\n## A simple linear regression with scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\nimport numpy\nfrom pandas import DataFrame\nfrom onnx import helper, numpy_helper, TensorProto\nfrom onnxruntime import (\n    InferenceSession, __version__ as ort_version,\n    TrainingParameters, SessionOptions, TrainingSession)\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom onnxcustom.plotting.plotting_onnx import plot_onnxs\nfrom tqdm import tqdm\n\nX, y = make_regression(n_features=2, bias=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nlr = LinearRegression()\nlr.fit(X, y)\nprint(lr.predict(X[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An equivalent ONNX graph.\n\nThis graph can be obtained with *sklearn-onnx`\n(see `l-orttraining-linreg`).\nFor clarity, this step is replaced by\nnext function which builds the exact same graph. It\nimplements a linear regression $y = AX + B$ with onnx operators.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def onnx_linear_regression(coefs, intercept):\n    if len(coefs.shape) == 1:\n        coefs = coefs.reshape((1, -1))\n    coefs = coefs.T\n\n    # input and output\n    X = helper.make_tensor_value_info(\n        'X', TensorProto.FLOAT, [None, coefs.shape[0]])\n    Y = helper.make_tensor_value_info(\n        'Y', TensorProto.FLOAT, [None, coefs.shape[1]])\n\n    # inference\n    node_matmul = helper.make_node('MatMul', ['X', 'coefs'], ['y1'], name='N1')\n    node_add = helper.make_node('Add', ['y1', 'intercept'], ['Y'], name='N2')\n\n    # initializer\n    init_coefs = numpy_helper.from_array(coefs, name=\"coefs\")\n    init_intercept = numpy_helper.from_array(intercept, name=\"intercept\")\n\n    # graph\n    graph_def = helper.make_graph(\n        [node_matmul, node_add], 'lr', [X], [Y],\n        [init_coefs, init_intercept])\n    model_def = helper.make_model(\n        graph_def, producer_name='orttrainer', ir_version=7,\n        producer_version=ort_version,\n        opset_imports=[helper.make_operatorsetid('', 14)])\n    return model_def\n\n\nonx = onnx_linear_regression(lr.coef_.astype(numpy.float32),\n                             lr.intercept_.astype(numpy.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_onnxs(onx, title=\"Linear Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check it produces the same outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(onx.SerializeToString(),\n                        providers=['CPUExecutionProvider'])\nprint(sess.run(None, {'X': X[:5]})[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training with onnxruntime-training\n\nThe model can be trained with a gradient descent algorithm.\nThe previous graph only predicts. A new graph needs to be created\nto compute the loss as function of the inputs and the expected outputs.\nIn our case, it is a square loss.\nThe new graph then requires two inputs, the features and the labels.\nIt has two outputs, the predicted values and the loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def onnx_linear_regression_training(coefs, intercept):\n    if len(coefs.shape) == 1:\n        coefs = coefs.reshape((1, -1))\n    coefs = coefs.T\n\n    # input\n    X = helper.make_tensor_value_info(\n        'X', TensorProto.FLOAT, [None, coefs.shape[0]])\n\n    # expected input\n    label = helper.make_tensor_value_info(\n        'label', TensorProto.FLOAT, [None, coefs.shape[1]])\n\n    # output\n    Y = helper.make_tensor_value_info(\n        'Y', TensorProto.FLOAT, [None, coefs.shape[1]])\n\n    # loss\n    loss = helper.make_tensor_value_info('loss', TensorProto.FLOAT, [])\n\n    # inference\n    node_matmul = helper.make_node('MatMul', ['X', 'coefs'], ['y1'], name='N1')\n    node_add = helper.make_node('Add', ['y1', 'intercept'], ['Y'], name='N2')\n\n    # loss\n    node_diff = helper.make_node('Sub', ['Y', 'label'], ['diff'], name='L1')\n    node_square = helper.make_node(\n        'Mul', ['diff', 'diff'], ['diff2'], name='L2')\n    node_square_sum = helper.make_node(\n        'ReduceSum', ['diff2'], ['loss'], name='L3')\n\n    # initializer\n    init_coefs = numpy_helper.from_array(coefs, name=\"coefs\")\n    init_intercept = numpy_helper.from_array(intercept, name=\"intercept\")\n\n    # graph\n    graph_def = helper.make_graph(\n        [node_matmul, node_add, node_diff, node_square, node_square_sum],\n        'lrt', [X, label], [loss, Y], [init_coefs, init_intercept])\n    model_def = helper.make_model(\n        graph_def, producer_name='orttrainer', ir_version=7,\n        producer_version=ort_version,\n        opset_imports=[helper.make_operatorsetid('', 14)])\n    return model_def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a graph with random coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx_train = onnx_linear_regression_training(\n    numpy.random.randn(*lr.coef_.shape).astype(numpy.float32),\n    numpy.random.randn(\n        *lr.intercept_.reshape((-1, )).shape).astype(numpy.float32))\n\nplot_onnxs(onx_train, title=\"Linear Regression with a loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoader\n\nNext class draws consecutive random observations from a dataset\nby batch. It iterates over the datasets by drawing *n* consecutive\nobservations. This class is equivalent to\n:class:`OrtDataLoader <onnxcustom.training.data_loader.OrtDataLoader>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n    \"\"\"\n    Draws consecutive random observations from a dataset\n    by batch. It iterates over the datasets by drawing\n    *batch_size* consecutive observations.\n\n    :param X: features\n    :param y: labels\n    :param batch_size: batch size (consecutive observations)\n    \"\"\"\n\n    def __init__(self, X, y, batch_size=20):\n        self.X, self.y = X, y\n        self.batch_size = batch_size\n        if len(self.y.shape) == 1:\n            self.y = self.y.reshape((-1, 1))\n        if self.X.shape[0] != self.y.shape[0]:\n            raise ValueError(\n                \"Shape mismatch X.shape=%r, y.shape=%r.\" % (\n                    self.X.shape, self.y.shape))\n\n    def __len__(self):\n        \"Returns the number of observations.\"\n        return self.X.shape[0]\n\n    def __iter__(self):\n        \"\"\"\n        Iterates over the datasets by drawing\n        *batch_size* consecutive observations.\n        \"\"\"\n        N = 0\n        b = len(self) - self.batch_size\n        while N < len(self):\n            i = numpy.random.randint(0, b)\n            N += self.batch_size\n            yield (self.X[i:i + self.batch_size],\n                   self.y[i:i + self.batch_size])\n\n    @property\n    def data(self):\n        \"Returns a tuple of the datasets.\"\n        return self.X, self.y\n\n\ndata_loader = DataLoader(X_train, y_train, batch_size=2)\n\n\nfor i, batch in enumerate(data_loader):\n    if i >= 2:\n        break\n    print(\"batch %r: %r\" % (i, batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First iterations of training\n\nPrediction needs an instance of class *InferenceSession*,\nthe training needs an instance of class *TrainingSession*.\nNext function creates this one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_training_session(\n        training_onnx, weights_to_train, loss_output_name='loss',\n        training_optimizer_name='SGDOptimizer'):\n    \"\"\"\n    Creates an instance of class `TrainingSession`.\n\n    :param training_onnx: ONNX graph used to train\n    :param weights_to_train: names of initializers to be optimized\n    :param loss_output_name: name of the loss output\n    :param training_optimizer_name: optimizer name\n    :return: instance of `TrainingSession`\n    \"\"\"\n    ort_parameters = TrainingParameters()\n    ort_parameters.loss_output_name = loss_output_name\n\n    output_types = {}\n    for output in training_onnx.graph.output:\n        output_types[output.name] = output.type.tensor_type\n\n    ort_parameters.weights_to_train = set(weights_to_train)\n    ort_parameters.training_optimizer_name = training_optimizer_name\n\n    ort_parameters.optimizer_attributes_map = {\n        name: {} for name in weights_to_train}\n    ort_parameters.optimizer_int_attributes_map = {\n        name: {} for name in weights_to_train}\n\n    session_options = SessionOptions()\n    session_options.use_deterministic_compute = True\n\n    session = TrainingSession(\n        training_onnx.SerializeToString(), ort_parameters, session_options)\n    return session\n\n\ntrain_session = create_training_session(onx_train, ['coefs', 'intercept'])\nprint(train_session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look into the expected inputs and outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in train_session.get_inputs():\n    print(\"+input: %s (%s%s)\" % (i.name, i.type, i.shape))\nfor o in train_session.get_outputs():\n    print(\"output: %s (%s%s)\" % (o.name, o.type, o.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A third parameter `Learning_Rate` was automatically added.\nThe training updates the weight with a gradient multiplied\nby this parameter. Let's see now how to\nretrieve the trained coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()\npprint(state_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now check the coefficients are updated after one iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs = {'X': X_train[:1],\n          'label': y_train[:1].reshape((-1, 1)),\n          'Learning_Rate': numpy.array([0.001], dtype=numpy.float32)}\n\ntrain_session.run(None, inputs)\nstate_tensors = train_session.get_state()\npprint(state_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They changed. Another iteration to be sure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs = {'X': X_train[:1],\n          'label': y_train[:1].reshape((-1, 1)),\n          'Learning_Rate': numpy.array([0.001], dtype=numpy.float32)}\nres = train_session.run(None, inputs)\nstate_tensors = train_session.get_state()\npprint(state_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works. The training loss can be obtained by looking into the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\nWe need to implement a gradient descent.\nLet's wrap this into a class similar following scikit-learn's API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CustomTraining:\n    \"\"\"\n    Implements a simple :epkg:`Stochastic Gradient Descent`.\n\n    :param model_onnx: ONNX graph to train\n    :param weights_to_train: list of initializers to train\n    :param loss_output_name: name of output loss\n    :param max_iter: number of training iterations\n    :param training_optimizer_name: optimizing algorithm\n    :param batch_size: batch size (see class *DataLoader*)\n    :param eta0: initial learning rate for the `'constant'`, `'invscaling'`\n        or `'adaptive'` schedules.\n    :param alpha: constant that multiplies the regularization term,\n        the higher the value, the stronger the regularization.\n        Also used to compute the learning rate when set to *learning_rate*\n        is set to `'optimal'`.\n    :param power_t: exponent for inverse scaling learning rate\n    :param learning_rate: learning rate schedule:\n        * `'constant'`: `eta = eta0`\n        * `'optimal'`: `eta = 1.0 / (alpha * (t + t0))` where *t0* is chosen\n            by a heuristic proposed by Leon Bottou.\n        * `'invscaling'`: `eta = eta0 / pow(t, power_t)`\n    :param verbose: use :epkg:`tqdm` to display the training progress\n    \"\"\"\n\n    def __init__(self, model_onnx, weights_to_train, loss_output_name='loss',\n                 max_iter=100, training_optimizer_name='SGDOptimizer',\n                 batch_size=10, eta0=0.01, alpha=0.0001, power_t=0.25,\n                 learning_rate='invscaling', verbose=0):\n        # See https://scikit-learn.org/stable/modules/generated/\n        # sklearn.linear_model.SGDRegressor.html\n        self.model_onnx = model_onnx\n        self.batch_size = batch_size\n        self.weights_to_train = weights_to_train\n        self.loss_output_name = loss_output_name\n        self.training_optimizer_name = training_optimizer_name\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.eta0 = eta0\n        self.alpha = alpha\n        self.power_t = power_t\n        self.learning_rate = learning_rate.lower()\n\n    def _init_learning_rate(self):\n        self.eta0_ = self.eta0\n        if self.learning_rate == \"optimal\":\n            typw = numpy.sqrt(1.0 / numpy.sqrt(self.alpha))\n            self.eta0_ = typw / max(1.0, (1 + typw) * 2)\n            self.optimal_init_ = 1.0 / (self.eta0_ * self.alpha)\n        else:\n            self.eta0_ = self.eta0\n        return self.eta0_\n\n    def _update_learning_rate(self, t, eta):\n        if self.learning_rate == \"optimal\":\n            eta = 1.0 / (self.alpha * (self.optimal_init_ + t))\n        elif self.learning_rate == \"invscaling\":\n            eta = self.eta0_ / numpy.power(t + 1, self.power_t)\n        return eta\n\n    def fit(self, X, y):\n        \"\"\"\n        Trains the model.\n        :param X: features\n        :param y: expected output\n        :return: self\n        \"\"\"\n        self.train_session_ = create_training_session(\n            self.model_onnx, self.weights_to_train,\n            loss_output_name=self.loss_output_name,\n            training_optimizer_name=self.training_optimizer_name)\n\n        data_loader = DataLoader(X, y, batch_size=self.batch_size)\n        lr = self._init_learning_rate()\n        self.input_names_ = [i.name for i in self.train_session_.get_inputs()]\n        self.output_names_ = [\n            o.name for o in self.train_session_.get_outputs()]\n        self.loss_index_ = self.output_names_.index(self.loss_output_name)\n\n        loop = (\n            tqdm(range(self.max_iter))\n            if self.verbose else range(self.max_iter))\n        train_losses = []\n        for it in loop:\n            loss = self._iteration(data_loader, lr)\n            lr = self._update_learning_rate(it, lr)\n            if self.verbose > 1:\n                loop.set_description(\"loss=%1.3g lr=%1.3g\" % (loss, lr))\n            train_losses.append(loss)\n        self.train_losses_ = train_losses\n        self.trained_coef_ = self.train_session_.get_state()\n        return self\n\n    def _iteration(self, data_loader, learning_rate):\n        \"\"\"\n        Processes one gradient iteration.\n\n        :param data_lower: instance of class `DataLoader`\n        :return: loss\n        \"\"\"\n        actual_losses = []\n        lr = numpy.array([learning_rate], dtype=numpy.float32)\n        for batch_idx, (data, target) in enumerate(data_loader):\n            if len(target.shape) == 1:\n                target = target.reshape((-1, 1))\n\n            inputs = {self.input_names_[0]: data,\n                      self.input_names_[1]: target,\n                      self.input_names_[2]: lr}\n            res = self.train_session_.run(None, inputs)\n            actual_losses.append(res[self.loss_index_])\n        return numpy.array(actual_losses).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now train the model in a very similar way\nthat it would be done with *scikit-learn*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = CustomTraining(onx_train, ['coefs', 'intercept'], verbose=1,\n                         max_iter=10)\ntrainer.fit(X, y)\nprint(\"training losses:\", trainer.train_losses_)\n\ndf = DataFrame({\"iteration\": numpy.arange(len(trainer.train_losses_)),\n                \"loss\": trainer.train_losses_})\ndf.set_index('iteration').plot(title=\"Training loss\", logy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare scikit-learn trained coefficients and the coefficients\nobtained with onnxruntime and check they are very close.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"scikit-learn\", lr.coef_, lr.intercept_)\nprint(\"onnxruntime\", trainer.trained_coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works. We could stop here or we could update the weights\nin the training model or the first model. That requires to\nupdate the constants in an ONNX graph.\n\n## Update weights in an ONNX graph\n\nLet's first check the output of the first model in ONNX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(onx.SerializeToString(),\n                        providers=['CPUExecutionProvider'])\nbefore = sess.run(None, {'X': X[:5]})[0]\nprint(before)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's replace the initializer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def update_onnx_graph(model_onnx, new_weights):\n    replace_weights = []\n    replace_indices = []\n    for i, w in enumerate(model_onnx.graph.initializer):\n        if w.name in new_weights:\n            replace_weights.append(\n                numpy_helper.from_array(new_weights[w.name], w.name))\n            replace_indices.append(i)\n    replace_indices.sort(reverse=True)\n    for w_i in replace_indices:\n        del model_onnx.graph.initializer[w_i]\n    model_onnx.graph.initializer.extend(replace_weights)\n\n\nupdate_onnx_graph(onx, trainer.trained_coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare with the previous output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(onx.SerializeToString(),\n                        providers=['CPUExecutionProvider'])\nafter = sess.run(None, {'X': X[:5]})[0]\nprint(after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks almost the same but slighly different.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(after - before)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next example will show how to train a linear regression on GPU:\n`l-orttraining-linreg-gpu`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}