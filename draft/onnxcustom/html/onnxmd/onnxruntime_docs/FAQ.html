
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>FAQ &#8212; onnxcustom</title>
    
    <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/thebelab-helper.js"></script>
    <script src="../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Roadmap.html">
   ONNX Runtime Roadmap
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Privacy.html">
   Privacy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Server.html">
   Build ONNX Runtime Server on Linux
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNX_Runtime_Server_Usage.html">
   How to Use build ONNX Runtime Server for Prediction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   FAQ
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OperatorKernels.html">
   Supported Operators and Data Types
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Versioning.html">
   Versioning
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Coding_Conventions_and_Standards.html">
   ONNX Runtime coding conventions and standards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ABI_Dev_Notes.html">
   Global Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PR_Guidelines.html">
   Guidelines for creating a good pull request
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Model_Test.html">
   Get the test data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NotesOnThreading.html">
   Notes on Threading in ORT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Python_Dev_Notes.html">
   Python Dev Notes
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C_API_Guidelines.html">
   ORT API Guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cmake_guideline.html">
   Scope the impact to minimal
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="onnxruntime_extensions.html">
   ONNXRuntime Extensions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ContribOperators.html">
   Contrib Operator Schemas
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Android_testing.html">
   Testing Android Changes using the Emulator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ORTMobilePackageOperatorTypeSupport.html">
   ONNX Runtime Mobile Pre-Built Package Operator and Type Support
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="WinML_principles.html">
   Contributing to Windows ML
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reduced_Operator_Kernel_build.html">
   ONNX Runtime Reduced Operator Kernel build
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ReleaseManagement.html">
   Release Management
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#do-the-gpu-builds-support-quantized-models">
   Do the GPU builds support quantized models?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-i-change-the-severity-level-of-the-default-logger-to-something-other-than-the-default-warning">
   How do I change the severity level of the default logger to something other than the default (WARNING)?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-i-load-and-run-models-that-have-multiple-inputs-and-outputs-using-the-c-c-api">
   How do I load and run models that have multiple inputs and outputs using the C/C++ API?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-i-force-single-threaded-execution-mode-in-ort-by-default-session-run-uses-all-the-computer-s-cores">
   How do I force single threaded execution mode in ORT? By default, session.run() uses all the computer’s cores.
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="faq">
<h1>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">¶</a></h1>
<p>Here are some commonly raised questions from users of ONNX Runtime and brought up in <a class="reference external" href="https://github.com/microsoft/onnxruntime/issues">Issues</a>.</p>
<section id="do-the-gpu-builds-support-quantized-models">
<h2>Do the GPU builds support quantized models?<a class="headerlink" href="#do-the-gpu-builds-support-quantized-models" title="Permalink to this headline">¶</a></h2>
<p>The default CUDA build supports 3 standard quantization operators: QuantizeLinear, DequantizeLinear, and MatMulInteger. The TensorRT EP has limited support for INT8 quantized ops. In general, support of quantized models through ORT is continuing to expand on a model-driven basis. For performance improvements, quantization is not always required, and we suggest trying alternative strategies to <span class="xref myst">performance tune</span> before determining that quantization is necessary.</p>
</section>
<section id="how-do-i-change-the-severity-level-of-the-default-logger-to-something-other-than-the-default-warning">
<h2>How do I change the severity level of the default logger to something other than the default (WARNING)?<a class="headerlink" href="#how-do-i-change-the-severity-level-of-the-default-logger-to-something-other-than-the-default-warning" title="Permalink to this headline">¶</a></h2>
<p>Setting the severity level to VERBOSE is most useful when debugging errors.</p>
<p>Refer to the API documentation:</p>
<ul class="simple">
<li><p>Python - <a class="reference external" href="https://microsoft.github.io/onnxruntime/python/api_summary.html#onnxruntime.RunOptions.log_severity_level">RunOptions.log_severity_level</a></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnxruntime</span> <span class="k">as</span> <span class="nn">ort</span>
<span class="n">ort</span><span class="o">.</span><span class="n">set_default_logger_severity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>C - <span class="xref myst">SetSessionLogSeverityLevel</span></p></li>
</ul>
</section>
<section id="how-do-i-load-and-run-models-that-have-multiple-inputs-and-outputs-using-the-c-c-api">
<h2>How do I load and run models that have multiple inputs and outputs using the C/C++ API?<a class="headerlink" href="#how-do-i-load-and-run-models-that-have-multiple-inputs-and-outputs-using-the-c-c-api" title="Permalink to this headline">¶</a></h2>
<p>See an example from the ‘override initializer’ test in <span class="xref myst">test_inference.cc</span> that has 3 inputs and 3 outputs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Ort</span><span class="p">::</span><span class="n">Value</span><span class="o">&gt;</span> <span class="n">ort_inputs</span><span class="p">;</span>
<span class="n">ort_inputs</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">move</span><span class="p">(</span><span class="n">label_input_tensor</span><span class="p">));</span>
<span class="n">ort_inputs</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">move</span><span class="p">(</span><span class="n">f2_input_tensor</span><span class="p">));</span>
<span class="n">ort_inputs</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">move</span><span class="p">(</span><span class="n">f11_input_tensor</span><span class="p">));</span>
<span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">const</span> <span class="n">char</span><span class="o">*&gt;</span> <span class="n">input_names</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Label&quot;</span><span class="p">,</span> <span class="s2">&quot;F2&quot;</span><span class="p">,</span> <span class="s2">&quot;F1&quot;</span><span class="p">};</span>
<span class="n">const</span> <span class="n">char</span><span class="o">*</span> <span class="n">const</span> <span class="n">output_names</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Label0&quot;</span><span class="p">,</span> <span class="s2">&quot;F20&quot;</span><span class="p">,</span> <span class="s2">&quot;F11&quot;</span><span class="p">};</span>
<span class="n">std</span><span class="p">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Ort</span><span class="p">::</span><span class="n">Value</span><span class="o">&gt;</span> <span class="n">ort_outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">Run</span><span class="p">(</span><span class="n">Ort</span><span class="p">::</span><span class="n">RunOptions</span><span class="p">{</span><span class="n">nullptr</span><span class="p">},</span> <span class="n">input_names</span><span class="o">.</span><span class="n">data</span><span class="p">(),</span>
<span class="n">ort_inputs</span><span class="o">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">ort_inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">output_names</span><span class="p">,</span> <span class="n">countof</span><span class="p">(</span><span class="n">output_names</span><span class="p">));</span>
</pre></div>
</div>
</section>
<section id="how-do-i-force-single-threaded-execution-mode-in-ort-by-default-session-run-uses-all-the-computer-s-cores">
<h2>How do I force single threaded execution mode in ORT? By default, session.run() uses all the computer’s cores.<a class="headerlink" href="#how-do-i-force-single-threaded-execution-mode-in-ort-by-default-session-run-uses-all-the-computer-s-cores" title="Permalink to this headline">¶</a></h2>
<p>To limit use to a single thread only:</p>
<ul class="simple">
<li><p>If built with OpenMP, set the environment variable OMP_NUM_THREADS to 1. The default inter_op_num_threads in session options is already 1.</p></li>
<li><p>If not built with OpenMP, set the session options intra_op_num_threads to 1. Do not change the default inter_op_num_threads (1).</p></li>
</ul>
<p>It’s recommended to build onnxruntime without openmp if you only need single threaded execution.</p>
<p>This is supported in ONNX Runtime v1.3.0+</p>
<p><strong>Python example:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/python3</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OMP_NUM_THREADS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="kn">import</span> <span class="nn">onnxruntime</span>

<span class="n">opts</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>
<span class="n">opts</span><span class="o">.</span><span class="n">inter_op_num_threads</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">opts</span><span class="o">.</span><span class="n">execution_mode</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">ExecutionMode</span><span class="o">.</span><span class="n">ORT_SEQUENTIAL</span>
<span class="n">ort_session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s1">&#39;/path/to/model.onnx&#39;</span><span class="p">,</span> <span class="n">sess_options</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>C++ example:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">initialize</span>  <span class="n">enviroment</span><span class="o">...</span><span class="n">one</span> <span class="n">enviroment</span> <span class="n">per</span> <span class="n">process</span>
<span class="n">Ort</span><span class="p">::</span><span class="n">Env</span> <span class="n">env</span><span class="p">(</span><span class="n">ORT_LOGGING_LEVEL_WARNING</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">);</span>

<span class="o">//</span> <span class="n">initialize</span> <span class="n">session</span> <span class="n">options</span> <span class="k">if</span> <span class="n">needed</span>
<span class="n">Ort</span><span class="p">::</span><span class="n">SessionOptions</span> <span class="n">session_options</span><span class="p">;</span>
<span class="n">session_options</span><span class="o">.</span><span class="n">SetInterOpNumThreads</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="c1">#ifdef _WIN32</span>
  <span class="n">const</span> <span class="n">wchar_t</span><span class="o">*</span> <span class="n">model_path</span> <span class="o">=</span> <span class="n">L</span><span class="s2">&quot;squeezenet.onnx&quot;</span><span class="p">;</span>
<span class="c1">#else</span>
  <span class="n">const</span> <span class="n">char</span><span class="o">*</span> <span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;squeezenet.onnx&quot;</span><span class="p">;</span>
<span class="c1">#endif</span>

<span class="n">Ort</span><span class="p">::</span><span class="n">Session</span> <span class="n">session</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">session_options</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>