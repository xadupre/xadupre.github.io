
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>ONNX Runtime Roadmap &#8212; onnxcustom</title>
    
    <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/thebelab-helper.js"></script>
    <script src="../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   ONNX Runtime Roadmap
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Privacy.html">
   Privacy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Server.html">
   Build ONNX Runtime Server on Linux
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNX_Runtime_Server_Usage.html">
   How to Use build ONNX Runtime Server for Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FAQ.html">
   FAQ
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OperatorKernels.html">
   Supported Operators and Data Types
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Versioning.html">
   Versioning
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Coding_Conventions_and_Standards.html">
   ONNX Runtime coding conventions and standards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ABI_Dev_Notes.html">
   Global Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PR_Guidelines.html">
   Guidelines for creating a good pull request
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Model_Test.html">
   Get the test data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NotesOnThreading.html">
   Notes on Threading in ORT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Python_Dev_Notes.html">
   Python Dev Notes
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C_API_Guidelines.html">
   ORT API Guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cmake_guideline.html">
   Scope the impact to minimal
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="onnxruntime_extensions.html">
   ONNXRuntime Extensions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ContribOperators.html">
   Contrib Operator Schemas
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Android_testing.html">
   Testing Android Changes using the Emulator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ORTMobilePackageOperatorTypeSupport.html">
   ONNX Runtime Mobile Pre-Built Package Operator and Type Support
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="WinML_principles.html">
   Contributing to Windows ML
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reduced_Operator_Kernel_build.html">
   ONNX Runtime Reduced Operator Kernel build
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ReleaseManagement.html">
   Release Management
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#high-level-goals">
   High Level Goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#investments">
   Investments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expanded-platform-compatibility">
     Expanded platform compatibility
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#architectures">
       Architectures
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#platforms">
       Platforms
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#languages">
       Languages
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accelerators-and-execution-providers">
     Accelerators and Execution Providers
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#new-eps">
       New EPs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cuda-operator-coverage">
       CUDA operator coverage
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simplify-ep-contributions">
       Simplify EP contributions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continued-performance-optimizations">
     Continued Performance Optimizations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examples-of-projects-the-team-is-working-on">
       Examples of projects the team is working on:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#optimizations-for-mobile-and-iot-edge-devices">
       Optimizations for mobile and IoT Edge devices
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expanded-model-compatibility">
     Expanded model compatibility
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#spec-coverage">
       Spec coverage
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#investments-in-popular-converters">
       Investments in popular converters
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#improved-error-handling">
       Improved error handling
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#community-driven-feature-additions">
       Community-driven feature additions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#increased-integration-with-popular-products">
     Increased integration with popular products
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="onnx-runtime-roadmap">
<h1>ONNX Runtime Roadmap<a class="headerlink" href="#onnx-runtime-roadmap" title="Permalink to this headline">¶</a></h1>
<p>ONNX Runtime is an active, fast-paced project backed by a strong team of Microsoft engineers and data scientists along with a worldwide community of partners and contributors. This roadmap summarizes the pending investments identified by the team to continually grow
ONNX Runtime as a robust, versatile, and high performance inference engine for DNN and traditional ML models.</p>
<section id="high-level-goals">
<h2>High Level Goals<a class="headerlink" href="#high-level-goals" title="Permalink to this headline">¶</a></h2>
<p>ONNX Runtime is a runtime accelerator that supports interoperable ML and DNN models based on the <a class="reference external" href="https://onnx.ai/">ONNX</a> spec. For key technical design objectives and considerations, see <span class="xref myst">ONNX Runtime Inference High Level Design</span>.</p>
<p>We recognize the challenges involved in operationalizing ML models performantly in an agile way, and we understand that high volume production services can be highly performance-sensitive and often need to support a variety of compute targets (we experience these first-hand at Microsoft across our vast array of products and services).</p>
<p>As such, our investments are directly in support of solving those challenges, focusing on areas such as:</p>
<ul class="simple">
<li><p>Platform coverage</p></li>
<li><p>Extensibility and customization</p></li>
<li><p>Performance (latency, memory, throughput, scale, etc)</p></li>
<li><p>Model coverage</p></li>
<li><p>Quality and ease of use - including backwards compatibility of models (older opsets) and APIs</p></li>
</ul>
<p>In addition to our OSS participation, we also internally use this technology in core products at Microsoft, with over 80 models in production providing an average of 2x+ performance improvement.</p>
</section>
<section id="investments">
<h2>Investments<a class="headerlink" href="#investments" title="Permalink to this headline">¶</a></h2>
<p>In support of the high level goals outlined above, the investment areas listed below represent our active and backlog projects,
which are largely driven by community demand and anticipated usage opportunities. We will work through our prioritized backlog as
quickly as possible, and if there are any specific features or enhancements you need, we gladly welcome community contributions for
these efforts or any of the <a class="reference external" href="https://github.com/microsoft/onnxruntime/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement">enhancements suggested on Github</a>. If you have a specific suggestion or unsupported use case, please let us
know by filing a <a class="reference external" href="https://github.com/microsoft/onnxruntime/issues">Github issue</a>.</p>
<ul class="simple">
<li><p><strong>Platform coverage</strong> - <span class="xref myst">Expanded platform compatibility</span></p></li>
<li><p><strong>Extensibility and customization</strong> - <span class="xref myst">Accelerators and Execution Providers</span></p></li>
<li><p><strong>Performance</strong> - <span class="xref myst">Continued performance optimizations</span></p></li>
<li><p><strong>Model coverage</strong> - <span class="xref myst">Expanded model compatibility</span></p></li>
<li><p><strong>Quality and ease of use</strong> - <span class="xref myst">Increased integration with popular ML products</span></p></li>
</ul>
<hr class="docutils" />
<section id="expanded-platform-compatibility">
<h3>Expanded platform compatibility<a class="headerlink" href="#expanded-platform-compatibility" title="Permalink to this headline">¶</a></h3>
<p>ONNX Runtime already supports a wide range of architectures, platforms, and languages, and this will continue to be an active investment area to broaden the availability of the engine for varied usage.
Additionally, we understand that lightweight devices and local applications may have constraints for package size, so there is active awareness to opportunistically minimize binary size.</p>
<section id="architectures">
<h4>Architectures<a class="headerlink" href="#architectures" title="Permalink to this headline">¶</a></h4>
<p>Supported</p>
<ul class="simple">
<li><p>X64</p></li>
<li><p>X86</p></li>
<li><p>ARM64</p></li>
<li><p>ARM32 (Limited)</p></li>
</ul>
</section>
<section id="platforms">
<h4>Platforms<a class="headerlink" href="#platforms" title="Permalink to this headline">¶</a></h4>
<p>Supported</p>
<ul class="simple">
<li><p>Windows 7+</p></li>
<li><p>Linux (various)</p></li>
<li><p>Mac OS X</p></li>
<li><p>Android (Preview)</p></li>
<li><p>iOS (Preview)</p></li>
</ul>
</section>
<section id="languages">
<h4>Languages<a class="headerlink" href="#languages" title="Permalink to this headline">¶</a></h4>
<p>Supported languages are listed in <span class="xref myst">API Documentation</span>. The core team is not actively working on other language bindings at this time. If there is a missing API, please file a request in <a class="reference external" href="https://github.com/microsoft/onnxruntime/issues">Issues</a>. Community contributions are welcome for other languages.</p>
</section>
</section>
<section id="accelerators-and-execution-providers">
<h3>Accelerators and Execution Providers<a class="headerlink" href="#accelerators-and-execution-providers" title="Permalink to this headline">¶</a></h3>
<section id="new-eps">
<h4>New EPs<a class="headerlink" href="#new-eps" title="Permalink to this headline">¶</a></h4>
<p>To achieve the best performance on a growing set of compute targets across cloud and the intelligent edge, we invest in and partner with hardware partners and community members to add new execution providers. The flexible pluggability of ONNX Runtime is critical to support a broad range of scenarios and compute options.</p>
<p>Supported EPs are listed <span class="xref myst">here</span>.</p>
</section>
<section id="cuda-operator-coverage">
<h4>CUDA operator coverage<a class="headerlink" href="#cuda-operator-coverage" title="Permalink to this headline">¶</a></h4>
<p>To maximize performance potential, we will be continually adding additional CUDA implementations for supported operators.</p>
</section>
<section id="simplify-ep-contributions">
<h4>Simplify EP contributions<a class="headerlink" href="#simplify-ep-contributions" title="Permalink to this headline">¶</a></h4>
<p>In addition to new execution providers, we aim to make it easy for community partners to contribute in a non-disruptive way. To support this, we are investing in improvements to the execution provider interface for easily registering new execution providers and separating out EPs from the core runtime engine.</p>
</section>
</section>
<section id="continued-performance-optimizations">
<h3>Continued Performance Optimizations<a class="headerlink" href="#continued-performance-optimizations" title="Permalink to this headline">¶</a></h3>
<p>Performance is a key focus for ONNX Runtime. From latency to memory utilization to CPU usage, we are constantly seeking strategies to deliver the best performance. Although DNNs are rapidly driving research areas for innovation, we acknowledge that in practice, many companies and developers are still using traditional ML frameworks for reasons ranging from expertise to privacy to legality. As such, ONNX Runtime is focused on improvements and support for both DNNs and traditional ML.</p>
<section id="examples-of-projects-the-team-is-working-on">
<h4>Examples of projects the team is working on:<a class="headerlink" href="#examples-of-projects-the-team-is-working-on" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>More quantization support</p></li>
<li><p>Improved multithreading (e.g. smarter work sharding, user supplied thread pools, etc)</p></li>
<li><p>Graph optimizations</p></li>
<li><p>Intelligent graph partitioning to maximize the value of different accelerators</p></li>
</ul>
</section>
<section id="optimizations-for-mobile-and-iot-edge-devices">
<h4>Optimizations for mobile and IoT Edge devices<a class="headerlink" href="#optimizations-for-mobile-and-iot-edge-devices" title="Permalink to this headline">¶</a></h4>
<p>IoT provides growing opportunity to execute ML workloads on the edge of the network, where the data is collected. However, the devices used for ML execution have different hardware specifications. To support compatibility with this group of devices, we will invest in strategies to optimize ONNX model execution across the breadth of IoT endpoints using different hardware configurations with CPUs, GPUs and custom NN ASICs.</p>
</section>
</section>
<section id="expanded-model-compatibility">
<h3>Expanded model compatibility<a class="headerlink" href="#expanded-model-compatibility" title="Permalink to this headline">¶</a></h3>
<p>The ONNX spec focuses on ML model interoperability rather than coverage of all operators from all frameworks.
We aim to continuously improve coverage to support popular as well as new state-of-the-art models.</p>
<section id="spec-coverage">
<h4>Spec coverage<a class="headerlink" href="#spec-coverage" title="Permalink to this headline">¶</a></h4>
<p>As more operators are added to the ONNX spec, ONNX Runtime will provide implementations (default CPU and GPU-CUDA) of each to stay in compliance with the latest ONNX spec.</p>
<p>This includes:</p>
<ul class="simple">
<li><p>Sparse Tensor support</p></li>
</ul>
</section>
<section id="investments-in-popular-converters">
<h4>Investments in popular converters<a class="headerlink" href="#investments-in-popular-converters" title="Permalink to this headline">¶</a></h4>
<p>We work with the OSS and ONNX community to ensure popular frameworks can export or be converted to ONNX format.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/onnx.html">PyTorch export</a></p></li>
<li><p><a class="reference external" href="https://github.com/onnx/tensorflow-onnx">Tensorflow-ONNX</a></p></li>
<li><p><a class="reference external" href="https://github.com/onnx/keras-onnx">Keras-ONNX</a></p></li>
<li><p><a class="reference external" href="https://github.com/onnx/sklearn-onnx">Sklearn-ONNX</a></p></li>
<li><p><a class="reference external" href="https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert">ONNXMLTools</a> (CoreML, XGBoost, LibSVM, LightGBM, SparkML)</p></li>
<li><p><a class="reference external" href="https://github.com/dotnet/machinelearning">ML.NET</a></p></li>
</ul>
</section>
<section id="improved-error-handling">
<h4>Improved error handling<a class="headerlink" href="#improved-error-handling" title="Permalink to this headline">¶</a></h4>
<p>To decrease the risk of model inferencing failures, we will improve the error handling and fallback strategies for missing types or unsupported operators. For EPs that have missing or incorrect implementations for ONNX operators, we aim to fallback or fail as gracefully as possible.</p>
</section>
<section id="community-driven-feature-additions">
<h4>Community-driven feature additions<a class="headerlink" href="#community-driven-feature-additions" title="Permalink to this headline">¶</a></h4>
<p>Focusing on practicality, we take a scenario driven approach to adding additional capabilities to ONNX Runtime.</p>
</section>
</section>
<section id="increased-integration-with-popular-products">
<h3>Increased integration with popular products<a class="headerlink" href="#increased-integration-with-popular-products" title="Permalink to this headline">¶</a></h3>
<p>We understand that data scientists and ML engineers work with many different products and toolsets to bring complex machine learning
algorithms to life through innovative user-facing applications. We want to ensure ONNX Runtime works as seamlessly as possible with
these. If you’ve identified any integration ideas or opportunities and have questions or need assistance, we encourage use of Github Issues as a discussion forum.</p>
<p>Some of these products include:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-onnx">AzureML</a>: simplify the process to train, convert, and deploy ONNX models to Azure</p></li>
<li><p><a class="reference external" href="https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability">Model Interpretability</a>: explainability for ONNX models</p></li>
<li><p><a class="reference external" href="https://docs.microsoft.com/en-us/dotnet/machine-learning/tutorials/object-detection-onnx">ML.NET</a>: inference ONNX models in .NET</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/onnx.html">PyTorch</a>: improve coverage for exporting trained models to ONNX</p></li>
<li><p><a class="reference external" href="https://docs.microsoft.com/en-us/windows/ai/windows-ml/index">Windows</a>: run ONNX models on Windows devices using the built-in Windows ML APIs. Windows ML APIs will be included in the ONNX Runtime builds and binaries to enable Windows developers to get OS-independent updates</p></li>
<li><p><a class="reference external" href="https://docs.microsoft.com/en-us/azure/sql-database-edge/deploy-onnx">SQL Database Edge</a>: predict with ONNX models in SQL Database Edge, an optimized relational database engine geared for IoT and IoT Edge deployments</p></li>
</ul>
<p>Have an idea or feature request? <a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/CONTRIBUTING.md">Contribute</a> or <a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/.github/ISSUE_TEMPLATE/feature_request.md">let us know</a>!</p>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>