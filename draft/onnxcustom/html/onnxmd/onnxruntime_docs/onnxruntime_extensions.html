
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>ONNXRuntime Extensions &#8212; onnxcustom</title>
    
    <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/thebelab-helper.js"></script>
    <script src="../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Roadmap.html">
   ONNX Runtime Roadmap
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Privacy.html">
   Privacy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Server.html">
   Build ONNX Runtime Server on Linux
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNX_Runtime_Server_Usage.html">
   How to Use build ONNX Runtime Server for Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FAQ.html">
   FAQ
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OperatorKernels.html">
   Supported Operators and Data Types
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Versioning.html">
   Versioning
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Coding_Conventions_and_Standards.html">
   ONNX Runtime coding conventions and standards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ABI_Dev_Notes.html">
   Global Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PR_Guidelines.html">
   Guidelines for creating a good pull request
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Model_Test.html">
   Get the test data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NotesOnThreading.html">
   Notes on Threading in ORT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Python_Dev_Notes.html">
   Python Dev Notes
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C_API_Guidelines.html">
   ORT API Guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cmake_guideline.html">
   Scope the impact to minimal
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   ONNXRuntime Extensions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ContribOperators.html">
   Contrib Operator Schemas
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Android_testing.html">
   Testing Android Changes using the Emulator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ORTMobilePackageOperatorTypeSupport.html">
   ONNX Runtime Mobile Pre-Built Package Operator and Type Support
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="WinML_principles.html">
   Contributing to Windows ML
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reduced_Operator_Kernel_build.html">
   ONNX Runtime Reduced Operator Kernel build
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ReleaseManagement.html">
   Release Management
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#custom-operators-supported">
   Custom Operators Supported
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#build-onnxruntime-with-extensions">
   Build ONNXRuntime with Extensions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-with-operators-config">
     Build with Operators Config
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-operators-config">
     Generate Operators Config
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-and-disable-exceptions">
     Build and Disable Exceptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-build-command">
     Example Build Command
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#e2e-example-using-custom-operators">
   E2E Example using Custom Operators
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#create-e2e-model">
     Create E2E Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-e2e-model-in-python">
     Run E2E Model in Python
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#run-e2e-model-in-javascript">
     Run E2E Model in JavaScript
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="onnxruntime-extensions">
<h1>ONNXRuntime Extensions<a class="headerlink" href="#onnxruntime-extensions" title="Permalink to this headline">¶</a></h1>
<p>ONNXRuntime Extensions is a comprehensive package to extend the capability of the ONNX conversion and inference. Please visit the documentation <a class="reference external" href="https://github.com/microsoft/onnxruntime-extensions">onnxruntime-extensions</a> to learn more about ONNXRuntime Extensions.</p>
<section id="custom-operators-supported">
<h2>Custom Operators Supported<a class="headerlink" href="#custom-operators-supported" title="Permalink to this headline">¶</a></h2>
<p>onnxruntime-extensions supports many useful custom operators to enhance the text processing capability of ONNXRuntime, which include some widely used <strong>string operators</strong> and popular <strong>tokenizers</strong>. For custom operators supported and how to use them, please check the documentation <a class="reference external" href="https://github.com/microsoft/onnxruntime-extensions/blob/main/docs/custom_text_ops.md">custom operators</a>.</p>
</section>
<section id="build-onnxruntime-with-extensions">
<h2>Build ONNXRuntime with Extensions<a class="headerlink" href="#build-onnxruntime-with-extensions" title="Permalink to this headline">¶</a></h2>
<p>We have supported build onnxruntime-extensions as a static library and link it into ONNXRuntime. To enable custom operators from onnxruntime-extensions, you should add argument <code class="docutils literal notranslate"><span class="pre">--use_extensions</span></code>, which will use onnxruntime-extensions from git submodule in path cmake/external/onnxruntime-extensions <strong>by default</strong>.</p>
<p>If you want to build ONNXRuntime with a pre-pulled onnxruntime-extensions, pass extra argument <code class="docutils literal notranslate"><span class="pre">--extensions_overridden_path</span> <span class="pre">&lt;path-to-onnxruntime-extensions&gt;</span></code>.</p>
<p>Note: Please remember to use <code class="docutils literal notranslate"><span class="pre">--minimal_build</span> <span class="pre">custom_ops</span></code> when you build minimal runtime with custom operators from onnxruntime-extensions.</p>
<section id="build-with-operators-config">
<h3>Build with Operators Config<a class="headerlink" href="#build-with-operators-config" title="Permalink to this headline">¶</a></h3>
<p>Also, you could pass the <strong>required operators config</strong> file by argument <code class="docutils literal notranslate"><span class="pre">--include_ops_by_config</span></code> to customize the operators you want to build in both onnxruntime and onnxruntime-extensions. Example content of <strong>required_operators.config</strong> are:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span># Generated from model/s
# domain;opset;op1,op2...
ai.onnx;12;Add,Cast,Concat,Squeeze
ai.onnx.contrib;1;GPT2Tokenizer,
</pre></div>
</div>
<p>In above operators config, <code class="docutils literal notranslate"><span class="pre">ai.onnx.contrib</span></code> is the domain name of operators in onnxruntime-extensions. We would parse this line to generate required operators in onnxruntime-extensions for build.</p>
</section>
<section id="generate-operators-config">
<h3>Generate Operators Config<a class="headerlink" href="#generate-operators-config" title="Permalink to this headline">¶</a></h3>
<p>To generate the <strong>required_operators.config</strong> file from model, please follow the guidance <a class="reference external" href="https://onnxruntime.ai/docs/how-to/mobile/model-conversion.html">Converting ONNX models to ORT format</a>.</p>
<p>If your model contains operators from onnxruntime-extensions, please add argument <code class="docutils literal notranslate"><span class="pre">--custom_op_library</span></code> and pass the path to <strong>ortcustomops</strong> shared library built following guidance <a class="reference external" href="https://github.com/microsoft/onnxruntime-extensions#the-share-library-for-non-python">share library</a>.</p>
<p>You could even manually edit the <strong>required_operators.config</strong> if you know the custom operators required and don’t want to build the shared library.</p>
</section>
<section id="build-and-disable-exceptions">
<h3>Build and Disable Exceptions<a class="headerlink" href="#build-and-disable-exceptions" title="Permalink to this headline">¶</a></h3>
<p>You could add argument <code class="docutils literal notranslate"><span class="pre">--disable_exceptions</span></code> to disable exceptions in both onnxruntime and onnxruntime-extensions.</p>
<p>However, if the custom operators you used in onnxruntime-extensions (such as BlingFireTokenizer) use c++ exceptions, then you will also need to add argument <code class="docutils literal notranslate"><span class="pre">--enable_wasm_exception_throwing_override</span></code> to enable <strong>Emscripten</strong> to link in exception throwing support library. If this argument is not set, Emscripten will throw linking errors.</p>
</section>
<section id="example-build-command">
<h3>Example Build Command<a class="headerlink" href="#example-build-command" title="Permalink to this headline">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">D:\onnxruntime&gt; build.bat --config Release --build_wasm --enable_wasm_threads --enable_wasm_simd --skip_tests --disable_exceptions --disable_wasm_exception_catching --enable_wasm_exception_throwing_override --disable_rtti --use_extensions --parallel --minimal_build custom_ops --include_ops_by_config D:\required_operators.config</span>
</pre></div>
</div>
</section>
</section>
<section id="e2e-example-using-custom-operators">
<h2>E2E Example using Custom Operators<a class="headerlink" href="#e2e-example-using-custom-operators" title="Permalink to this headline">¶</a></h2>
<p>A common NLP task would probably contain several steps, including pre-processing, DL model and post-processing. It would be very efficient and productive to convert the pre/post processing code snippets into ONNX model since ONNX graph is actually a computation graph, and it can represent the most programming code, theoretically.</p>
<p>Here is an E2E NLP example to show the usage of onnxruntime-extensions:</p>
<section id="create-e2e-model">
<h3>Create E2E Model<a class="headerlink" href="#create-e2e-model" title="Permalink to this headline">¶</a></h3>
<p>You could use ONNX helper functions to create an ONNX model with custom operators.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">helper</span>

<span class="c1"># ...</span>
<span class="n">e2e_nodes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># tokenizer node</span>
<span class="n">tokenizer_node</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s1">&#39;GPT2Tokenizer&#39;</span><span class="p">,</span> <span class="c1"># custom operator supported in onnxruntime-extensions</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_str&#39;</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;token_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">],</span>
    <span class="n">vocab</span><span class="o">=</span><span class="n">get_file_content</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">),</span>
    <span class="n">merges</span><span class="o">=</span><span class="n">get_file_content</span><span class="p">(</span><span class="n">merges_file</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gpt2_tokenizer&#39;</span><span class="p">,</span>
    <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;ai.onnx.contrib&#39;</span> <span class="c1"># domain of custom operator</span>
<span class="p">)</span>
<span class="n">e2e_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokenizer_node</span><span class="p">)</span>

<span class="c1"># deep learning model</span>
<span class="n">dl_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;dl_model.onnx&quot;</span><span class="p">)</span>
<span class="n">dl_nodes</span> <span class="o">=</span> <span class="n">dl_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span>
<span class="n">e2e_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">dl_nodes</span><span class="p">)</span>

<span class="c1"># construct E2E ONNX graph and model</span>
<span class="n">e2e_graph</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_graph</span><span class="p">(</span>
    <span class="n">e2e_nodes</span><span class="p">,</span>
    <span class="s1">&#39;e2e_graph&#39;</span><span class="p">,</span>
    <span class="p">[</span><span class="n">input_tensors</span><span class="p">],</span>
    <span class="p">[</span><span class="n">output_tensors</span><span class="p">],</span>
<span class="p">)</span>
<span class="c1"># ...</span>
</pre></div>
</div>
<p>For more usage of ONNX helper, please visit the document <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md">Python API Overview</a>.</p>
</section>
<section id="run-e2e-model-in-python">
<h3>Run E2E Model in Python<a class="headerlink" href="#run-e2e-model-in-python" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnxruntime</span> <span class="k">as</span> <span class="nn">_ort</span>
<span class="kn">from</span> <span class="nn">onnxruntime_extensions</span> <span class="kn">import</span> <span class="n">get_library_path</span> <span class="k">as</span> <span class="n">_lib_path</span>

<span class="n">so</span> <span class="o">=</span> <span class="n">_ort</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>
<span class="c1"># register onnxruntime-extensions library</span>
<span class="n">so</span><span class="o">.</span><span class="n">register_custom_ops_library</span><span class="p">(</span><span class="n">_lib_path</span><span class="p">())</span>

<span class="c1"># run onnxruntime session</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">_ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">e2e_model</span><span class="p">,</span> <span class="n">so</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-e2e-model-in-javascript">
<h3>Run E2E Model in JavaScript<a class="headerlink" href="#run-e2e-model-in-javascript" title="Permalink to this headline">¶</a></h3>
<p>To run E2E ONNX model in JavaScript, you need to first <a class="reference external" href="https://github.com/microsoft/onnxruntime/tree/master/js">prepare ONNX Runtime WebAssembly artifacts</a>, include the generated <code class="docutils literal notranslate"><span class="pre">ort.min.js</span></code>, and then load and run the model in JS.</p>
<div class="highlight-js notranslate"><div class="highlight"><pre><span></span><span class="c1">// use an async context to call onnxruntime functions</span>
<span class="k">async</span> <span class="kd">function</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">try</span> <span class="p">{</span>
        <span class="c1">// create a new session and load the e2e model</span>
        <span class="kd">const</span> <span class="nx">session</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">ort</span><span class="p">.</span><span class="nx">InferenceSession</span><span class="p">.</span><span class="nx">create</span><span class="p">(</span><span class="s1">&#39;./e2e_model.onnx&#39;</span><span class="p">);</span>

        <span class="c1">// prepare inputs</span>
        <span class="kd">const</span> <span class="nx">tensorA</span> <span class="o">=</span> <span class="ow">new</span> <span class="nx">ort</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">(...);</span>
        <span class="kd">const</span> <span class="nx">tensorB</span> <span class="o">=</span> <span class="ow">new</span> <span class="nx">ort</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">(...);</span>

        <span class="c1">// prepare feeds: use model input names as keys</span>
        <span class="kd">const</span> <span class="nx">feeds</span> <span class="o">=</span> <span class="p">{</span> <span class="nx">a</span><span class="o">:</span> <span class="nx">tensorA</span><span class="p">,</span> <span class="nx">b</span><span class="o">:</span> <span class="nx">tensorB</span> <span class="p">};</span>

        <span class="c1">// feed inputs and run</span>
        <span class="kd">const</span> <span class="nx">results</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">session</span><span class="p">.</span><span class="nx">run</span><span class="p">(</span><span class="nx">feeds</span><span class="p">);</span>

        <span class="c1">// read from results</span>
        <span class="kd">const</span> <span class="nx">dataC</span> <span class="o">=</span> <span class="nx">results</span><span class="p">.</span><span class="nx">c</span><span class="p">.</span><span class="nx">data</span><span class="p">;</span>
        <span class="nb">document</span><span class="p">.</span><span class="nx">write</span><span class="p">(</span><span class="sb">`data of result tensor &#39;c&#39;: </span><span class="si">${</span><span class="nx">dataC</span><span class="si">}</span><span class="sb">`</span><span class="p">);</span>

    <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="nx">e</span><span class="p">)</span> <span class="p">{</span>
        <span class="nb">document</span><span class="p">.</span><span class="nx">write</span><span class="p">(</span><span class="sb">`failed to inference ONNX model: </span><span class="si">${</span><span class="nx">e</span><span class="si">}</span><span class="sb">.`</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>