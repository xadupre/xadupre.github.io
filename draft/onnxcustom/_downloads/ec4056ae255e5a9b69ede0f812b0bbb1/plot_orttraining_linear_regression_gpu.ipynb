{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Train a linear regression with onnxruntime-training on GPU in details\n\nThis example follows the same steps introduced in example\n`l-orttraining-linreg-cpu` but on GPU. This example works\non CPU and GPU but automatically chooses GPU if it is\navailable. The main change in this example is the parameter `device`\nwhich indicates where the computation takes place, on CPU or GPU.\n\n## A simple linear regression with scikit-learn\n\nThis code begins like example `l-orttraining-linreg-cpu`.\nIt creates a graph to train a linear regression initialized\nwith random coefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\nimport numpy\nfrom pandas import DataFrame\nfrom onnx import helper, numpy_helper, TensorProto\nfrom onnxruntime import (\n    __version__ as ort_version, get_device,\n    TrainingParameters, SessionOptions, TrainingSession)\nfrom onnxruntime.capi._pybind_state import (  # pylint: disable=E0611\n    OrtValue as C_OrtValue)\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom onnxcustom.plotting.plotting_onnx import plot_onnxs\nfrom onnxcustom.utils.onnxruntime_helper import get_ort_device\nfrom tqdm import tqdm\n\nX, y = make_regression(n_features=2, bias=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\ndef onnx_linear_regression_training(coefs, intercept):\n    if len(coefs.shape) == 1:\n        coefs = coefs.reshape((1, -1))\n    coefs = coefs.T\n\n    # input\n    X = helper.make_tensor_value_info(\n        'X', TensorProto.FLOAT, [None, coefs.shape[0]])\n\n    # expected input\n    label = helper.make_tensor_value_info(\n        'label', TensorProto.FLOAT, [None, coefs.shape[1]])\n\n    # output\n    Y = helper.make_tensor_value_info(\n        'Y', TensorProto.FLOAT, [None, coefs.shape[1]])\n\n    # loss\n    loss = helper.make_tensor_value_info('loss', TensorProto.FLOAT, [])\n\n    # inference\n    node_matmul = helper.make_node('MatMul', ['X', 'coefs'], ['y1'], name='N1')\n    node_add = helper.make_node('Add', ['y1', 'intercept'], ['Y'], name='N2')\n\n    # loss\n    node_diff = helper.make_node('Sub', ['Y', 'label'], ['diff'], name='L1')\n    node_square = helper.make_node(\n        'Mul', ['diff', 'diff'], ['diff2'], name='L2')\n    node_square_sum = helper.make_node(\n        'ReduceSum', ['diff2'], ['loss'], name='L3')\n\n    # initializer\n    init_coefs = numpy_helper.from_array(coefs, name=\"coefs\")\n    init_intercept = numpy_helper.from_array(intercept, name=\"intercept\")\n\n    # graph\n    graph_def = helper.make_graph(\n        [node_matmul, node_add, node_diff, node_square, node_square_sum],\n        'lrt', [X, label], [loss, Y], [init_coefs, init_intercept])\n    model_def = helper.make_model(\n        graph_def, producer_name='orttrainer', ir_version=7,\n        producer_version=ort_version,\n        opset_imports=[helper.make_operatorsetid('', 14)])\n    return model_def\n\n\nonx_train = onnx_linear_regression_training(\n    numpy.random.randn(2).astype(numpy.float32),\n    numpy.random.randn(1).astype(numpy.float32))\n\nplot_onnxs(onx_train, title=\"Graph with Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First iterations of training on GPU\n\nPrediction needs an instance of class *InferenceSession*,\nthe training needs an instance of class *TrainingSession*.\nNext function creates this one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if get_device().upper() == 'GPU' else 'cpu'\n\nprint(\"device=%r get_device()=%r\" % (device, get_device()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function creating the training session.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_training_session(\n        training_onnx, weights_to_train, loss_output_name='loss',\n        training_optimizer_name='SGDOptimizer', device='cpu'):\n    \"\"\"\n    Creates an instance of class `TrainingSession`.\n\n    :param training_onnx: ONNX graph used to train\n    :param weights_to_train: names of initializers to be optimized\n    :param loss_output_name: name of the loss output\n    :param training_optimizer_name: optimizer name\n    :param device: `'cpu'` or `'cuda'`\n    :return: instance of `TrainingSession`\n    \"\"\"\n    ort_parameters = TrainingParameters()\n    ort_parameters.loss_output_name = loss_output_name\n\n    output_types = {}\n    for output in training_onnx.graph.output:\n        output_types[output.name] = output.type.tensor_type\n\n    ort_parameters.weights_to_train = set(weights_to_train)\n    ort_parameters.training_optimizer_name = training_optimizer_name\n\n    ort_parameters.optimizer_attributes_map = {\n        name: {} for name in weights_to_train}\n    ort_parameters.optimizer_int_attributes_map = {\n        name: {} for name in weights_to_train}\n\n    session_options = SessionOptions()\n    session_options.use_deterministic_compute = True\n\n    if hasattr(device, 'device_type'):\n        if device.device_type() == device.cpu():\n            provider = ['CPUExecutionProvider']\n        elif device.device_type() == device.cuda():\n            provider = ['CUDAExecutionProvider']\n        else:\n            raise ValueError(\"Unexpected device %r.\" % device)\n    else:\n        if device == 'cpu':\n            provider = ['CPUExecutionProvider']\n        elif device.startswith(\"cuda\"):\n            provider = ['CUDAExecutionProvider']\n        else:\n            raise ValueError(\"Unexpected device %r.\" % device)\n\n    session = TrainingSession(\n        training_onnx.SerializeToString(), ort_parameters, session_options,\n        providers=provider)\n    return session\n\n\ntrain_session = create_training_session(\n    onx_train, ['coefs', 'intercept'], device=device)\nprint(train_session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()\npprint(state_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now check the coefficients are updated after one iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = get_ort_device(device)\nortx = C_OrtValue.ortvalue_from_numpy(X_train[:1], dev)\norty = C_OrtValue.ortvalue_from_numpy(y_train[:1].reshape((-1, 1)), dev)\nortlr = C_OrtValue.ortvalue_from_numpy(\n    numpy.array([0.01], dtype=numpy.float32), dev)\n\nbind = train_session.io_binding()._iobinding\nbind.bind_ortvalue_input('X', ortx)\nbind.bind_ortvalue_input('label', orty)\nbind.bind_ortvalue_input('Learning_Rate', ortlr)\nbind.bind_output('loss', dev)\ntrain_session._sess.run_with_iobinding(bind, None)\noutputs = bind.copy_outputs_to_cpu()\npprint(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check the coefficients have changed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_tensors = train_session.get_state()\npprint(state_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training on GPU\n\nWe still need to implement a gradient descent.\nLet's wrap this into a class similar following scikit-learn's API.\nIt needs to have an extra parameter *device*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DataLoaderDevice:\n    \"\"\"\n    Draws consecutive random observations from a dataset\n    by batch. It iterates over the datasets by drawing\n    *batch_size* consecutive observations.\n\n    :param X: features\n    :param y: labels\n    :param batch_size: batch size (consecutive observations)\n    :param device: `'cpu'`, `'cuda'`, `'cuda:0'`, ...\n    \"\"\"\n\n    def __init__(self, X, y, batch_size=20, device='cpu'):\n        if len(y.shape) == 1:\n            y = y.reshape((-1, 1))\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\n                \"Shape mismatch X.shape=%r, y.shape=%r.\" % (X.shape, y.shape))\n        self.X = numpy.ascontiguousarray(X)\n        self.y = numpy.ascontiguousarray(y)\n        self.batch_size = batch_size\n        self.device = get_ort_device(device)\n\n    def __len__(self):\n        \"Returns the number of observations.\"\n        return self.X.shape[0]\n\n    def __iter__(self):\n        \"\"\"\n        Iterates over the datasets by drawing\n        *batch_size* consecutive observations.\n        \"\"\"\n        N = 0\n        b = len(self) - self.batch_size\n        while N < len(self):\n            i = numpy.random.randint(0, b)\n            N += self.batch_size\n            yield (\n                C_OrtValue.ortvalue_from_numpy(\n                    self.X[i:i + self.batch_size],\n                    self.device),\n                C_OrtValue.ortvalue_from_numpy(\n                    self.y[i:i + self.batch_size],\n                    self.device))\n\n    @property\n    def data(self):\n        \"Returns a tuple of the datasets.\"\n        return self.X, self.y\n\n\ndata_loader = DataLoaderDevice(X_train, y_train, batch_size=2)\n\n\nfor i, batch in enumerate(data_loader):\n    if i >= 2:\n        break\n    print(\"batch %r: %r\" % (i, batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CustomTraining:\n    \"\"\"\n    Implements a simple :epkg:`Stochastic Gradient Descent`.\n\n    :param model_onnx: ONNX graph to train\n    :param weights_to_train: list of initializers to train\n    :param loss_output_name: name of output loss\n    :param max_iter: number of training iterations\n    :param training_optimizer_name: optimizing algorithm\n    :param batch_size: batch size (see class *DataLoader*)\n    :param eta0: initial learning rate for the `'constant'`, `'invscaling'`\n        or `'adaptive'` schedules.\n    :param alpha: constant that multiplies the regularization term,\n        the higher the value, the stronger the regularization.\n        Also used to compute the learning rate when set to *learning_rate*\n        is set to `'optimal'`.\n    :param power_t: exponent for inverse scaling learning rate\n    :param learning_rate: learning rate schedule:\n        * `'constant'`: `eta = eta0`\n        * `'optimal'`: `eta = 1.0 / (alpha * (t + t0))` where *t0* is chosen\n            by a heuristic proposed by Leon Bottou.\n        * `'invscaling'`: `eta = eta0 / pow(t, power_t)`\n    :param device: `'cpu'` or `'cuda'`\n    :param verbose: use :epkg:`tqdm` to display the training progress\n    \"\"\"\n\n    def __init__(self, model_onnx, weights_to_train, loss_output_name='loss',\n                 max_iter=100, training_optimizer_name='SGDOptimizer',\n                 batch_size=10, eta0=0.01, alpha=0.0001, power_t=0.25,\n                 learning_rate='invscaling', device='cpu', verbose=0):\n        # See https://scikit-learn.org/stable/modules/generated/\n        # sklearn.linear_model.SGDRegressor.html\n        self.model_onnx = model_onnx\n        self.batch_size = batch_size\n        self.weights_to_train = weights_to_train\n        self.loss_output_name = loss_output_name\n        self.training_optimizer_name = training_optimizer_name\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.eta0 = eta0\n        self.alpha = alpha\n        self.power_t = power_t\n        self.learning_rate = learning_rate.lower()\n        self.device = get_ort_device(device)\n\n    def _init_learning_rate(self):\n        self.eta0_ = self.eta0\n        if self.learning_rate == \"optimal\":\n            typw = numpy.sqrt(1.0 / numpy.sqrt(self.alpha))\n            self.eta0_ = typw / max(1.0, (1 + typw) * 2)\n            self.optimal_init_ = 1.0 / (self.eta0_ * self.alpha)\n        else:\n            self.eta0_ = self.eta0\n        return self.eta0_\n\n    def _update_learning_rate(self, t, eta):\n        if self.learning_rate == \"optimal\":\n            eta = 1.0 / (self.alpha * (self.optimal_init_ + t))\n        elif self.learning_rate == \"invscaling\":\n            eta = self.eta0_ / numpy.power(t + 1, self.power_t)\n        return eta\n\n    def fit(self, X, y):\n        \"\"\"\n        Trains the model.\n        :param X: features\n        :param y: expected output\n        :return: self\n        \"\"\"\n        self.train_session_ = create_training_session(\n            self.model_onnx, self.weights_to_train,\n            loss_output_name=self.loss_output_name,\n            training_optimizer_name=self.training_optimizer_name,\n            device=self.device)\n\n        data_loader = DataLoaderDevice(\n            X, y, batch_size=self.batch_size, device=self.device)\n        lr = self._init_learning_rate()\n        self.input_names_ = [i.name for i in self.train_session_.get_inputs()]\n        self.output_names_ = [\n            o.name for o in self.train_session_.get_outputs()]\n        self.loss_index_ = self.output_names_.index(self.loss_output_name)\n\n        bind = self.train_session_.io_binding()._iobinding\n\n        loop = (\n            tqdm(range(self.max_iter))\n            if self.verbose else range(self.max_iter))\n        train_losses = []\n        for it in loop:\n            bind_lr = C_OrtValue.ortvalue_from_numpy(\n                numpy.array([lr], dtype=numpy.float32),\n                self.device)\n            loss = self._iteration(data_loader, bind_lr, bind)\n            lr = self._update_learning_rate(it, lr)\n            if self.verbose > 1:\n                loop.set_description(\"loss=%1.3g lr=%1.3g\" % (loss, lr))\n            train_losses.append(loss)\n        self.train_losses_ = train_losses\n        self.trained_coef_ = self.train_session_.get_state()\n        return self\n\n    def _iteration(self, data_loader, learning_rate, bind):\n        actual_losses = []\n        for batch_idx, (data, target) in enumerate(data_loader):\n\n            bind.bind_ortvalue_input(self.input_names_[0], data)\n            bind.bind_ortvalue_input(self.input_names_[1], target)\n            bind.bind_ortvalue_input(self.input_names_[2], learning_rate)\n            bind.bind_output('loss', self.device)\n            self.train_session_._sess.run_with_iobinding(bind, None)\n            outputs = bind.copy_outputs_to_cpu()\n            actual_losses.append(outputs[self.loss_index_])\n        return numpy.array(actual_losses).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now train the model in a very similar way\nthat it would be done with *scikit-learn*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = CustomTraining(onx_train, ['coefs', 'intercept'], verbose=1,\n                         max_iter=10, device=device)\ntrainer.fit(X, y)\nprint(\"training losses:\", trainer.train_losses_)\n\ndf = DataFrame({\"iteration\": numpy.arange(len(trainer.train_losses_)),\n                \"loss\": trainer.train_losses_})\ndf.set_index('iteration').plot(title=\"Training loss\", logy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"onnxruntime\", trainer.trained_coef_)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}