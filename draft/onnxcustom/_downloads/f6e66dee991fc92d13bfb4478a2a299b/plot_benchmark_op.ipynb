{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark and profile of operator Slice\n\nThis short code compares the execution of the operator *Slice*\nbetween :epkg:`numpy` and :epkg:`onnxruntime` for three\nconfigurations.\n\n## A simple example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\nimport numpy\nfrom numpy.testing import assert_almost_equal\nimport pandas\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\nfrom onnxruntime import InferenceSession, get_device, SessionOptions\nfrom onnxruntime.capi._pybind_state import (  # pylint: disable=E0611\n    OrtValue as C_OrtValue)\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom skl2onnx.algebra.onnx_ops import OnnxSlice, OnnxAdd, OnnxMul\nfrom cpyquickhelper.numbers import measure_time\nfrom tqdm import tqdm\nfrom mlprodict.testing.experimental_c_impl.experimental_c import (\n    code_optimisation)\nfrom mlprodict.onnxrt.ops_whole.session import OnnxWholeSession\nfrom onnxcustom.utils.onnxruntime_helper import get_ort_device\n\nprint([code_optimisation(), get_device()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The functions to compare.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_ort_op(op_version=14, save=None, **kwargs):  # opset=13, 14, ...\n    slices = kwargs['slices']\n    slice1, slice2 = slices\n    slice1 = slice(0, None) if slice1 is None else slice(*slice1)\n    slice2 = slice(0, None) if slice2 is None else slice(*slice2)\n\n    axes = []\n    starts = []\n    ends = []\n    for i in [0, 1]:\n        if slices[i] is None:\n            continue\n        axes.append(i)\n        starts.append(slices[i][0])\n        ends.append(slices[i][1])\n    starts = numpy.array(starts, dtype=numpy.int64)\n    ends = numpy.array(ends, dtype=numpy.int64)\n    axes = numpy.array(axes, dtype=numpy.int64)\n    node1 = OnnxSlice('X', starts, ends, axes, op_version=op_version)\n    node2 = OnnxAdd(node1, numpy.array([1], dtype=numpy.float32),\n                    op_version=op_version)\n    node3 = OnnxSlice(node2, starts, ends, axes,\n                      op_version=op_version)\n    node4 = OnnxMul(node3, numpy.array([2], dtype=numpy.float32),\n                    op_version=op_version, output_names=['Y'])\n    onx = node4.to_onnx(inputs=[('X', FloatTensorType([None, None]))],\n                        target_opset=op_version)\n    sess = InferenceSession(onx.SerializeToString(),\n                            providers=[\"CPUExecutionProvider\"])\n    if save is not None:\n        with open(save, \"wb\") as f:\n            f.write(onx.SerializeToString())\n\n    def npy_fct(x):\n        return ((x[slice1, slice2] + 1)[slice1, slice2] * 2).copy()\n\n    rnd = numpy.random.randn(10, 10).astype(numpy.float32)\n    expected = npy_fct(rnd)\n    got = sess.run(None, {'X': rnd})[0]\n    try:\n        assert_almost_equal(expected, got)\n    except AssertionError as e:\n        raise AssertionError(\n            \"kwargs=%r slice1=%r slice2=%r shapes=%r ? %r \"\n            \"(x[slice1, slice2].shape)=%r\" % (\n                kwargs, slice1, slice2, expected.shape,\n                got.shape, rnd[slice1, slice2].shape)) from e\n\n    if get_device().upper() == 'GPU':\n        sessg = InferenceSession(onx.SerializeToString(),\n                                 providers=[\"CUDAExecutionProvider\"])\n        io_binding = sessg.io_binding()._iobinding\n        device = get_ort_device('cuda:0')\n\n        def run_gpu(x):\n            io_binding.bind_input(\n                'X', device, numpy.float32, x.shape(), x.data_ptr())\n            io_binding.bind_output('Y', device)\n            return sessg._sess.run_with_iobinding(io_binding, None)\n\n        return onx, lambda x: sess.run(None, {'X': x}), npy_fct, run_gpu\n    else:\n        return onx, lambda x: sess.run(None, {'X': x}), npy_fct, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def loop_fct(fct, xs):\n    for x in xs:\n        fct(x)\n\n\ndef benchmark_op(repeat=10, number=10, name=\"Slice\", shape_slice_fct=None,\n                 save=None, opset=14, repeat_profile=1500, verbose=1):\n    if verbose:\n        print(\"[benchmark_op] start repeat=%d number=%d repeat_profile=%d\"\n              \" opset=%d.\" % (repeat, number, repeat_profile, opset))\n    res = []\n    for dim in tqdm([8, 16, 32, 64, 100, 128, 200,\n                     256, 400, 512, 600, 784, 800,\n                     1000, 1024, 1200]):\n        shape, slices = shape_slice_fct(dim)\n        onx, ort_fct, npy_fct, ort_fct_gpu = build_ort_op(\n            save=save, op_version=opset, slices=slices)\n\n        n_arrays = 20\n        if dim >= 512:\n            n_arrays = 10\n        xs = [numpy.random.rand(*shape).astype(numpy.float32)\n              for _ in range(n_arrays)]\n        info = dict(shape=shape)\n\n        ctx = dict(xs=xs, loop_fct=loop_fct)\n\n        # numpy\n        ctx['fct'] = npy_fct\n        obs = measure_time(\n            lambda: loop_fct(npy_fct, xs),\n            div_by_number=True, context=ctx, repeat=repeat, number=number)\n        obs['dim'] = dim\n        obs['fct'] = 'numpy'\n        obs['shape'] = \",\".join(map(str, shape))\n        obs['slices'] = str(slices)\n        obs.update(info)\n        res.append(obs)\n\n        # onnxruntime\n        ctx['fct'] = ort_fct\n        obs = measure_time(\n            lambda: loop_fct(ort_fct, xs),\n            div_by_number=True, context=ctx, repeat=repeat, number=number)\n        obs['dim'] = dim\n        obs['fct'] = 'ort'\n        obs['shape'] = \",\".join(map(str, shape))\n        obs['slices'] = str(slices)\n        obs.update(info)\n        res.append(obs)\n\n        if ort_fct_gpu is not None:\n\n            # onnxruntime\n            dev = get_ort_device('cuda:0')\n            ctx['xs'] = [\n                C_OrtValue.ortvalue_from_numpy(x, dev)\n                for x in xs]\n            ctx['fct'] = ort_fct_gpu\n            obs = measure_time(\n                lambda: loop_fct(ort_fct_gpu, ctx['xs']),\n                div_by_number=True, context=ctx, repeat=repeat, number=number)\n            obs['dim'] = dim\n            obs['fct'] = 'ort_gpu'\n            obs['shape'] = \",\".join(map(str, shape))\n            obs['slices'] = str(slices)\n            obs.update(info)\n            res.append(obs)\n\n    # profiling CPU\n    if verbose:\n        print(\"[benchmark_op] done.\")\n        print(\"[benchmark_op] profile CPU.\")\n    so = SessionOptions()\n    so.enable_profiling = True\n    sess = InferenceSession(onx.SerializeToString(), so,\n                            providers=[\"CPUExecutionProvider\"])\n    for i in range(0, repeat_profile):\n        sess.run(None, {'X': xs[-1]}, )\n    prof = sess.end_profiling()\n    with open(prof, \"r\") as f:\n        js = json.load(f)\n    dfprof = DataFrame(OnnxWholeSession.process_profiling(js))\n    dfprof['shape'] = \",\".join(map(str, shape))\n    dfprof['slices'] = str(slices)\n    if verbose:\n        print(\"[benchmark_op] done.\")\n\n    # profiling CPU\n    if ort_fct_gpu is not None:\n        if verbose:\n            print(\"[benchmark_op] profile GPU.\")\n        so = SessionOptions()\n        so.enable_profiling = True\n        sess = InferenceSession(onx.SerializeToString(), so,\n                                providers=[\"CUDAExecutionProvider\"])\n        io_binding = sess.io_binding()._iobinding\n        device = get_ort_device('cpu')\n\n        for i in range(0, repeat_profile):\n            x = ctx['xs'][-1]\n            io_binding.bind_input(\n                'X', device, numpy.float32, x.shape(), x.data_ptr())\n            io_binding.bind_output('Y', device)\n            sess._sess.run_with_iobinding(io_binding, None)\n\n        prof = sess.end_profiling()\n        with open(prof, \"r\") as f:\n            js = json.load(f)\n        dfprofgpu = DataFrame(OnnxWholeSession.process_profiling(js))\n        dfprofgpu['shape'] = \",\".join(map(str, shape))\n        dfprofgpu['slices'] = str(slices)\n        if verbose:\n            print(\"[benchmark_op] profile done.\")\n    else:\n        dfprofgpu = None\n\n    # Dataframes\n    shape_name = str(shape).replace(str(dim), \"N\")\n    df = pandas.DataFrame(res)\n    piv = df.pivot('shape', 'fct', 'average')\n\n    rs = piv.copy()\n    for c in ['numpy', 'ort', 'ort_gpu']:\n        if c in rs.columns:\n            rs[\"numpy/%s\" % c] = rs['numpy'] / rs[c]\n    rs = rs[[c for c in rs.columns if \"/numpy\" not in c]].copy()\n\n    # Graphs.\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    piv.plot(logx=True, logy=True, ax=ax[0],\n             title=\"%s benchmark\\n%r\"\n                   \" lower better\" % (name, shape_name))\n    ax[0].legend(prop={\"size\": 9})\n    rs.plot(logx=True, logy=True, ax=ax[1],\n            title=\"%s Speedup, baseline=numpy\\n%r\"\n                  \" higher better\" % (name, shape_name))\n    ax[1].plot([min(rs.index), max(rs.index)], [0.5, 0.5], 'g--')\n    ax[1].plot([min(rs.index), max(rs.index)], [2., 2.], 'g--')\n    ax[1].legend(prop={\"size\": 9})\n    return dfprof, dfprofgpu, df, rs, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nth = int(code_optimisation().split('=')[1])\ncols_profile = [\"shape\", \"slices\", \"args_op_name\", 'args_provider']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## shape = (100, N) - slice = [1:-1], :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfs = []\ndfprof, dfprofgpu, df, piv, ax = benchmark_op(\n    shape_slice_fct=lambda dim: ((256, dim), ((1, -1), None)),\n    save=\"bslice.onnx\", number=nth * 4, repeat=8, repeat_profile=100 * nth)\n\ndfs.append(df)\npiv2 = df.pivot(\"fct\", \"shape\", \"average\")\nprint(\"slices = [1:-1], :\")\nprint(piv.to_markdown())\nprint(dfprof.drop(['pid', 'tid', 'ts'], axis=1).groupby(\n    cols_profile).sum().to_markdown())\nif dfprofgpu is not None:\n    print(dfprofgpu.drop(['pid', 'tid'], axis=1).groupby(\n        cols_profile).sum().to_markdown())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## shape = (100, N) - slice = :, [1:-1]\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfs = []\ndfprof, dfprofgpu, df, piv, ax = benchmark_op(\n    shape_slice_fct=lambda dim: ((256, dim), (None, (1, -1))),\n    save=\"bslice.onnx\", number=nth * 4, repeat=8, repeat_profile=100 * nth)\n\ndfs.append(df)\npiv2 = df.pivot(\"fct\", \"shape\", \"average\")\nprint(\"slices = :, [1:-1]\")\nprint(piv.to_markdown())\nprint(dfprof.drop(['pid', 'tid', 'ts'], axis=1).groupby(\n    cols_profile).sum().to_markdown())\nif dfprofgpu is not None:\n    print(dfprofgpu.drop(['pid', 'tid'], axis=1).groupby(\n        cols_profile).sum().to_markdown())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## shape = (100, N) - slice = [1:-1], [1:-1]\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfs = []\ndfprof, dfprofgpu, df, piv, ax = benchmark_op(\n    shape_slice_fct=lambda dim: ((256, dim), ((1, -1), (1, -1))),\n    save=\"bslice.onnx\", number=nth * 4, repeat=8, repeat_profile=100 * nth)\n\ndfs.append(df)\npiv2 = df.pivot(\"fct\", \"shape\", \"average\")\nprint(\"slices = [1:-1], [1:-1]\")\nprint(piv.to_markdown())\nprint(dfprof.drop(['pid', 'tid', 'ts'], axis=1).groupby(\n    cols_profile).sum().to_markdown())\nif dfprofgpu is not None:\n    print(dfprofgpu.drop(['pid', 'tid'], axis=1).groupby(\n        cols_profile).sum().to_markdown())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}