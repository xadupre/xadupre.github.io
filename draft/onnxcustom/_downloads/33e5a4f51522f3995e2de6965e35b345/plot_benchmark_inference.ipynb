{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark inference for a linear regression\n\nThis short code compares the execution of a couple of runtime\nfor inference including :epkg:`onnxruntime`. This benchmark\nleverages the example `Benchmark Linear Regression\n<http://www.xavierdupre.fr/app/mlprodict/helpsphinx/\ngyexamples/plot_opml_linear_regression.html>`_.\nThis simple model is useful to measure unsignificant cost\nfor large models.\n\n## Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nfrom time import perf_counter as time\nfrom multiprocessing import cpu_count\nimport numpy\nfrom numpy.random import rand\nfrom numpy.testing import assert_almost_equal\nimport matplotlib.pyplot as plt\nimport pandas\nfrom onnxruntime import InferenceSession\nfrom onnxruntime.capi._pybind_state import (  # pylint: disable=E0611\n    SessionIOBinding, OrtDevice as C_OrtDevice)\nfrom sklearn import config_context\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.utils._testing import ignore_warnings\nfrom skl2onnx import to_onnx\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom mlprodict.onnxrt import OnnxInference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nprint(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementations to benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fcts_model(X, y, n_jobs):\n    \"LinearRegression.\"\n    model = LinearRegression(n_jobs=n_jobs)\n    model.fit(X, y)\n\n    initial_types = [('X', FloatTensorType([None, X.shape[1]]))]\n    onx = to_onnx(model, initial_types=initial_types,\n                  black_op={'LinearRegressor'})\n    sess = InferenceSession(onx.SerializeToString(),\n                            providers=['CPUExecutionProvider'])\n    outputs = [o.name for o in sess.get_outputs()]\n    oinf = OnnxInference(onx, runtime=\"python\")\n    bind = SessionIOBinding(sess._sess)\n    # ort_device = C_OrtDevice.cpu()\n    ort_device = C_OrtDevice(\n        C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)\n\n    def predict_skl_predict(X, model=model):\n        return model.predict(X)\n\n    def predict_onnxrt_predict(X, sess=sess):\n        return sess.run(outputs[:1], {'X': X})[0]\n\n    def predict_onnx_inference(X, oinf=oinf):\n        return oinf.run({'X': X})[\"variable\"]\n\n    def predict_onnxrt_predict_bind(X, sess=sess, bind=bind,\n                                    ort_device=ort_device):\n        if X.__array_interface__['strides'] is not None:\n            raise RuntimeError(\"onnxruntime only supports contiguous arrays.\")\n        bind.bind_input('X', ort_device, X.dtype, X.shape,\n                        X.__array_interface__['data'][0])\n        bind.bind_output('variable', ort_device)\n        sess._sess.run_with_iobinding(bind, None)\n        ortvalues = bind.get_outputs()\n        return ortvalues[0].numpy()\n\n    return {'predict': {\n        'skl': predict_skl_predict,\n        'ort': predict_onnxrt_predict,\n        'numpy': predict_onnx_inference,\n        'ort-bind': predict_onnxrt_predict_bind\n    }}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def allow_configuration(**kwargs):\n    return True\n\n\ndef bench(n_obs, n_features, n_jobss,\n          methods, repeat=10, verbose=False):\n    res = []\n    for nfeat in n_features:\n\n        ntrain = 50000\n        X_train = numpy.empty((ntrain, nfeat)).astype(numpy.float32)\n        X_train[:, :] = rand(ntrain, nfeat)[:, :]\n        eps = rand(ntrain) - 0.5\n        y_train = X_train.sum(axis=1) + eps\n\n        for n_jobs in n_jobss:\n            fcts = fcts_model(X_train, y_train, n_jobs)\n\n            for n in n_obs:\n                for method in methods:\n\n                    if not allow_configuration(n=n, nfeat=nfeat,\n                                               n_jobs=n_jobs, method=method):\n                        continue\n\n                    obs = dict(n_obs=n, nfeat=nfeat, method=method,\n                               n_jobs=n_jobs)\n\n                    # creates different inputs to avoid caching in any ways\n                    Xs = []\n                    for r in range(repeat):\n                        x = numpy.empty((n, nfeat))\n                        x[:, :] = rand(n, nfeat)[:, :]\n                        Xs.append(x.astype(numpy.float32))\n\n                    for name, fct in fcts[method].items():\n\n                        if name == 'skl':\n                            # measures the baseline\n                            with config_context(assume_finite=True):\n                                st = time()\n                                repeated = 0\n                                for X in Xs:\n                                    p1 = fct(X)\n                                    repeated += 1\n                                    if time() - st >= 1:\n                                        break  # stops if longer than a second\n                                end = time()\n                                obs[\"time_skl\"] = (end - st) / repeated\n                        else:\n                            st = time()\n                            r2 = 0\n                            for X in Xs:\n                                p2 = fct(X)\n                                r2 += 1\n                                if r2 >= repeated:\n                                    break\n                            end = time()\n                            obs[\"time_\" + name] = (end - st) / r2\n\n                    # final\n                    res.append(obs)\n                    if verbose and (len(res) % 1 == 0 or n >= 10000):\n                        print(\"bench\", len(res), \":\", obs)\n\n                    # checks that both produce the same outputs\n                    if n <= 10000:\n                        if len(p1.shape) == 1 and len(p2.shape) == 2:\n                            p2 = p2.ravel()\n                        try:\n                            assert_almost_equal(\n                                p1.ravel(), p2.ravel(), decimal=5)\n                        except AssertionError as e:\n                            warnings.warn(str(e))\n    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_rf_models(dfr):\n\n    def autolabel(ax, rects):\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('%1.1fx' % height,\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom',\n                        fontsize=8)\n\n    engines = [_.split('_')[-1] for _ in dfr.columns if _.startswith(\"time_\")]\n    engines = [_ for _ in engines if _ != 'skl']\n    for engine in engines:\n        dfr[\"speedup_%s\" % engine] = dfr[\"time_skl\"] / dfr[\"time_%s\" % engine]\n    print(dfr.tail().T)\n\n    ncols = 2\n    fig, axs = plt.subplots(len(engines), ncols, figsize=(\n        14, 4 * len(engines)), sharey=True)\n\n    row = 0\n    for row, engine in enumerate(engines):\n        pos = 0\n        name = \"LinearRegression - %s\" % engine\n        for nf in sorted(set(dfr.nfeat)):\n            for n_jobs in sorted(set(dfr.n_jobs)):\n                sub = dfr[(dfr.nfeat == nf) & (dfr.n_jobs == n_jobs)]\n                ax = axs[row, pos]\n                labels = sub.n_obs\n                means = sub[\"speedup_%s\" % engine]\n\n                x = numpy.arange(len(labels))\n                width = 0.90\n\n                rects1 = ax.bar(x, means, width, label='Speedup')\n                if pos == 0:\n                    # ax.set_yscale('log')\n                    ax.set_ylim([0.1, max(dfr[\"speedup_%s\" % engine])])\n\n                if pos == 0:\n                    ax.set_ylabel('Speedup')\n                ax.set_title('%s\\n%d features\\n%d jobs' % (name, nf, n_jobs))\n                if row == len(engines) - 1:\n                    ax.set_xlabel('batch size')\n                ax.set_xticks(x)\n                ax.set_xticklabels(labels)\n                autolabel(ax, rects1)\n                for tick in ax.xaxis.get_major_ticks():\n                    tick.label.set_fontsize(8)\n                for tick in ax.yaxis.get_major_ticks():\n                    tick.label.set_fontsize(8)\n                pos += 1\n\n    fig.tight_layout()\n    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run benchs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ignore_warnings(category=FutureWarning)\ndef run_bench(repeat=200, verbose=False):\n    n_obs = [1, 10, 100, 1000, 10000]\n    methods = ['predict']\n    n_features = [10, 50]\n    n_jobss = [cpu_count()]\n\n    start = time()\n    results = bench(n_obs, n_features, n_jobss,\n                    methods, repeat=repeat, verbose=verbose)\n    end = time()\n\n    results_df = pandas.DataFrame(results)\n    print(\"Total time = %0.3f sec cpu=%d\\n\" % (end - start, cpu_count()))\n\n    # plot the results\n    return results_df\n\n\nname = \"plot_linear_regression\"\ndf = run_bench(verbose=True)\n# df.to_csv(\"%s.csv\" % name, index=False)\n# df.to_excel(\"%s.xlsx\" % name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plot_rf_models(df)\nfig.savefig(\"%s.png\" % name)\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}