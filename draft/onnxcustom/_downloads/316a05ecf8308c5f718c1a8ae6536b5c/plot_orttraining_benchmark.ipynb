{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark, comparison scikit-learn - onnxruntime-training\n\nThe benchmark compares the processing time between :epkg:`scikit-learn`\nand :epkg:`onnxruntime-training` on a linear regression and a neural network.\nIt uses the model trained in `l-orttraining-nn-gpu`.\n\n## First comparison: neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nfrom pprint import pprint\nimport time\nimport numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom onnxruntime import get_device\nfrom pyquickhelper.pycode.profiling import profile, profile2graph\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom mlprodict.onnx_conv import to_onnx\nfrom onnxcustom.utils.orttraining_helper import (\n    add_loss_output, get_train_initializer)\nfrom onnxcustom.training.optimizers import OrtGradientOptimizer\n\n\nX, y = make_regression(1000, n_features=100, bias=2)\nX = X.astype(numpy.float32)\ny = y.astype(numpy.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark(skl_model, train_session, name, verbose=True):\n    \"\"\"\n    :param skl_model: model from scikit-learn\n    :param train_session: instance of OrtGradientOptimizer\n    :param name: experiment name\n    :param verbose: to debug\n    \"\"\"\n    print(\"[benchmark] %s\" % name)\n    begin = time.perf_counter()\n    skl_model.fit(X, y)\n    duration_skl = time.perf_counter() - begin\n    length_skl = len(skl_model.loss_curve_)\n    print(\"[benchmark] skl=%r iterations - %r seconds\" % (\n        length_skl, duration_skl))\n\n    begin = time.perf_counter()\n    train_session.fit(X, y)\n    duration_ort = time.perf_counter() - begin\n    length_ort = len(train_session.train_losses_)\n    print(\"[benchmark] ort=%r iterations - %r seconds\" % (\n        length_ort, duration_ort))\n\n    return dict(skl=duration_skl, ort=duration_ort, name=name,\n                iter_skl=length_skl, iter_ort=length_ort,\n                losses_skl=skl_model.loss_curve_,\n                losses_ort=train_session.train_losses_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Common parameters and model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 15\nmax_iter = 100\n\nnn = MLPRegressor(hidden_layer_sizes=(50, 10), max_iter=max_iter,\n                  solver='sgd', learning_rate_init=5e-4, alpha=0,\n                  n_iter_no_change=max_iter * 3, batch_size=batch_size,\n                  nesterovs_momentum=False, momentum=0,\n                  learning_rate='invscaling')\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversion to ONNX and trainer initialization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "onx = to_onnx(nn, X_train[:1].astype(numpy.float32), target_opset=15)\nonx_train = add_loss_output(onx)\n\nweights = get_train_initializer(onx)\npprint(list((k, v[0].shape) for k, v in weights.items()))\n\ntrain_session = OrtGradientOptimizer(\n    onx_train, list(weights), device='cpu', learning_rate=1e-5,\n    warm_start=False, max_iter=max_iter, batch_size=batch_size)\n\n\nbenches = [benchmark(nn, train_session, name='NN-CPU')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiling\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def clean_name(text):\n    pos = text.find('onnxruntime')\n    if pos >= 0:\n        return text[pos:]\n    pos = text.find('onnxcustom')\n    if pos >= 0:\n        return text[pos:]\n    pos = text.find('site-packages')\n    if pos >= 0:\n        return text[pos:]\n    return text\n\n\nps = profile(lambda: benchmark(nn, train_session, name='NN-CPU'))[0]\nroot, nodes = profile2graph(ps, clean_text=clean_name)\ntext = root.to_text()\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## if GPU is available\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if get_device().upper() == 'GPU':\n\n    train_session = OrtGradientOptimizer(\n        onx_train, list(weights), device='cuda', learning_rate=5e-4,\n        warm_start=False, max_iter=200, batch_size=batch_size)\n\n    benches.append(benchmark(nn, train_session, name='NN-GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = MLPRegressor(hidden_layer_sizes=tuple(), max_iter=max_iter,\n                  solver='sgd', learning_rate_init=5e-2, alpha=0,\n                  n_iter_no_change=max_iter * 3, batch_size=batch_size,\n                  nesterovs_momentum=False, momentum=0,\n                  learning_rate='invscaling')\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    lr.fit(X, y)\n\n\nonx = to_onnx(nn, X_train[:1].astype(numpy.float32), target_opset=15)\nonx_train = add_loss_output(onx)\n\ninits = get_train_initializer(onx)\nweights = {k: v for k, v in inits.items() if k != \"shape_tensor\"}\npprint(list((k, v[0].shape) for k, v in weights.items()))\n\ntrain_session = OrtGradientOptimizer(\n    onx_train, list(weights), device='cpu', learning_rate=1e-4,\n    warm_start=False, max_iter=max_iter, batch_size=batch_size)\n\nbenches.append(benchmark(lr, train_session, name='LR-CPU'))\n\nif get_device().upper() == 'GPU':\n\n    train_session = OrtGradientOptimizer(\n        onx_train, list(weights), device='cuda', learning_rate=1e-4,\n        warm_start=False, max_iter=200, batch_size=batch_size)\n\n    benches.append(benchmark(nn, train_session, name='LR-GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU profiling\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if get_device().upper() == 'GPU':\n    ps = profile(lambda: benchmark(nn, train_session, name='LR-GPU'))[0]\n    root, nodes = profile2graph(ps, clean_text=clean_name)\n    text = root.to_text()\n    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n\nDataframe first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(benches).set_index('name')\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "text output\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\ndf[['skl', 'ort']].plot.bar(title=\"Processing time\", ax=ax[0])\nax[0].tick_params(axis='x', rotation=30)\nfor bench in benches:\n    ax[1].plot(bench['losses_skl'][1:], label='skl-' + bench['name'])\n    ax[1].plot(bench['losses_ort'][1:], label='ort-' + bench['name'])\nax[1].set_title(\"Losses\")\nax[1].set_yscale('log')\nax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient update are not exactly the same.\nIt should be improved for a fair comprison.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}