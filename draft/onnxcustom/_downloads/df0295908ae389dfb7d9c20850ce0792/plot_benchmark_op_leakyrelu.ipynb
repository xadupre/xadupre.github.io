{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark operator LeakyRelu\n\nThe operator `LeakyRelu` is equivalent to the function:\n$LeayRelu(x) = \\begin{array}{l} x \\text{ if } x > 0  \\\\\n\\alpha x \\text{otherwise} \\end{array}$. But it could be rewritten into\nthe following decomposition\n$LeayRelu(x) = x (\\indicatrice{x} + \\alpha (1 - \\indicatrice{x})) =\nx ((1 - \\alpha) \\indicatrice{x} + \\alpha)$. Let's compare the\ntwo implementation with onnx runtimes.\n\n## The ONNX graphs for both implementations of LeakyRely\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nfrom numpy.testing import assert_almost_equal\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom onnx import TensorProto\nfrom onnxruntime import InferenceSession, get_device\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom skl2onnx.algebra.onnx_ops import (\n    OnnxLeakyRelu, OnnxSign, OnnxMul, OnnxAdd, OnnxDiv,\n    OnnxGreater, OnnxCast)\nfrom cpyquickhelper.numbers.speed_measure import measure_time\nfrom mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nfrom mlprodict.plotting.plotting import onnx_simple_text_plot\nfrom onnxcustom.plotting.plotting_onnx import plot_onnxs\nfrom tqdm import tqdm\n\n\nprint([code_optimisation(), get_device()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First implementation: the operator LeayRelu.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_leaky_relu(alpha=0.5, target_opset=15):\n    x = OnnxLeakyRelu('X', alpha=alpha, op_version=target_opset,\n                      output_names=['Y'])\n    return x.to_onnx({'X': FloatTensorType()},\n                     outputs={'Y': FloatTensorType()},\n                     target_opset=target_opset)\n\n\nonx_leaky = build_leaky_relu()\nprint(onnx_simple_text_plot(onx_leaky))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second option, the formula introduced above must adapted as\nONNX operator Sign returns -1 if *x* is negative and not 0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_leaky_relu_decomposed(alpha=0.5, target_opset=15):\n    signo = OnnxSign('X', op_version=target_opset)\n    sign = OnnxDiv(\n        OnnxAdd(signo, numpy.array([1], dtype=numpy.float32),\n                op_version=target_opset),\n        numpy.array([2], dtype=numpy.float32), op_version=target_opset)\n    fact = OnnxAdd(\n        OnnxMul(sign, numpy.array([1 - alpha], dtype=numpy.float32),\n                op_version=target_opset),\n        numpy.array([alpha], dtype=numpy.float32),\n        op_version=target_opset)\n    x = OnnxMul('X', fact, op_version=target_opset,\n                output_names=['Y'])\n    return x.to_onnx({'X': FloatTensorType()},\n                     outputs={'Y': FloatTensorType()},\n                     target_opset=target_opset)\n\n\nonx_leaky_dec = build_leaky_relu_decomposed()\nprint(onnx_simple_text_plot(onx_leaky_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Third option, use of operater Greater\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_leaky_relu_decomposed_greater(alpha=0.5, target_opset=15):\n    signo = OnnxGreater('X', numpy.array([0], dtype=numpy.float32),\n                        op_version=target_opset)\n    sign = OnnxCast(signo, to=TensorProto.FLOAT,\n                    op_version=target_opset)\n    fact = OnnxAdd(\n        OnnxMul(sign, numpy.array([1 - alpha], dtype=numpy.float32),\n                op_version=target_opset),\n        numpy.array([alpha], dtype=numpy.float32),\n        op_version=target_opset)\n    x = OnnxMul('X', fact, op_version=target_opset,\n                output_names=['Y'])\n    return x.to_onnx({'X': FloatTensorType()},\n                     outputs={'Y': FloatTensorType()},\n                     target_opset=target_opset)\n\n\nonx_leaky_dec_greater = build_leaky_relu_decomposed_greater()\nprint(onnx_simple_text_plot(onx_leaky_dec_greater))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visually\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_onnxs(onx_leaky, onx_leaky_dec, onx_leaky_dec_greater,\n           title=[\"One operator\", \"Decomposed\\nLeakyRelu\",\n                  \"Decomposed\\nLeakyRelu Greater\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check that both graph returns are equivalent\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess1 = InferenceSession(onx_leaky.SerializeToString(),\n                         providers=['CPUExecutionProvider'])\nsess_dec = InferenceSession(onx_leaky_dec.SerializeToString(),\n                            providers=['CPUExecutionProvider'])\nsess_dec_greater = InferenceSession(onx_leaky_dec_greater.SerializeToString(),\n                                    providers=['CPUExecutionProvider'])\n\nfor shape in [(1, ), (10, ), (5, 5), (7, 2, 4)]:\n    rnd = numpy.random.randn(*shape).astype(numpy.float32)\n    res1 = sess1.run(None, {'X': rnd})[0]\n    res_dec = sess_dec.run(None, {'X': rnd})[0]\n    res_dec_greater = sess_dec_greater.run(None, {'X': rnd})[0]\n    assert_almost_equal(res1, res_dec)\n    assert_almost_equal(res1, res_dec_greater)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fcts = [('leakyrelu', sess1), ('dec', sess_dec),\n        ('dec_greater', sess_dec_greater)]\n\nN = 100\ndata = []\nfor dim in tqdm([10, 128, 256, 512, 1000, 2000]):\n    for shape in [(N, dim), (dim, N)]:\n        rnd = numpy.random.randn(*shape).astype(numpy.float32)\n        for name, sess in fcts:\n            repeat = int(4001 / dim)\n            obs = measure_time(\n                lambda: sess.run(None, {'X': rnd}),\n                context=dict(rnd=rnd, sess=sess),\n                div_by_number=True, repeat=repeat, number=200)\n            obs['name'] = name\n            obs['N'] = N\n            obs['dim'] = dim\n            obs['orient'] = shape[0] == N\n            obs['shape'] = \"%dx%d\" % shape\n            data.append(obs)\n\ndf = DataFrame(data)\ndf[['name', 'N', 'dim', 'average', 'deviation']]\n\nprint(df[['name', 'N', 'dim', 'average']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other to way to look at it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def speedup(piv):\n    for c in piv.columns:\n        if c == 'leakyrelu':\n            continue\n        piv[c] = piv['leakyrelu'] / piv[c]\n    piv['leakyrelu'] = 1\n    return piv\n\n\npiv = speedup(df.pivot('shape', 'name', 'average'))\npiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\nspeedup(df[df.orient].pivot('dim', 'name', 'average')).plot(ax=ax[0])\nax[0].set_title(\"LeakyRelu speedup, shape=(%d,dim)\"\n                \"\\nThe higher the better\" % N)\nspeedup(df[~df.orient].pivot('dim', 'name', 'average')).plot(ax=ax[1])\nax[1].set_title(\"LeakyRelu speedup, shape=(dim,%d)\"\n                \"\\nThe higher the better\" % N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This kind of benchmark helps finding better implementation\nof operator runtime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}