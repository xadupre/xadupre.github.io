{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark onnxruntime optimization\n\n:epkg:`onnxruntime` does optimize the ONNX graph before\nrunning the inference. It tries for example to fuse a matrix multiplication\nfollowing or followed by a transpose, choosing the most efficient path.\n\n## One ONNX file\n\nThis section creates an ONNX graph if there is not one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom collections import OrderedDict, Counter\nimport numpy\nimport onnx\nfrom cpyquickhelper.numbers.speed_measure import measure_time\nimport pandas\nfrom onnxruntime import InferenceSession, SessionOptions, get_device\nfrom onnxruntime.capi._pybind_state import (  # pylint: disable=E0611\n    SessionIOBinding, OrtDevice as C_OrtDevice, OrtValue as C_OrtValue,\n    GraphOptimizationLevel)\nfrom sklearn.neighbors import RadiusNeighborsRegressor\nfrom skl2onnx import to_onnx\nfrom tqdm import tqdm\nfrom mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filename = \"onnx_to_profile.onnx\"\n\nif not os.path.exists(filename):\n    print(\"Generate a graph for %r.\" % filename)\n    X = numpy.random.randn(1000, 10).astype(numpy.float64)\n    y = X.sum(axis=1).reshape((-1, 1))\n\n    model = RadiusNeighborsRegressor()\n    model.fit(X, y)\n    onx = to_onnx(model, X, options={'optim': 'cdist'})\n\n    with open(filename, \"wb\") as f:\n        f.write(onx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions\n\nWe need to generate random inputs to test the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def random_input(typ, shape, batch):\n    if typ == 'tensor(double)':\n        dtype = numpy.float64\n    elif typ == 'tensor(float)':\n        dtype = numpy.float32\n    else:\n        raise NotImplementedError(\n            \"Unable to guess dtype from %r.\" % typ)\n\n    if len(shape) <= 1:\n        new_shape = shape\n    elif shape[0] is None:\n        new_shape = tuple([batch] + list(shape[1:]))\n    else:\n        new_shape = shape\n    return numpy.random.randn(*new_shape).astype(dtype)\n\n\ndef random_feed(sess, batch=10):\n    \"\"\"\n    Creates a dictionary of random inputs.\n\n    :param batch: dimension to use as batch dimension if unknown\n    :return: dictionary\n    \"\"\"\n    inputs = sess.get_inputs()\n    res = OrderedDict()\n    for inp in inputs:\n        name = inp.name\n        typ = inp.type\n        shape = inp.shape\n        res[name] = random_input(typ, shape, batch)\n    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A function which calls the API for any device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_with_iobinding(sess, bind, ort_device, feed_ort_value, outputs):\n    for name, (value, dtype) in feed_ort_value.items():\n        bind.bind_input(name, ort_device, dtype, value.shape(),\n                        value.data_ptr())\n    for out in outputs:\n        bind.bind_output(out, ort_device)\n    sess._sess.run_with_iobinding(bind, None)\n    ortvalues = bind.get_outputs()\n    return [o.numpy() for o in ortvalues]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark\n\nLet's choose the device available on this machine.\nbatch dimension is set to 10.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = 200\n\nif get_device().upper() == 'GPU':\n    ort_device = C_OrtDevice(\n        C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0)\n    provider = 'CUDAExecutionProvider'\nelse:\n    ort_device = C_OrtDevice(\n        C_OrtDevice.cpu(), C_OrtDevice.default_memory(), 0)\n    provider = 'CPUExecutionProvider'\nprint(\"provider = %r\" % provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(filename, 'rb') as f:\n    onx = onnx.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create of the session.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\nfiles = []\nlegend = []\n\nfor graph_opt, name_opt in tqdm([\n        (GraphOptimizationLevel.ORT_DISABLE_ALL, \"ORT_DISABLE_ALL\"),\n        (GraphOptimizationLevel.ORT_ENABLE_BASIC, \"ORT_ENABLE_BASIC\"),\n        (GraphOptimizationLevel.ORT_ENABLE_EXTENDED, \"ORT_ENABLE_EXTENDED\"),\n        (GraphOptimizationLevel.ORT_ENABLE_ALL, \"ORT_ENABLE_ALL\")]):\n\n    so = SessionOptions()\n    so.graph_optimization_level = graph_opt\n    so.optimized_model_filepath = (\n        os.path.split(filename)[-1] + \".optimized.%s.onnx\" % name_opt)\n    files.append(so.optimized_model_filepath)\n    legend.append(name_opt)\n    sess = InferenceSession(onx.SerializeToString(), so,\n                            providers=[provider])\n    bind = SessionIOBinding(sess._sess)\n\n    #####################################\n    # Creates random data\n    feed = random_feed(sess, batch)\n\n    #####################################\n    # moving the data on CPU or GPU\n    feed_ort_value = OrderedDict(\n        (name, (C_OrtValue.ortvalue_from_numpy(v, ort_device), v.dtype))\n        for name, v in feed.items())\n    outputs = [o.name for o in sess.get_outputs()]\n\n    #######################################\n    # The profiling.\n\n    obs = measure_time(\n        lambda: run_with_iobinding(\n            sess, bind, ort_device, feed_ort_value, outputs),\n        context=dict(run_with_iobinding=run_with_iobinding,\n                     feed_ort_value=feed_ort_value, outputs=outputs,\n                     sess=sess, bind=bind, ort_device=ort_device),\n        repeat=10, number=10, div_by_number=True)\n    obs['name'] = name_opt\n    data.append(obs)\n\n\ndf = pandas.DataFrame(data)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = df.set_index('name')\ndev = df[['deviation']].copy()\ndev.columns = ['average']\nax = df[['average']].plot.bar(yerr=dev)\nax.set_title(os.path.split(filename)[-1])\nax.tick_params(axis='x', labelrotation=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result are similar because the optimized model was very similar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\nfor name in files:\n    with open(name, \"rb\") as f:\n        onx = onnx.load(f)\n    op_names = [op.op_type for op in onx.graph.node]\n    data.append(Counter(op_names))\n\ndf = pandas.DataFrame(data).T\ndf.columns = legend\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = df.plot.barh(yerr=dev)\nax.set_title(os.path.split(filename)[-1])\n\n# import matplotlib.pyplot as plt\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}