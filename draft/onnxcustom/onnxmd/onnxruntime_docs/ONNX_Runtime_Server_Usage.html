
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>How to Use build ONNX Runtime Server for Prediction &#8212; onnxcustom</title>
    
    <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/thebelab-helper.js"></script>
    <script src="../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Roadmap.html">
   ONNX Runtime Roadmap
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Privacy.html">
   Privacy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Server.html">
   Build ONNX Runtime Server on Linux
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   How to Use build ONNX Runtime Server for Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="FAQ.html">
   FAQ
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OperatorKernels.html">
   Supported Operators and Data Types
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Versioning.html">
   Versioning
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Coding_Conventions_and_Standards.html">
   ONNX Runtime coding conventions and standards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ABI_Dev_Notes.html">
   Global Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PR_Guidelines.html">
   Guidelines for creating a good pull request
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Model_Test.html">
   Get the test data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NotesOnThreading.html">
   Notes on Threading in ORT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Python_Dev_Notes.html">
   Python Dev Notes
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C_API_Guidelines.html">
   ORT API Guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cmake_guideline.html">
   Scope the impact to minimal
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="onnxruntime_extensions.html">
   ONNXRuntime Extensions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ContribOperators.html">
   Contrib Operator Schemas
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Android_testing.html">
   Testing Android Changes using the Emulator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ORTMobilePackageOperatorTypeSupport.html">
   ONNX Runtime Mobile Pre-Built Package Operator and Type Support
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="WinML_principles.html">
   Contributing to Windows ML
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reduced_Operator_Kernel_build.html">
   ONNX Runtime Reduced Operator Kernel build
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ReleaseManagement.html">
   Release Management
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   How to Use build ONNX Runtime Server for Prediction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-use-onnx-runtime-server-for-prediction">
   How to Use ONNX Runtime Server for Prediction
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#start-the-server">
     Start the Server
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#http-endpoint">
     HTTP Endpoint
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#request-and-response-payload">
       Request and Response Payload
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inferencing">
       Inferencing
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interactive-tutorial-notebook">
       Interactive tutorial notebook
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grpc-endpoint">
     GRPC Endpoint
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advanced-topics">
     Advanced Topics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#number-of-worker-threads">
       Number of Worker Threads
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#request-id-and-client-request-id">
       Request ID and Client Request ID
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#rsyslog-support">
       rsyslog Support
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#report-issues">
     Report Issues
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1><span style="color:red">Note: ONNX Runtime Server has been deprecated.</span></h1>
<section id="how-to-use-build-onnx-runtime-server-for-prediction">
<h1>How to Use build ONNX Runtime Server for Prediction<a class="headerlink" href="#how-to-use-build-onnx-runtime-server-for-prediction" title="Permalink to this headline">¶</a></h1>
<p>ONNX Runtime Server provides an easy way to start an inferencing server for prediction with both HTTP and GRPC endpoints.</p>
<p>The CLI command to build the server is</p>
<p>Default CPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>python3 /onnxruntime/tools/ci_build/build.py --build_dir /onnxruntime/build --config Release --build_server --parallel --cmake_extra_defines ONNXRUNTIME_VERSION=$(cat ./VERSION_NUMBER
</pre></div>
</div>
</section>
<section id="how-to-use-onnx-runtime-server-for-prediction">
<h1>How to Use ONNX Runtime Server for Prediction<a class="headerlink" href="#how-to-use-onnx-runtime-server-for-prediction" title="Permalink to this headline">¶</a></h1>
<p>The CLI command to start the server is shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ./onnxruntime_server
Version: &lt;Build number&gt;
Commit ID: &lt;The latest commit ID&gt;

the option &#39;--model_path&#39; is required but missing
Allowed options:
  -h [ --help ]                Shows a help message and exits
  --log_level arg (=info)      Logging level. Allowed options (case sensitive):
                               verbose, info, warning, error, fatal
  --model_path arg             Path to ONNX model
  --address arg (=0.0.0.0)     The base HTTP address
  --http_port arg (=8001)      HTTP port to listen to requests
  --num_http_threads arg (=&lt;# of your cpu cores&gt;) Number of http threads
  --grpc_port arg (=50051)     GRPC port to listen to requests
</pre></div>
</div>
<p><strong>Note</strong>: The only mandatory argument for the program here is <code class="docutils literal notranslate"><span class="pre">model_path</span></code></p>
<section id="start-the-server">
<h2>Start the Server<a class="headerlink" href="#start-the-server" title="Permalink to this headline">¶</a></h2>
<p>To host an ONNX model as an inferencing server, simply run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">onnxruntime_server</span> <span class="o">--</span><span class="n">model_path</span> <span class="o">/&lt;</span><span class="n">your</span><span class="o">&gt;/&lt;</span><span class="n">model</span><span class="o">&gt;/&lt;</span><span class="n">path</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="http-endpoint">
<h2>HTTP Endpoint<a class="headerlink" href="#http-endpoint" title="Permalink to this headline">¶</a></h2>
<p>The prediction URL for HTTP endpoint is in this format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">http</span><span class="p">:</span><span class="o">//&lt;</span><span class="n">your_ip_address</span><span class="o">&gt;</span><span class="p">:</span><span class="o">&lt;</span><span class="n">port</span><span class="o">&gt;/</span><span class="n">v1</span><span class="o">/</span><span class="n">models</span><span class="o">/&lt;</span><span class="n">your</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;/</span><span class="n">versions</span><span class="o">/&lt;</span><span class="n">your</span><span class="o">-</span><span class="n">version</span><span class="o">&gt;</span><span class="p">:</span><span class="n">predict</span>
</pre></div>
</div>
<p><strong>Note</strong>: Since we currently only support one model, the model name and version can be any string length &gt; 0. In the future, model_names and versions will be verified.</p>
<section id="request-and-response-payload">
<h3>Request and Response Payload<a class="headerlink" href="#request-and-response-payload" title="Permalink to this headline">¶</a></h3>
<p>The request and response need to be a protobuf message. The Protobuf definition can be found <span class="xref myst">here</span>.</p>
<p>A protobuf message could have two formats: binary and JSON. Usually the binary payload has better latency, in the meanwhile the JSON format is easy for human readability.</p>
<p>The HTTP request header field <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> tells the server how to handle the request and thus it is mandatory for all requests. Requests missing <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> will be rejected as <code class="docutils literal notranslate"><span class="pre">400</span> <span class="pre">Bad</span> <span class="pre">Request</span></code>.</p>
<ul class="simple">
<li><p>For <code class="docutils literal notranslate"><span class="pre">&quot;Content-Type:</span> <span class="pre">application/json&quot;</span></code>, the payload will be deserialized as JSON string in UTF-8 format</p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">&quot;Content-Type:</span> <span class="pre">application/vnd.google.protobuf&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Content-Type:</span> <span class="pre">application/x-protobuf&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;Content-Type:</span> <span class="pre">application/octet-stream&quot;</span></code>, the payload will be consumed as protobuf message directly.</p></li>
</ul>
<p>Clients can control the response type by setting the request with an <code class="docutils literal notranslate"><span class="pre">Accept</span></code> header field and the server will serialize in your desired format. The choices currently available are the same as the <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> header field. If this field is not set in the request, the server will use the same type as your request.</p>
</section>
<section id="inferencing">
<h3>Inferencing<a class="headerlink" href="#inferencing" title="Permalink to this headline">¶</a></h3>
<p>To send a request to the server, you can use any tool which supports making HTTP requests. Here is an example using <code class="docutils literal notranslate"><span class="pre">curl</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span>  <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="o">-</span><span class="n">d</span> <span class="s2">&quot;@predict_request_0.json&quot;</span> <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">8001</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">mymodel</span><span class="o">/</span><span class="n">versions</span><span class="o">/</span><span class="mi">3</span><span class="p">:</span><span class="n">predict</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="o">--</span><span class="n">data</span><span class="o">-</span><span class="n">binary</span> <span class="s2">&quot;@predict_request_0.pb&quot;</span> <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/octet-stream&quot;</span> <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Foo: 1234&quot;</span>  <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">8001</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">mymodel</span><span class="o">/</span><span class="n">versions</span><span class="o">/</span><span class="mi">3</span><span class="p">:</span><span class="n">predict</span>
</pre></div>
</div>
</section>
<section id="interactive-tutorial-notebook">
<h3>Interactive tutorial notebook<a class="headerlink" href="#interactive-tutorial-notebook" title="Permalink to this headline">¶</a></h3>
<p>A simple Jupyter notebook demonstrating the usage of ONNX Runtime server to host an ONNX model and perform inferencing can be found <a class="reference external" href="https://github.com/onnx/tutorials/blob/master/tutorials/OnnxRuntimeServerSSDModel.ipynb">here</a>.</p>
</section>
</section>
<section id="grpc-endpoint">
<h2>GRPC Endpoint<a class="headerlink" href="#grpc-endpoint" title="Permalink to this headline">¶</a></h2>
<p>If you prefer using the GRPC endpoint, the protobuf could be found <span class="xref myst">here</span>. You could generate your client and make a GRPC call to it. To learn more about how to generate the client code and call to the server, please refer to <a class="reference external" href="https://grpc.io/docs/tutorials/">the tutorials of GRPC</a>.</p>
</section>
<section id="advanced-topics">
<h2>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h2>
<section id="number-of-worker-threads">
<h3>Number of Worker Threads<a class="headerlink" href="#number-of-worker-threads" title="Permalink to this headline">¶</a></h3>
<p>You can change this to optimize server utilization. The default is the number of CPU cores on the host machine.</p>
</section>
<section id="request-id-and-client-request-id">
<h3>Request ID and Client Request ID<a class="headerlink" href="#request-id-and-client-request-id" title="Permalink to this headline">¶</a></h3>
<p>For easy tracking of requests, we provide the following header fields:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x-ms-request-id</span></code>: will be in the response header, no matter the request result. It will be a GUID/uuid with dash, e.g. <code class="docutils literal notranslate"><span class="pre">72b68108-18a4-493c-ac75-d0abd82f0a11</span></code>. If the request headers contain this field, the value will be ignored.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x-ms-client-request-id</span></code>: a field for clients to tracking their requests. The content will persist in the response headers.</p></li>
</ul>
</section>
<section id="rsyslog-support">
<h3>rsyslog Support<a class="headerlink" href="#rsyslog-support" title="Permalink to this headline">¶</a></h3>
<p>If you prefer using an ONNX Runtime Server with <a class="reference external" href="https://www.rsyslog.com/">rsyslog</a> support(<a class="reference external" href="https://www.onnxruntime.ai/docs/how-to/build.html#build-onnx-runtime-server-on-linux">build instruction</a>), you should be able to see the log in <code class="docutils literal notranslate"><span class="pre">/var/log/syslog</span></code> after the ONNX Runtime Server runs. For detail about how to use rsyslog, please reference <a class="reference external" href="https://www.rsyslog.com/category/guides-for-rsyslog/">here</a>.</p>
</section>
</section>
<section id="report-issues">
<h2>Report Issues<a class="headerlink" href="#report-issues" title="Permalink to this headline">¶</a></h2>
<p>If you see any issues or want to ask questions about the server, please feel free to do so in this repo with the version and commit id from the command line.</p>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>