
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>ONNX Interface for Framework Integration: API Proposal &#8212; onnxcustom</title>
    
    <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/thebelab-helper.js"></script>
    <script src="../../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ultimate-goal">
   Ultimate Goal
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#design-choices">
   Design Choices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proposed-interface">
   Proposed Interface
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-use-pattern-for-deep-learning-frameworks">
   General Use Pattern for Deep Learning Frameworks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-notes">
   Implementation Notes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backend-object">
     Backend object
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-object">
     Graph object
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#library-initialization">
     Library initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnxgetnumbackends">
     onnxGetNumBackends
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnxgetbackendinfo">
     onnxGetBackendInfo
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <!--- SPDX-License-Identifier: Apache-2.0 -->
<section id="onnx-interface-for-framework-integration-api-proposal">
<h1>ONNX Interface for Framework Integration: API Proposal<a class="headerlink" href="#onnx-interface-for-framework-integration-api-proposal" title="Permalink to this headline">¶</a></h1>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>Leading hardware and systems vendors offer highly optimized software to run neural network graphs. These software can deliver order-of-magnitude speedups compared to generic implementations, but their integration with deep learning frameworks and applications is complicated by large variety in vendor-specific interfaces, and subtle incompatibilities with the software stack of high-level applications.</p>
<p>So far, ONNX format targets the problem of offline conversion of neural network models between different high-level frameworks and vendor-specific libraries through offline translation. In this proposal, we suggest that ONNX ecosystem could be enriched to enable runtime discovery and selection of high-performance graph execution backends, and online (in runtime) conversion of ONNX graph to internal representations of these implementations.</p>
</section>
<section id="ultimate-goal">
<h2>Ultimate Goal<a class="headerlink" href="#ultimate-goal" title="Permalink to this headline">¶</a></h2>
<p>We should strive for consensus on a library API to interface with optimized backends and offload parts of ONNX graphs to these high-performance hardware and software implementation. The API should enable wide interoperability between high-level deep learning frameworks, software implementations of optimized graph runtimes, and existing and upcoming neural network acceleration hardware.</p>
<p>The standardized API should reduce friction in deploying neural network models for all involved parties:</p>
<ul class="simple">
<li><p>Applications would be able to ship only one version of a neural network model (either in ONNX format, or in the format of their deep learning framework, and convert it on the fly to ONNX).</p></li>
<li><p>Deep learning frameworks would be able to integrate with many hardware vendors by using only a single interface.</p></li>
<li><p>Hardware vendors would be able to implement only one interface and get integration with many deep learning frameworks.</p></li>
</ul>
</section>
<section id="design-choices">
<h2>Design Choices<a class="headerlink" href="#design-choices" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Interface must use only highly portable aspects of C ABI.</p></li>
<li><p>Neural network graphs are passed as serialized ONNX ModelProto messages. To avoid serialization overhead, weights can be passed as raw memory blobs.</p></li>
<li><p>Input and output tensors are allocated by the caller and use NCHW layout.</p></li>
<li><p>Intermediate tensors are allocated by the vendor implementation, and can use any layout.</p></li>
<li><p>Backends (software implementations and hardware accelerators) are discovered, selected, and initialized on-demand in run-time. Multiple backends can be used in the same application simultaneously.</p></li>
<li><p>There is no minimal set of ONNX operators to implement. The implementer and the user (a deep learning framework) of the API decide which operators can and will be offloaded in runtime.</p></li>
<li><p>The proposal includes the minimal functionality to let deep learning frameworks and vendor libraries work together. Several extension mechanisms can be used for more efficient vendor- or platform-specific functionality.</p></li>
</ul>
</section>
<section id="proposed-interface">
<h2>Proposed Interface<a class="headerlink" href="#proposed-interface" title="Permalink to this headline">¶</a></h2>
<p>We propose a small C-based API, which includes the following functionality:</p>
<ul class="simple">
<li><p>Discover (<code class="docutils literal notranslate"><span class="pre">onnxGetNumBackends</span></code>) and query information (<code class="docutils literal notranslate"><span class="pre">onnxGetBackendInfo</span></code>) about high-performance backends</p></li>
<li><p>Initialize (<code class="docutils literal notranslate"><span class="pre">onnxInitBackend</span></code>) and deinitialize (<code class="docutils literal notranslate"><span class="pre">onnxReleaseBackend</span></code>) high-performance backends</p></li>
<li><p>Query if a backend supports an ONNX operator with particular parameters and input shapes (<code class="docutils literal notranslate"><span class="pre">onnxGetBackendCompatibility</span></code>)</p></li>
<li><p>Convert an ONNX graph to opaque vendor-specific representation of a backend (<code class="docutils literal notranslate"><span class="pre">onnxInitGraph</span></code>)</p></li>
<li><p>Specify memory locations and metadata about graph inputs and outputs (<code class="docutils literal notranslate"><span class="pre">onnxSetGraphIO</span></code>)</p></li>
<li><p>Run an ONNX graph, converted to vendor-specific representation (<code class="docutils literal notranslate"><span class="pre">onnxRunGraph</span></code>)</p></li>
<li><p>Release the vendor-specific representation of a graph and associated resources (<code class="docutils literal notranslate"><span class="pre">onnxReleaseGraph</span></code>)</p></li>
</ul>
</section>
<section id="general-use-pattern-for-deep-learning-frameworks">
<h2>General Use Pattern for Deep Learning Frameworks<a class="headerlink" href="#general-use-pattern-for-deep-learning-frameworks" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>The user (deep learning framework) iterates operators in a model graph one-by-one, convert them to ONNX, and calls <code class="docutils literal notranslate"><span class="pre">onnxGetBackendCompatibility</span></code> to check which of the operators can be offloaded to the backend.</p></li>
<li><p>The user constructs connected subgraphs of operators that can be offloaded to the backend.</p></li>
<li><p>(Optional) For each subgraph, the user estimates if it is beneficial to offload it to the optimized backend:</p>
<p>a. The user queries the backend about it high-level performance characteristics using <code class="docutils literal notranslate"><span class="pre">ONNX_BACKEND_MACS_*</span></code> and <code class="docutils literal notranslate"><span class="pre">ONNX_BACKEND_MEMORY_BANDWIDTH</span></code> information queries. These data let the user build a simple roofline model of backend performance.</p>
<p>b. For every subgraph the user estimates time to do inference using the roofline model.</p>
<p>c. The user additionally estimates time to transfer subgraph inputs to the backend using <code class="docutils literal notranslate"><span class="pre">ONNX_BACKEND_CPU_MEMORY_READ_BANDWIDTH</span></code> information query and to transfer subgraph outputs from the backend using <code class="docutils literal notranslate"><span class="pre">ONNX_BACKEND_CPU_MEMORY_WRITE_BANDWIDTH</span></code>.</p>
<p>d. If predicted time to transfer inputs to the backend, do inference, and transfer outputs from the backend exceeds predicted time to do the inference on default engine (e.g. CPU), the user falls back to a different ONNX backend, or to the default engine.</p>
</li>
<li><p>The user initialized the backend, and offloads the subgraph execution to the ONNX backend by calling <code class="docutils literal notranslate"><span class="pre">onnxInitGraph</span></code>, <code class="docutils literal notranslate"><span class="pre">onnxSetGraphIO</span></code> and <code class="docutils literal notranslate"><span class="pre">onnxRunGraph</span></code></p></li>
</ol>
</section>
<section id="implementation-notes">
<h2>Implementation Notes<a class="headerlink" href="#implementation-notes" title="Permalink to this headline">¶</a></h2>
<section id="backend-object">
<h3>Backend object<a class="headerlink" href="#backend-object" title="Permalink to this headline">¶</a></h3>
<p>Backend is a combination of software library and hardware device. The same device (e.g. “NVIDIA Tesla P100 on CUDA index #0” accessed though different software libraries would be seen as different backends. A single software library can expose multiple backends, one per device  (e.g. each CUDA GPU in a system is exposed as a separate backend, or CPU, GPU, and DSP on a mobile chipset are exposed as three different backends).</p>
<p>We recommend that vendors make the backend object reference-counted, and use <code class="docutils literal notranslate"><span class="pre">uint32_t</span> <span class="pre">magic</span></code> as the first data field of the object:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">MyBackend</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">magic</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">referenceCount</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="cm">/* This line won&#39;t compile, but gives you an idea of relation between MyBackend structure and onnxBackend type. */</span><span class="w"></span>
<span class="k">typedef</span><span class="w"> </span><span class="n">MyBackend</span><span class="o">*</span><span class="w"> </span><span class="n">onnxBackend</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Magic is an arbitrary 32-bit integer unique for a library implementing the API. It should be used to verify that the backend object passed to <code class="docutils literal notranslate"><span class="pre">onnxInitGraph</span></code> was created by <code class="docutils literal notranslate"><span class="pre">onnxInitBackend</span></code> in the same library.</p>
</section>
<section id="graph-object">
<h3>Graph object<a class="headerlink" href="#graph-object" title="Permalink to this headline">¶</a></h3>
<p>Graph object is a vendor-specific representation of ONNX ModelProto message. Graph is logically related to the backend used to create it, and a typical implementation of a graph object would hold a reference to its backend object.</p>
<p>We recommend that vendors use <code class="docutils literal notranslate"><span class="pre">uint32_t</span> <span class="pre">magic</span></code> as the first data field of the graph object:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">MyGraph</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">magic</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="k">struct</span><span class="w"> </span><span class="nc">MyBackend</span><span class="o">*</span><span class="w"> </span><span class="n">backend</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="cm">/* This line won&#39;t compile, but gives you an idea of relation between MyGraph structure and onnxGraph type. */</span><span class="w"></span>
<span class="k">typedef</span><span class="w"> </span><span class="n">MyGraph</span><span class="o">*</span><span class="w"> </span><span class="n">onnxGraph</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Magic is an arbitrary 32-bit integer unique for a library implementing the API. It should be used to verify that the backend object passed to <code class="docutils literal notranslate"><span class="pre">onnxInitGraph</span></code> was created by <code class="docutils literal notranslate"><span class="pre">onnxInitBackend</span></code> in the same library. Magic for a graph object should be different from magic of a backend object of the same library.</p>
</section>
<section id="library-initialization">
<h3>Library initialization<a class="headerlink" href="#library-initialization" title="Permalink to this headline">¶</a></h3>
<p>During one-time library initialization, the implementation of the API would detect <code class="docutils literal notranslate"><span class="pre">n</span></code> supported devices and map them to backend indices in <code class="docutils literal notranslate"><span class="pre">0...(n-1)</span></code> range. The implementation of device discovery and checking required device characteristics is highly vendor- and platform-specific, e.g.:</p>
<ul class="simple">
<li><p>A CPU implementation may always expose 1 device.</p></li>
<li><p>A CUDA-based implementation may call <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceCount</span></code> to get the number of CUDA-enabled devices, then
call <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceProperties</span></code> for each device, and map CUDA devices which satisfy the minimum required functionality, such as compute capability, to backend indices.</p></li>
<li><p>An OpenCL-based implementation for a mobile GPU would try to load OpenCL library, call <code class="docutils literal notranslate"><span class="pre">clGetPlatformIDs</span></code> and <code class="docutils literal notranslate"><span class="pre">clGetPlatformInfo</span></code> to find a supported platform, then call <code class="docutils literal notranslate"><span class="pre">clGetDeviceIDs</span></code> and <code class="docutils literal notranslate"><span class="pre">clGetDeviceInfo</span></code> to find a supported GPU device, and map it to the only exposed backend if such device exists, or expose 0 devices otherwise.</p></li>
<li><p>An implementation for hardware neural network accelerators would call vendor-specific driver API to discover accelerator devices installed in the system and map them to backend indices.</p></li>
</ul>
<p>We recommend that library initialization is triggered on the first call to <code class="docutils literal notranslate"><span class="pre">onnxGetNumBackends</span></code>, <code class="docutils literal notranslate"><span class="pre">onnxGetBackendInfo</span></code>, or <code class="docutils literal notranslate"><span class="pre">onnxInitBackend</span></code>. Using a global static C++ object for initialization may hurt portability if library initialization involves loading other shared libraries (DLLs): on Windows <code class="docutils literal notranslate"><span class="pre">LoadLibrary</span></code> function can’t be used in initializers of global static objects.</p>
</section>
<section id="onnxgetnumbackends">
<h3>onnxGetNumBackends<a class="headerlink" href="#onnxgetnumbackends" title="Permalink to this headline">¶</a></h3>
<p>Implementation would <span class="xref myst">initialize the library</span>, if it wasn’t initialized already, and return the number <code class="docutils literal notranslate"><span class="pre">n</span></code> of available backends.</p>
</section>
<section id="onnxgetbackendinfo">
<h3>onnxGetBackendInfo<a class="headerlink" href="#onnxgetbackendinfo" title="Permalink to this headline">¶</a></h3>
<p>Implementation would <span class="xref myst">initialize the library</span>, if it wasn’t initialized already, and query information about the backend using vendor- or platform-specific API (e.g. <code class="docutils literal notranslate"><span class="pre">cudaGetDeviceProperties</span></code>, <code class="docutils literal notranslate"><span class="pre">clGetDeviceInfo</span></code>, CPUID instruction). Implementation can cache this information when it is first queried or during initialization, and return the cached value.</p>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>