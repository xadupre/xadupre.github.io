
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Python API Overview &#8212; onnxcustom</title>
    
    <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/thebelab-helper.js"></script>
    <script src="../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="IR.html">
   Open Neural Network Exchange Intermediate Representation (ONNX IR) Specification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Python API Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OpConventions.html">
   Operator Conventions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DimensionDenotation.html">
   Dimension Denotation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Broadcasting.html">
   Broadcasting in ONNX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ExternalData.html">
   External Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Hub.html">
   ONNX Model Hub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_metadata.html">
   Metatdata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ShapeInference.html">
   ONNX Shape Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CIPipelines.html">
   ONNX CI Pipelines
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Syntax.html">
   Overview
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Versioning.html">
   ONNX Versioning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VersionConverter.html">
   ONNX Version Converter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Relicensing.html">
   Relicensing MIT to Apache-2.0
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_releases.html">
   Onnx Releases
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_operators.html">
   ONNX Operators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_operators_ml.html">
   ONNX ML Operators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_changelog.html">
   Change Logs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_changelog_ml.html">
   ML Change Logs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_test_coverage.html">
   Test Coverage (Operators)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_test_coverage_ml.html">
   Test Coverage (ML Operators)
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_contributing.html">
   Contributing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_add_new_op.html">
   Adding a new operator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ImplementingAnOnnxBackend.html">
   Implementing an ONNX backend
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OnnxBackendTest.html">
   ONNX Backend Test
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_managing.html">
   Onnx Releases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNXIFI.html">
   ONNX Interface for Framework Integration (ONNXIFI)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNXTypes.html">
   Optional Type
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TypeAnnotations.html">
   Type annotations for ONNX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TypeDenotation.html">
   Type Denotation
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="DefineDifferentiability.html">
   A Short Guide on the Differentiability Tag for ONNX Operators
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-an-onnx-model">
   Loading an ONNX Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-an-onnx-model-with-external-data">
   Loading an ONNX Model with External Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#converting-an-onnx-model-to-external-data">
   Converting an ONNX Model to External Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-an-onnx-model">
   Saving an ONNX Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#converting-and-saving-an-onnx-model-to-external-data">
   Converting and Saving an ONNX Model to External Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#manipulating-tensorproto-and-numpy-array">
   Manipulating TensorProto and Numpy Array
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-an-onnx-model-using-helper-functions">
   Creating an ONNX Model Using Helper Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#checking-an-onnx-model">
   Checking an ONNX Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#checking-a-large-onnx-model-2gb">
     Checking a Large ONNX Model &gt;2GB
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-shape-inference-on-an-onnx-model">
   Running Shape Inference on an ONNX Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shape-inference-a-large-onnx-model-2gb">
     Shape inference a Large ONNX Model &gt;2GB
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#converting-version-of-an-onnx-model-within-default-domain-ai-onnx">
   Converting Version of an ONNX Model within Default Domain (“”/”ai.onnx”)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utility-functions">
   Utility Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extracting-sub-model-with-inputs-outputs-tensor-names">
     Extracting Sub-model with Inputs Outputs Tensor Names
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-compose">
     ONNX Compose
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tools">
   Tools
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#updating-model-s-inputs-outputs-dimension-sizes-with-variable-length">
     Updating Model’s Inputs Outputs Dimension Sizes with Variable Length
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#onnx-parser">
   ONNX Parser
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <!--- SPDX-License-Identifier: Apache-2.0 -->
<section id="python-api-overview">
<h1>Python API Overview<a class="headerlink" href="#python-api-overview" title="Permalink to this headline">¶</a></h1>
<section id="loading-an-onnx-model">
<h2>Loading an ONNX Model<a class="headerlink" href="#loading-an-onnx-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># onnx_model is an in-memory ModelProto</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Runnable IPython notebooks:</p>
<ul class="simple">
<li><p><span class="xref myst">load_model.ipynb</span></p></li>
</ul>
</section>
<section id="loading-an-onnx-model-with-external-data">
<h2>Loading an ONNX Model with External Data<a class="headerlink" href="#loading-an-onnx-model-with-external-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>[Default] If the external data is under the same directory of the model, simply use <code class="docutils literal notranslate"><span class="pre">onnx.load()</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If the external data is under another directory, use <code class="docutils literal notranslate"><span class="pre">load_external_data_for_model()</span></code> to specify the directory path and load after using <code class="docutils literal notranslate"><span class="pre">onnx.load()</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx.external_data_helper</span> <span class="kn">import</span> <span class="n">load_external_data_for_model</span>

<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">,</span> <span class="n">load_external_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">load_external_data_for_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="s1">&#39;data/directory/path/&#39;</span><span class="p">)</span>
<span class="c1"># Then the onnx_model has loaded the external data from the specific directory</span>
</pre></div>
</div>
</section>
<section id="converting-an-onnx-model-to-external-data">
<h2>Converting an ONNX Model to External Data<a class="headerlink" href="#converting-an-onnx-model-to-external-data" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">onnx.external_data_helper</span> <span class="kn">import</span> <span class="n">convert_model_to_external_data</span>

<span class="c1"># onnx_model is an in-memory ModelProto</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">convert_model_to_external_data</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">all_tensors_to_one_file</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="s1">&#39;filename&#39;</span><span class="p">,</span> <span class="n">size_threshold</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">convert_attribute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Then the onnx_model has converted raw data as external data</span>
<span class="c1"># Must be followed by save</span>
</pre></div>
</div>
</section>
<section id="saving-an-onnx-model">
<h2>Saving an ONNX Model<a class="headerlink" href="#saving-an-onnx-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># onnx_model is an in-memory ModelProto</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># Save the ONNX model</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Runnable IPython notebooks:</p>
<ul class="simple">
<li><p><span class="xref myst">save_model.ipynb</span></p></li>
</ul>
</section>
<section id="converting-and-saving-an-onnx-model-to-external-data">
<h2>Converting and Saving an ONNX Model to External Data<a class="headerlink" href="#converting-and-saving-an-onnx-model-to-external-data" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># onnx_model is an in-memory ModelProto</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="s1">&#39;path/to/save/the/model.onnx&#39;</span><span class="p">,</span> <span class="n">save_as_external_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">all_tensors_to_one_file</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="s1">&#39;filename&#39;</span><span class="p">,</span> <span class="n">size_threshold</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">convert_attribute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Then the onnx_model has converted raw data as external data and saved to specific directory</span>
</pre></div>
</div>
</section>
<section id="manipulating-tensorproto-and-numpy-array">
<h2>Manipulating TensorProto and Numpy Array<a class="headerlink" href="#manipulating-tensorproto-and-numpy-array" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">numpy_helper</span>

<span class="c1"># Preprocessing: create a Numpy array</span>
<span class="n">numpy_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original Numpy array:</span><span class="se">\n</span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">))</span>

<span class="c1"># Convert the Numpy array to a TensorProto</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">numpy_helper</span><span class="o">.</span><span class="n">from_array</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;TensorProto:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tensor</span><span class="p">))</span>

<span class="c1"># Convert the TensorProto to a Numpy array</span>
<span class="n">new_array</span> <span class="o">=</span> <span class="n">numpy_helper</span><span class="o">.</span><span class="n">to_array</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After round trip, Numpy array:</span><span class="se">\n</span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">new_array</span><span class="p">))</span>

<span class="c1"># Save the TensorProto</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;tensor.pb&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>

<span class="c1"># Load a TensorProto</span>
<span class="n">new_tensor</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">TensorProto</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;tensor.pb&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">new_tensor</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After saving and loading, new TensorProto:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">))</span>
</pre></div>
</div>
<p>Runnable IPython notebooks:</p>
<ul class="simple">
<li><p><span class="xref myst">np_array_tensorproto.ipynb</span></p></li>
</ul>
</section>
<section id="creating-an-onnx-model-using-helper-functions">
<h2>Creating an ONNX Model Using Helper Functions<a class="headerlink" href="#creating-an-onnx-model-using-helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">helper</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">AttributeProto</span><span class="p">,</span> <span class="n">TensorProto</span><span class="p">,</span> <span class="n">GraphProto</span>

<span class="c1"># The protobuf definition can be found here:</span>
<span class="c1"># https://github.com/onnx/onnx/blob/main/onnx/onnx.proto</span>

<span class="c1"># Create one input (ValueInfoProto)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_tensor_value_info</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">TensorProto</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">pads</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_tensor_value_info</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">TensorProto</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">value</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_tensor_value_info</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">AttributeProto</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Create one output (ValueInfoProto)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_tensor_value_info</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">TensorProto</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Create a node (NodeProto) - This is based on Pad-11</span>
<span class="n">node_def</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span>
    <span class="s1">&#39;Pad&#39;</span><span class="p">,</span>                  <span class="c1"># name</span>
    <span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="c1"># inputs</span>
    <span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span>                  <span class="c1"># outputs</span>
    <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span>        <span class="c1"># attributes</span>
<span class="p">)</span>

<span class="c1"># Create the graph (GraphProto)</span>
<span class="n">graph_def</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_graph</span><span class="p">(</span>
    <span class="p">[</span><span class="n">node_def</span><span class="p">],</span>        <span class="c1"># nodes</span>
    <span class="s1">&#39;test-model&#39;</span><span class="p">,</span>      <span class="c1"># name</span>
    <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span>  <span class="c1"># inputs</span>
    <span class="p">[</span><span class="n">Y</span><span class="p">],</span>               <span class="c1"># outputs</span>
<span class="p">)</span>

<span class="c1"># Create the model (ModelProto)</span>
<span class="n">model_def</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_model</span><span class="p">(</span><span class="n">graph_def</span><span class="p">,</span> <span class="n">producer_name</span><span class="o">=</span><span class="s1">&#39;onnx-example&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model is:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_def</span><span class="p">))</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">model_def</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model is checked!&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Runnable IPython notebooks:</p>
<ul class="simple">
<li><p><span class="xref myst">make_model.ipynb</span></p></li>
<li><p><span class="xref myst">Protobufs.ipynb</span></p></li>
</ul>
</section>
<section id="checking-an-onnx-model">
<h2>Checking an ONNX Model<a class="headerlink" href="#checking-an-onnx-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># Preprocessing: load the ONNX model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/the/model.onnx&#39;</span>
<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model is:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">))</span>

<span class="c1"># Check the model</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>
<span class="k">except</span> <span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">ValidationError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model is invalid: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">e</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model is valid!&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Runnable IPython notebooks:</p>
<ul class="simple">
<li><p><span class="xref myst">check_model.ipynb</span></p></li>
</ul>
<section id="checking-a-large-onnx-model-2gb">
<h3>Checking a Large ONNX Model &gt;2GB<a class="headerlink" href="#checking-a-large-onnx-model-2gb" title="Permalink to this headline">¶</a></h3>
<p>Current checker supports checking models with external data, but for those models larger than 2GB, please use the model path for onnx.checker and the external data needs to be under the same directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>
<span class="c1"># onnx.checker.check_model(loaded_onnx_model) will fail if given &gt;2GB model</span>
</pre></div>
</div>
</section>
</section>
<section id="running-shape-inference-on-an-onnx-model">
<h2>Running Shape Inference on an ONNX Model<a class="headerlink" href="#running-shape-inference-on-an-onnx-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">helper</span><span class="p">,</span> <span class="n">shape_inference</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">TensorProto</span>

<span class="c1"># Preprocessing: create a model with two nodes, Y&#39;s shape is unknown</span>
<span class="n">node1</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span><span class="s1">&#39;Transpose&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">node2</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_node</span><span class="p">(</span><span class="s1">&#39;Transpose&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">],</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_graph</span><span class="p">(</span>
    <span class="p">[</span><span class="n">node1</span><span class="p">,</span> <span class="n">node2</span><span class="p">],</span>
    <span class="s1">&#39;two-transposes&#39;</span><span class="p">,</span>
    <span class="p">[</span><span class="n">helper</span><span class="o">.</span><span class="n">make_tensor_value_info</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">TensorProto</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))],</span>
    <span class="p">[</span><span class="n">helper</span><span class="o">.</span><span class="n">make_tensor_value_info</span><span class="p">(</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span> <span class="n">TensorProto</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))],</span>
<span class="p">)</span>

<span class="n">original_model</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">make_model</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">producer_name</span><span class="o">=</span><span class="s1">&#39;onnx-examples&#39;</span><span class="p">)</span>

<span class="c1"># Check the model and print Y&#39;s shape information</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">original_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before shape inference, the shape info of Y is:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">value_info</span><span class="p">))</span>

<span class="c1"># Apply shape inference on the model</span>
<span class="n">inferred_model</span> <span class="o">=</span> <span class="n">shape_inference</span><span class="o">.</span><span class="n">infer_shapes</span><span class="p">(</span><span class="n">original_model</span><span class="p">)</span>

<span class="c1"># Check the model and print Y&#39;s shape information</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">inferred_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After shape inference, the shape info of Y is:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">inferred_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">value_info</span><span class="p">))</span>
</pre></div>
</div>
<p>Runnable IPython notebooks:</p>
<ul class="simple">
<li><p><span class="xref myst">shape_inference.ipynb</span></p></li>
</ul>
<section id="shape-inference-a-large-onnx-model-2gb">
<h3>Shape inference a Large ONNX Model &gt;2GB<a class="headerlink" href="#shape-inference-a-large-onnx-model-2gb" title="Permalink to this headline">¶</a></h3>
<p>Current shape_inference supports models with external data, but for those models larger than 2GB, please use the model path for onnx.shape_inference.infer_shapes_path and the external data needs to be under the same directory. You can specify the output path for saving the inferred model; otherwise, the default output path is same as the original model path.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># output the inferred model to the original model path</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">shape_inference</span><span class="o">.</span><span class="n">infer_shapes_path</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>

<span class="c1"># output the inferred model to the specified model path</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">shape_inference</span><span class="o">.</span><span class="n">infer_shapes_path</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">,</span> <span class="s1">&#39;output/inferred/model.onnx&#39;</span><span class="p">)</span>

<span class="c1"># inferred_model = onnx.shape_inference.infer_shapes(loaded_onnx_model) will fail if given &gt;2GB model</span>
</pre></div>
</div>
</section>
</section>
<section id="converting-version-of-an-onnx-model-within-default-domain-ai-onnx">
<h2>Converting Version of an ONNX Model within Default Domain (“”/”ai.onnx”)<a class="headerlink" href="#converting-version-of-an-onnx-model-within-default-domain-ai-onnx" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">version_converter</span><span class="p">,</span> <span class="n">helper</span>

<span class="c1"># Preprocessing: load the model to be converted.</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/the/model.onnx&#39;</span>
<span class="n">original_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model before conversion:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_model</span><span class="p">))</span>

<span class="c1"># A full list of supported adapters can be found here:</span>
<span class="c1"># https://github.com/onnx/onnx/blob/main/onnx/version_converter.py#L21</span>
<span class="c1"># Apply the version conversion on the original model</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">version_converter</span><span class="o">.</span><span class="n">convert_version</span><span class="p">(</span><span class="n">original_model</span><span class="p">,</span> <span class="o">&lt;</span><span class="nb">int</span> <span class="n">target_version</span><span class="o">&gt;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The model after conversion:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">converted_model</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="utility-functions">
<h2>Utility Functions<a class="headerlink" href="#utility-functions" title="Permalink to this headline">¶</a></h2>
<section id="extracting-sub-model-with-inputs-outputs-tensor-names">
<h3>Extracting Sub-model with Inputs Outputs Tensor Names<a class="headerlink" href="#extracting-sub-model-with-inputs-outputs-tensor-names" title="Permalink to this headline">¶</a></h3>
<p>Function <code class="docutils literal notranslate"><span class="pre">extract_model()</span></code> extracts sub-model from an ONNX model.
The sub-model is defined by the names of the input and output tensors <em>exactly</em>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="n">input_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/the/original/model.onnx&#39;</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/save/the/extracted/model.onnx&#39;</span>
<span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;input_0&#39;</span><span class="p">,</span> <span class="s1">&#39;input_1&#39;</span><span class="p">,</span> <span class="s1">&#39;input_2&#39;</span><span class="p">]</span>
<span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;output_0&#39;</span><span class="p">,</span> <span class="s1">&#39;output_1&#39;</span><span class="p">]</span>

<span class="n">onnx</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">extract_model</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="n">output_path</span><span class="p">,</span> <span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="p">)</span>
</pre></div>
</div>
<p>Note: For control-flow operators, e.g. If and Loop, the <em>boundary of sub-model</em>,
which is defined by the input and output tensors, should not <em>cut through</em> the
subgraph that is connected to the <em>main graph</em> as attributes of these operators.</p>
</section>
<section id="onnx-compose">
<h3>ONNX Compose<a class="headerlink" href="#onnx-compose" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">onnx.compose</span></code> module provides tools to create combined models.</p>
<p><code class="docutils literal notranslate"><span class="pre">onnx.compose.merge_models</span></code> can be used to merge two models, by connecting some of the outputs
from the first model with inputs from the second model. By default, inputs/outputs not present in the
<code class="docutils literal notranslate"><span class="pre">io_map</span></code> argument will remain as inputs/outputs of the combined model.</p>
<p>In this example we merge two models by connecting each output of the first model to an output in the second. The resulting model will have the same inputs as the first model and the same outputs as the second:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="n">model1</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/model1.onnx&#39;</span><span class="p">)</span>
<span class="c1"># agraph (float[N] A, float[N] B) =&gt; (float[N] C, float[N] D)</span>
<span class="c1">#   {</span>
<span class="c1">#      C = Add(A, B)</span>
<span class="c1">#      D = Sub(A, B)</span>
<span class="c1">#   }</span>

<span class="n">model2</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/model2.onnx&#39;</span><span class="p">)</span>
<span class="c1">#   agraph (float[N] X, float[N] Y) =&gt; (float[N] Z)</span>
<span class="c1">#   {</span>
<span class="c1">#      Z = Mul(X, Y)</span>
<span class="c1">#   }</span>

<span class="n">combined_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">merge_models</span><span class="p">(</span>
    <span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">,</span>
    <span class="n">io_map</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Additionally, a user can specify a list of <code class="docutils literal notranslate"><span class="pre">inputs</span></code>/<code class="docutils literal notranslate"><span class="pre">outputs</span></code> to be included in the combined model,
effectively dropping the part of the graph that doesn’t contribute to the combined model outputs.
In the following example, we are connecting only one of the two outputs in the first model
to both inputs in the second. By specifying the outputs of the combined model explicitly, we are dropping the output not consumed from the first model, and the relevant part of the graph:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># Default case. Include all outputs in the combined model</span>
<span class="n">combined_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">merge_models</span><span class="p">(</span>
    <span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">,</span>
    <span class="n">io_map</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">)],</span>
<span class="p">)</span>  <span class="c1"># outputs: &#39;D&#39;, &#39;Z&#39;</span>

<span class="c1"># Explicit outputs. &#39;Y&#39; output and the Sub node are not present in the combined model</span>
<span class="n">combined_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">merge_models</span><span class="p">(</span>
    <span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">,</span>
    <span class="n">io_map</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">)],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">],</span>
<span class="p">)</span>  <span class="c1"># outputs: &#39;Z&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">onnx.compose.add_prefix</span></code> allows you to add a prefix to names in the model, to avoid a name collision
when merging them. By default, it renames all names in the graph: inputs, outputs, edges, nodes,
initializers, sparse initializers and value infos.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>
<span class="c1"># model - outputs: [&#39;out0&#39;, &#39;out1&#39;], inputs: [&#39;in0&#39;, &#39;in1&#39;]</span>

<span class="n">new_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">add_prefix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;m1/&#39;</span><span class="p">)</span>
<span class="c1"># new_model - outputs: [&#39;m1/out0&#39;, &#39;m1/out1&#39;], inputs: [&#39;m1/in0&#39;, &#39;m1/in1&#39;]</span>

<span class="c1"># Can also be run in-place</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">add_prefix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;m1/&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">onnx.compose.expand_out_dim</span></code> can be used to connect models that expect a different number
of dimensions by inserting dimensions with extent one. This can be useful, when combining a
model producing samples with a model that works with batches of samples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># outputs: &#39;out0&#39;, shape=[200, 200, 3]</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model1.onnx&#39;</span><span class="p">)</span>

<span class="c1"># outputs: &#39;in0&#39;, shape=[N, 200, 200, 3]</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model2.onnx&#39;</span><span class="p">)</span>

<span class="c1"># outputs: &#39;out0&#39;, shape=[1, 200, 200, 3]</span>
<span class="n">new_model1</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">expand_out_dims</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">dim_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Models can now be merged</span>
<span class="n">combined_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">merge_models</span><span class="p">(</span>
    <span class="n">new_model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">,</span> <span class="n">io_map</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;out0&#39;</span><span class="p">,</span> <span class="s1">&#39;in0&#39;</span><span class="p">)]</span>
<span class="p">)</span>

<span class="c1"># Can also be run in-place</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">compose</span><span class="o">.</span><span class="n">expand_out_dims</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">dim_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="tools">
<h2>Tools<a class="headerlink" href="#tools" title="Permalink to this headline">¶</a></h2>
<section id="updating-model-s-inputs-outputs-dimension-sizes-with-variable-length">
<h3>Updating Model’s Inputs Outputs Dimension Sizes with Variable Length<a class="headerlink" href="#updating-model-s-inputs-outputs-dimension-sizes-with-variable-length" title="Permalink to this headline">¶</a></h3>
<p>Function <code class="docutils literal notranslate"><span class="pre">update_inputs_outputs_dims</span></code> updates the dimension of the inputs and outputs of the model,
to the provided values in the parameter. You could provide both static and dynamic dimension size,
by using dim_param. For more information on static and dynamic dimension size, checkout <span class="xref myst">Tensor Shapes</span>.</p>
<p>The function runs model checker after the input/output sizes are updated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">from</span> <span class="nn">onnx.tools</span> <span class="kn">import</span> <span class="n">update_model_dims</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;path/to/the/model.onnx&#39;</span><span class="p">)</span>
<span class="c1"># Here both &#39;seq&#39;, &#39;batch&#39; and -1 are dynamic using dim_param.</span>
<span class="n">variable_length_model</span> <span class="o">=</span> <span class="n">update_model_dims</span><span class="o">.</span><span class="n">update_inputs_outputs_dims</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;input_name&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;seq&#39;</span><span class="p">,</span> <span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="p">{</span><span class="s1">&#39;output_name&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;seq&#39;</span><span class="p">,</span> <span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]})</span>
</pre></div>
</div>
</section>
</section>
<section id="onnx-parser">
<h2>ONNX Parser<a class="headerlink" href="#onnx-parser" title="Permalink to this headline">¶</a></h2>
<p>Functions <code class="docutils literal notranslate"><span class="pre">onnx.parser.parse_model</span></code> and <code class="docutils literal notranslate"><span class="pre">onnx.parser.parse_graph</span></code> can be used to create an ONNX model
or graph from a textual representation as shown below. See <a class="reference internal" href="Syntax.html"><span class="doc std std-doc">Language Syntax</span></a> for more details
about the language syntax.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="s1">   agraph (float[N, 128] X, float[128, 10] W, float[10] B) =&gt; (float[N, 10] C)</span>
<span class="s1">   {</span>
<span class="s1">        T = MatMul(X, W)</span>
<span class="s1">        S = Add(T, B)</span>
<span class="s1">        C = Softmax(S)</span>
<span class="s1">   }</span>
<span class="s1">&#39;&#39;&#39;</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">parser</span><span class="o">.</span><span class="n">parse_graph</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="s1">   &lt;</span>
<span class="s1">     ir_version: 7,</span>
<span class="s1">     opset_import: [&quot;&quot; : 10]</span>
<span class="s1">   &gt;</span>
<span class="s1">   agraph (float[N, 128] X, float[128, 10] W, float[10] B) =&gt; (float[N, 10] C)</span>
<span class="s1">   {</span>
<span class="s1">      T = MatMul(X, W)</span>
<span class="s1">      S = Add(T, B)</span>
<span class="s1">      C = Softmax(S)</span>
<span class="s1">   }</span>
<span class="s1">&#39;&#39;&#39;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">parser</span><span class="o">.</span><span class="n">parse_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

</pre></div>
</div>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>