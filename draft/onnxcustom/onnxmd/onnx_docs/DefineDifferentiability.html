
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>A Short Guide on the Differentiability Tag for ONNX Operators &#8212; onnxcustom</title>
    
    <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/thebelab-helper.js"></script>
    <script src="../../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../tutorials/index.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../api/apis.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../gyexamples/index.html">
  Examples Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../all_notebooks.html">
  Notebooks Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../other_pages.html">
  Other pages
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../blog/blogindex.html">
  Blog Gallery
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="IR.html">
   Open Neural Network Exchange Intermediate Representation (ONNX IR) Specification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PythonAPIOverview.html">
   Python API Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OpConventions.html">
   Operator Conventions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DimensionDenotation.html">
   Dimension Denotation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Broadcasting.html">
   Broadcasting in ONNX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ExternalData.html">
   External Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Hub.html">
   ONNX Model Hub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_metadata.html">
   Metatdata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ShapeInference.html">
   ONNX Shape Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CIPipelines.html">
   ONNX CI Pipelines
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Syntax.html">
   Overview
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Versioning.html">
   ONNX Versioning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VersionConverter.html">
   ONNX Version Converter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Relicensing.html">
   Relicensing MIT to Apache-2.0
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_releases.html">
   Onnx Releases
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_operators.html">
   ONNX Operators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_operators_ml.html">
   ONNX ML Operators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_changelog.html">
   Change Logs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_changelog_ml.html">
   ML Change Logs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_test_coverage.html">
   Test Coverage (Operators)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_test_coverage_ml.html">
   Test Coverage (ML Operators)
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_contributing.html">
   Contributing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_add_new_op.html">
   Adding a new operator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ImplementingAnOnnxBackend.html">
   Implementing an ONNX backend
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OnnxBackendTest.html">
   ONNX Backend Test
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_managing.html">
   Onnx Releases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNXIFI.html">
   ONNX Interface for Framework Integration (ONNXIFI)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ONNXTypes.html">
   Optional Type
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TypeAnnotations.html">
   Type annotations for ONNX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TypeDenotation.html">
   Type Denotation
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   A Short Guide on the Differentiability Tag for ONNX Operators
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#differentiability-tag">
   Differentiability Tag
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ways-to-define-differentiability-tag">
   Ways to Define Differentiability Tag
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#method-1-reuse-existing-deep-learning-frameworks">
     Method 1: Reuse Existing Deep Learning Frameworks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#method-2-manually-do-the-math">
     Method 2: Manually Do the Math
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <!--- SPDX-License-Identifier: Apache-2.0 -->
<section id="a-short-guide-on-the-differentiability-tag-for-onnx-operators">
<h1>A Short Guide on the Differentiability Tag for ONNX Operators<a class="headerlink" href="#a-short-guide-on-the-differentiability-tag-for-onnx-operators" title="Permalink to this headline">¶</a></h1>
<section id="differentiability-tag">
<h2>Differentiability Tag<a class="headerlink" href="#differentiability-tag" title="Permalink to this headline">¶</a></h2>
<p>The ONNX operator schema for each operator includes a differentiability tag for each input and output.
In this document, we explain the meaning of this tag and how to ensure the correctness of the tags.
Briefly, the tag identifies the set of differentiable inputs and differentiable outputs of an operator.
The meaning of the tag is that the partial derivative of each differentiable output is defined with respect to each differentiable output.</p>
</section>
<section id="ways-to-define-differentiability-tag">
<h2>Ways to Define Differentiability Tag<a class="headerlink" href="#ways-to-define-differentiability-tag" title="Permalink to this headline">¶</a></h2>
<p>The differentiability definition of an operator consists of several aspects.</p>
<ul class="simple">
<li><p>Differentiable inputs, which can be referenced in Gradient’s <code class="docutils literal notranslate"><span class="pre">xs</span></code> attribute.</p></li>
<li><p>Differentiable outputs, which can be referenced in Gradient’s <code class="docutils literal notranslate"><span class="pre">y</span></code> attribute.</p></li>
<li><p>The math equation to compute the Jacobian matrix (or tensor). If a variable (input or output) is differentiable or not is judged by math. If the Jacobian matrix (or tensor) exists, then the considered operator has some differentiable inputs and outputs.</p></li>
</ul>
<p>There are several strategies to implement auto-differentiation such as forward accumulation, backward accumulation, and dual variable.
Because most deep learning frameworks are backward-based, the reviewers should ensure the PR authors of tags provide enough details on that.
We present a couple of methods below to verify the differentiability for ONNX operator.</p>
<section id="method-1-reuse-existing-deep-learning-frameworks">
<h3>Method 1: Reuse Existing Deep Learning Frameworks<a class="headerlink" href="#method-1-reuse-existing-deep-learning-frameworks" title="Permalink to this headline">¶</a></h3>
<p>The first way is to show that the considered operator’s backward operation exists in an existing framework such as Pytorch or Tensorflow. In this case, the author should provide a runnable python script which computes the backward pass of the considered operator. The author should also point out how to map the Pytorch or Tensor code to ONNX format (for example, the author can call <code class="docutils literal notranslate"><span class="pre">torch.onnx.export</span></code> to save an ONNX model). The following script shows the differentiability of ONNX Reshape using  Pytorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># A single-operator model. It&#39;s literally a Pytorch Reshape.</span>
<span class="c1"># Note that Pytorch Reshape can be directly mapped to ONNX Reshape.</span>
<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">(),))</span>
    <span class="n">y</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">dy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>

<span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">([</span><span class="n">y</span><span class="p">],</span>
  <span class="n">grad_tensors</span><span class="o">=</span><span class="p">[</span><span class="n">dy</span><span class="p">],</span>
  <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="n">grad_variables</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># This example shows the input and the output in Pytorch are differentiable.</span>
<span class="c1"># From the exported ONNX model below, we also see that &quot;x&quot; is the first input</span>
<span class="c1"># of ONNX Reshape and &quot;y&quot; the output of ONNX Reshape. Therefore, we can say</span>
<span class="c1"># the first input and the output of ONNX Reshape are differentiable.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
  <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="method-2-manually-do-the-math">
<h3>Method 2: Manually Do the Math<a class="headerlink" href="#method-2-manually-do-the-math" title="Permalink to this headline">¶</a></h3>
<p>The second way is formally proving the existence of the Jacobian matrix (or tensor) from outputs to inputs with at least two numerical examples. In this case, the reviewer should go through the math and confirm if the numerical result is correct. The author should add enough details so that any STEM graduated student can easily review it.</p>
<p>For example, to show the differentiability of Add, the author may first write down its equation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">B</span>
</pre></div>
</div>
<p>For the sake of simplicity, assume <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> are same-shape vector.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">]</span><span class="o">^</span><span class="n">T</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[</span><span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">]</span><span class="o">^</span><span class="n">T</span>
<span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">]</span><span class="o">^</span><span class="n">T</span>
</pre></div>
</div>
<p>Here we use the symbol <code class="docutils literal notranslate"><span class="pre">^T</span></code> to denote transpose of the attached matrix or vector.
Let <code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">=</span> <span class="pre">[a1,</span> <span class="pre">a2,</span> <span class="pre">b1,</span> <span class="pre">b2]^T</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span> <span class="pre">=</span> <span class="pre">[c1,</span> <span class="pre">c2]^T</span></code> and consider Add as a function which maps <code class="docutils literal notranslate"><span class="pre">X</span></code> to <code class="docutils literal notranslate"><span class="pre">Y</span></code>.
Then, this function’s Jacobian matrix is a 4-by-2 matrix,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">J</span> <span class="o">=</span> <span class="p">[[</span><span class="n">dc1</span><span class="o">/</span><span class="n">da1</span><span class="p">,</span> <span class="n">dc2</span><span class="o">/</span><span class="n">da1</span><span class="p">],</span>
     <span class="p">[</span><span class="n">dc1</span><span class="o">/</span><span class="n">da2</span><span class="p">,</span> <span class="n">dc2</span><span class="o">/</span><span class="n">da2</span><span class="p">],</span>
     <span class="p">[</span><span class="n">dc1</span><span class="o">/</span><span class="n">db1</span><span class="p">,</span> <span class="n">dc2</span><span class="o">/</span><span class="n">db1</span><span class="p">],</span>
     <span class="p">[</span><span class="n">dc1</span><span class="o">/</span><span class="n">db2</span><span class="p">,</span> <span class="n">dc2</span><span class="o">/</span><span class="n">db2</span><span class="p">]]</span>
  <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
<p>If</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dL</span><span class="o">/</span><span class="n">dC</span> <span class="o">=</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">dc1</span><span class="p">,</span> <span class="n">dL</span><span class="o">/</span><span class="n">dc2</span><span class="p">]</span><span class="o">^</span><span class="n">T</span><span class="p">,</span>
</pre></div>
</div>
<p>then <code class="docutils literal notranslate"><span class="pre">dL/dA</span> <span class="pre">=</span> <span class="pre">[dL/da1,</span> <span class="pre">dL/da2]^T</span></code> and <code class="docutils literal notranslate"><span class="pre">dL/dB</span> <span class="pre">=</span> <span class="pre">[dL/db1,</span> <span class="pre">dL/db2]^T</span></code> can be computed from elements in</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="p">[[</span><span class="n">dL</span><span class="o">/</span><span class="n">da1</span><span class="p">],</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">da2</span><span class="p">],</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">db1</span><span class="p">],</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">db2</span><span class="p">]]</span>
<span class="o">=</span> <span class="n">J</span> <span class="o">*</span> <span class="n">dL</span><span class="o">/</span><span class="n">dC</span>
<span class="o">=</span> <span class="p">[[</span><span class="n">dL</span><span class="o">/</span><span class="n">dc1</span><span class="p">],</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">dc2</span><span class="p">],</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">dc1</span><span class="p">],</span> <span class="p">[</span><span class="n">dL</span><span class="o">/</span><span class="n">dc2</span><span class="p">]]</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">*</span></code> is standard matrix multiplication.
If <code class="docutils literal notranslate"><span class="pre">dL/dC</span> <span class="pre">=</span> <span class="pre">[0.2,</span> <span class="pre">0.8]^T</span></code>, then <code class="docutils literal notranslate"><span class="pre">dL/dA</span> <span class="pre">=</span> <span class="pre">[0.2,</span> <span class="pre">0.8]^T</span></code> and <code class="docutils literal notranslate"><span class="pre">dL/dB</span> <span class="pre">=</span> <span class="pre">[0.2,</span> <span class="pre">0.8]^T</span></code>.
Notice that the procedure to compute <code class="docutils literal notranslate"><span class="pre">dL/dA</span></code> and <code class="docutils literal notranslate"><span class="pre">dL/dB</span></code> from <code class="docutils literal notranslate"><span class="pre">dL/dC</span></code> is usually called backward of an operator.
We can see backward operator of Add takes <code class="docutils literal notranslate"><span class="pre">dL/dC</span></code> as an input and produces two outputs <code class="docutils literal notranslate"><span class="pre">dL/dA</span></code> and <code class="docutils literal notranslate"><span class="pre">dL/dB</span></code>.
Consequently, all of <code class="docutils literal notranslate"><span class="pre">A</span></code>, <code class="docutils literal notranslate"><span class="pre">B</span></code>, and <code class="docutils literal notranslate"><span class="pre">C</span></code> are differentiable.
By flattening tensor into 1-D vector, this example can be extended to cover all tensors when shape broadcasting is not needed.
If broadcasting happens, the broadcasted element’s gradient is the sum of all associated elements’ gradient in its <strong>non-broadcasting</strong> case.
Let’s consider the above example again.
If <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">=</span> <span class="pre">[b]^T</span></code> becomes an 1-element vector, <code class="docutils literal notranslate"><span class="pre">B</span></code> may be broadcasted to <code class="docutils literal notranslate"><span class="pre">[b1,</span> <span class="pre">b2]^T</span></code> and <code class="docutils literal notranslate"><span class="pre">dL/dB</span> <span class="pre">=</span> <span class="pre">[dL/</span> <span class="pre">db]^T</span> <span class="pre">=</span> <span class="pre">[dL/db1</span> <span class="pre">+</span> <span class="pre">dL/db2]^T</span></code>.
For high-dimensional tensors, this is in fact a ReduceSum operation along all expanded axes.</p>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>