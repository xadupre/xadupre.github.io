{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Measuring CPU/GPU performance with a vector sum\n\nThe examples compares multiple versions of a vector sum,\nCPU, GPU.\n\n## Vector Sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\nimport numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\nfrom onnx_extended.validation.cpu._validation import (\n    vector_sum_array_avx as vector_sum_avx,\n    vector_sum_array_avx_parallel as vector_sum_avx_parallel,\n)\n\ntry:\n    from onnx_extended.validation.cuda.cuda_example_py import (\n        vector_sum0,\n        vector_sum6,\n        vector_sum_atomic,\n    )\nexcept ImportError:\n    # CUDA is not available\n    vector_sum0 = None\n\nobs = []\ndims = [500, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 2000]\nif unit_test_going():\n    dims = dims[:3]\nfor dim in tqdm(dims):\n    values = numpy.ones((dim, dim), dtype=numpy.float32).ravel()\n\n    diff = abs(vector_sum_avx(dim, values) - dim**2)\n    res = measure_time(lambda: vector_sum_avx(dim, values), max_time=0.5)\n\n    obs.append(\n        dict(\n            dim=dim,\n            size=values.size,\n            time=res[\"average\"],\n            direction=\"avx\",\n            time_per_element=res[\"average\"] / dim**2,\n            diff=diff,\n        )\n    )\n\n    diff = abs(vector_sum_avx_parallel(dim, values) - dim**2)\n    res = measure_time(lambda: vector_sum_avx_parallel(dim, values), max_time=0.5)\n\n    obs.append(\n        dict(\n            dim=dim,\n            size=values.size,\n            time=res[\"average\"],\n            direction=\"avx//\",\n            time_per_element=res[\"average\"] / dim**2,\n            diff=diff,\n        )\n    )\n\n    if vector_sum0 is None:\n        # CUDA is not available\n        continue\n\n    diff = abs(vector_sum0(values, 32) - dim**2)\n    res = measure_time(lambda: vector_sum0(values, 32), max_time=0.5)\n\n    obs.append(\n        dict(\n            dim=dim,\n            size=values.size,\n            time=res[\"average\"],\n            direction=\"0cuda32\",\n            time_per_element=res[\"average\"] / dim**2,\n            diff=diff,\n        )\n    )\n\n    diff = abs(vector_sum_atomic(values, 32) - dim**2)\n    res = measure_time(lambda: vector_sum_atomic(values, 32), max_time=0.5)\n\n    obs.append(\n        dict(\n            dim=dim,\n            size=values.size,\n            time=res[\"average\"],\n            direction=\"Acuda32\",\n            time_per_element=res[\"average\"] / dim**2,\n            diff=diff,\n        )\n    )\n\n    diff = abs(vector_sum6(values, 32) - dim**2)\n    res = measure_time(lambda: vector_sum6(values, 32), max_time=0.5)\n\n    obs.append(\n        dict(\n            dim=dim,\n            size=values.size,\n            time=res[\"average\"],\n            direction=\"6cuda32\",\n            time_per_element=res[\"average\"] / dim**2,\n            diff=diff,\n        )\n    )\n\n    diff = abs(vector_sum6(values, 256) - dim**2)\n    res = measure_time(lambda: vector_sum6(values, 256), max_time=0.5)\n\n    obs.append(\n        dict(\n            dim=dim,\n            size=values.size,\n            time=res[\"average\"],\n            direction=\"6cuda256\",\n            time_per_element=res[\"average\"] / dim**2,\n            diff=diff,\n        )\n    )\n\ndf = DataFrame(obs)\npiv = df.pivot(index=\"dim\", columns=\"direction\", values=\"time_per_element\")\nprint(piv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv_diff = df.pivot(index=\"dim\", columns=\"direction\", values=\"diff\")\npiv_time = df.pivot(index=\"dim\", columns=\"direction\", values=\"time\")\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 6))\npiv.plot(ax=ax[0], logx=True, title=\"Comparison between two summation\")\npiv_diff.plot(ax=ax[1], logx=True, logy=True, title=\"Summation errors\")\npiv_time.plot(ax=ax[2], logx=True, logy=True, title=\"Total time\")\nfig.savefig(\"plot_bench_gpu_vector_sum_gpu.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results should look like the following.\n\n<img src=\"file://../_static/vector_sum6_results.png\">\n\nAVX is still faster. Let's try to understand why.\n\n## Profiling\n\nThe profiling indicates where the program is most of the time.\nIt shows when the GPU is waiting and when the memory is copied from\nfrom host (CPU) to device (GPU) and the other way around. There are\nthe two steps we need to reduce or avoid to make use of the GPU.\n\nProfiling with [nsight-compute](https://developer.nvidia.com/nsight-compute):\n\n::\n\n    nsys profile --trace=cuda,cudnn,cublas,osrt,nvtx,openmp python <file>\n\nIf `nsys` fails to find `python`, the command `which python` should locate it.\n`<file> can be `plot_bench_gpu_vector_sum_gpu.py` for example.\n\nThen command `nsys-ui` starts the Visual Interface interface of the profiling.\nA screen shot shows the following after loading the profiling.\n\n<img src=\"file://../_static/vector_sum6.png\">\n\nMost of time is spent in copy the data from CPU memory to GPU memory.\nIn our case, GPU is not really useful because just copying the data from CPU\nto GPU takes more time than processing it with CPU and AVX instructions.\n\nGPU is useful for deep learning because many operations can be chained and\nthe data stays on GPU memory until the very end. When multiple tools are involved,\ntorch, numpy, onnxruntime, the [DLPack](https://github.com/dmlc/dlpack)\navoids copying the data when switching.\n\nThe copy of a big tensor can happens by block. The computation may start\nbefore the data is fully copied.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}