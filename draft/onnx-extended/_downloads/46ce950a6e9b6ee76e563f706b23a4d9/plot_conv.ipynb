{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Using C implementation of operator Conv\n\n*onnx-extended* includes an implementation of operator Conv\nin language C++ must faster than the python implementation\navailable in package :epkg:`onnx`. These implementations\nare automatically available through class\n:class:`onnx.reference.CReferenceEvaluator`.\nThe following example compares the processing time for three runtimes.\n\n## Creation of a simple model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom tqdm import tqdm\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_graph,\n    make_model,\n    make_node,\n    make_opsetid,\n    make_tensor_value_info,\n)\nfrom onnx.reference import ReferenceEvaluator\nfrom onnxruntime import InferenceSession\nfrom onnx_extended.ext_test_case import measure_time, unit_test_going\nfrom onnx_extended.reference import CReferenceEvaluator\n\n\nX = make_tensor_value_info(\"X\", TensorProto.FLOAT, [None, None, None, None])\nY = make_tensor_value_info(\"Y\", TensorProto.FLOAT, [None, None, None, None])\nB = make_tensor_value_info(\"B\", TensorProto.FLOAT, [None, None, None, None])\nW = make_tensor_value_info(\"W\", TensorProto.FLOAT, [None, None, None, None])\nnode = make_node(\n    \"Conv\",\n    [\"X\", \"W\", \"B\"],\n    [\"Y\"],\n    pads=[1, 1, 1, 1],\n    dilations=[1, 1],\n    strides=[2, 2],\n)\ngraph = make_graph([node], \"g\", [X, W, B], [Y])\nonnx_model = make_model(graph, opset_imports=[make_opsetid(\"\", 18)], ir_version=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReferenceEvaluator and CReferenceEvaluator\nLet's first compare the outputs are the same.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sH, sW = 64, 64\nX = np.arange(sW * sH).reshape((1, 1, sH, sW)).astype(np.float32)\nW = np.ones((1, 1, 3, 3), dtype=np.float32)\nB = np.array([[[[0]]]], dtype=np.float32)\n\nsess1 = ReferenceEvaluator(onnx_model)\nsess2 = CReferenceEvaluator(onnx_model)\n\nexpected = sess1.run(None, {\"X\": X, \"W\": W, \"B\": B})[0]\ngot = sess2.run(None, {\"X\": X, \"W\": W, \"B\": B})[0]\ndiff = np.abs(expected - got).max()\nprint(f\"difference: {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything works fine.\n\n## Time measurement\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feeds = {\"X\": X, \"W\": W, \"B\": B}\n\nt1 = measure_time(lambda: sess1.run(None, feeds))\nprint(f\"ReferenceEvaluator: {t1['average']}s\")\n\nt2 = measure_time(lambda: sess2.run(None, feeds))\nprint(f\"CReferenceEvaluator: {t2['average']}s\")\nprint(f\"speedup is {t1['average'] / t2['average']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add :epkg:`onnxruntime` as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess3 = InferenceSession(\n    onnx_model.SerializeToString(), provider=[\"CPUExecutionProvider\"]\n)\n\nt3 = measure_time(lambda: sess3.run(None, feeds))\nprint(f\"InferenceSession: {t3['average']}s\")\nprint(f\"speedup is {t1['average'] / t3['average']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = []\n\nfor i in tqdm([16, 32, 48, 64]):\n    sH, sW = i, i\n    X = np.arange(sW * sH).reshape((1, 1, sH, sW)).astype(np.float32)\n    W = np.ones((1, 1, 3, 3), dtype=np.float32)\n    B = np.array([[[[0]]]], dtype=np.float32)\n    feeds = {\"X\": X, \"W\": W, \"B\": B}\n    t1 = measure_time(lambda: sess1.run(None, feeds))\n    t2 = measure_time(lambda: sess2.run(None, feeds))\n    obs = dict(size=i, onnx=t1[\"average\"], onnx_extended=t2[\"average\"])\n    data.append(obs)\n    if unit_test_going() and len(data) >= 3:\n        break\n\ndf = DataFrame(data)\ndf\n\nprint(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = df.set_index(\"size\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\ndf.plot(\n    ax=ax, logx=True, logy=True, title=\"Comparison python / C implementation for Conv\"\n)\ndf[\"speedup\"] = df[\"onnx\"] / df[\"onnx_extended\"]\nax2 = ax.twinx()\ndf[[\"speedup\"]].plot(ax=ax2, color=\"green\")\n\nfig.savefig(\"plot_conv.png\")\n# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}