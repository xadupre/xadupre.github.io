
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>ONNX Backends for onnxruntime1 &#8212; Python Runtime for ONNX</title>
    
    <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script src="../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="shortcut icon" href="../_static/project_ico.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="scikit-learn Converters and Benchmarks" href="../onnx_bench.html" />
    <link rel="prev" title="ONNX Backends for Python/Numpy runtime" href="backend_python.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../installation.html">
  Installation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorial/index.html">
  Tutorial
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/index.html">
  API
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../onnx.html">
  ONNX, Runtime, Backends
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../onnx_bench.html">
  scikit-learn Converters and Benchmarks
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_cmd.html">
  Command lines
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_ex.html">
  Examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_index.html">
  FAQ, code, …
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../gyexamples/index.html">
  Gallery of examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../all_notebooks.html">
  Notebook Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../HISTORY.html">
  History
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_runtime.html">
   Runtimes for ONNX
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   ONNX Backends
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="backend_python.html">
     ONNX Backends for Python/Numpy runtime
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     ONNX Backends for onnxruntime1
    </a>
   </li>
  </ul>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="onnx-backends-for-onnxruntime1">
<h1>ONNX Backends for onnxruntime1<a class="headerlink" href="#onnx-backends-for-onnxruntime1" title="Permalink to this headline">¶</a></h1>
<p>Backend class: <code class="xref py py-class docutils literal notranslate"><span class="pre">OnnxInferenceBackendOrt</span></code>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">redirect_stdout</span><span class="p">,</span> <span class="n">redirect_stderr</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="kn">from</span> <span class="nn">onnx.backend.test</span> <span class="kn">import</span> <span class="n">BackendTest</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">onnx_version</span>
<span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">ort_version</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">npy_version</span>
<span class="kn">import</span> <span class="nn">mlprodict.onnxrt.backend_ort</span> <span class="k">as</span> <span class="nn">backend</span>

<span class="n">back_test</span> <span class="o">=</span> <span class="n">BackendTest</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">include</span><span class="p">(</span><span class="s1">&#39;.*_cpu&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_blvc_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_densenet_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_densenet121_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_inception_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_resnet50_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_shufflenet_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_squeezenet_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_vgg19_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_zfnet512_.*&#39;</span><span class="p">)</span>
<span class="nb">globals</span><span class="p">()</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">back_test</span><span class="o">.</span><span class="n">enable_report</span><span class="p">()</span><span class="o">.</span><span class="n">test_cases</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;python&#39;</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;onnx&#39;</span><span class="p">,</span> <span class="n">onnx_version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;onnxruntime&#39;</span><span class="p">,</span> <span class="n">ort_version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="n">npy_version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">(),</span> <span class="s2">&quot;BEGIN&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>

<span class="n">buffer</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span>
<span class="k">if</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">redirect_stdout</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">redirect_stderr</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">exit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">exit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">testsRun</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">testsRun</span>
<span class="n">errors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
<span class="n">skipped</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">skipped</span><span class="p">)</span>
<span class="n">unexpectedSuccesses</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">unexpectedSuccesses</span><span class="p">)</span>
<span class="n">expectedFailures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">expectedFailures</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">(),</span> <span class="s2">&quot;END&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;testsRun=</span><span class="si">%d</span><span class="s2"> errors=</span><span class="si">%d</span><span class="s2"> skipped=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">testsRun</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="n">skipped</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;unexpectedSuccesses=</span><span class="si">%d</span><span class="s2"> expectedFailures=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
    <span class="n">unexpectedSuccesses</span><span class="p">,</span> <span class="n">expectedFailures</span><span class="p">))</span>
<span class="n">ran</span> <span class="o">=</span> <span class="n">testsRun</span> <span class="o">-</span> <span class="n">skipped</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ratio=</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">errors</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">ran</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    ---------------------------------
    python 3.9.1 (default, Jan 18 2021, 16:35:58) 
    [GCC 8.3.0]
    onnx 1.11.0
    onnxruntime 1.10.91
    numpy 1.21.5
    ---------------------------------
    2022-03-10 10:30:34.741236 BEGIN
    ---------------------------------
    ---------------------------------
    2022-03-10 10:31:21.401150 END
    ---------------------------------
    testsRun=2026 errors=131 skipped=1021
    unexpectedSuccesses=0 expectedFailures=0
    ratio=0.869652
    ---------------------------------
    test_abs_cpu (__main__.OnnxBackendNodeModelTest) ... /usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py:188: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      if ref_outputs[i].dtype == np.object:
    ok
    test_abs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acos_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acos_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acosh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acosh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acosh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acosh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adagrad_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adagrad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adagrad_multiple_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adagrad_multiple_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adam_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adam_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adam_multiple_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adam_multiple_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_add_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_add_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_add_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_add_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_add_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_add_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast3v1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast3v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast4v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast4v3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast4v4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_negative_axis_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_negative_axis_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_negative_axis_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_negative_axis_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_negative_axis_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_negative_axis_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_negative_axis_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_negative_axis_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asin_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asin_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asin_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asin_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asinh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asinh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asinh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asinh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atan_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atan_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atan_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atan_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atanh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atanh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atanh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atanh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_1d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_1d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_ceil_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_ceil_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_pads_count_include_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_pads_count_include_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_pads_count_include_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_pads_count_include_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_same_lower_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_same_lower_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_3d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_3d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_basic_conv_with_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_basic_conv_with_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_basic_conv_without_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_basic_conv_without_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_basic_convinteger_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_basic_convinteger_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_epsilon_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_batchnorm_epsilon_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_epsilon_training_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_batchnorm_epsilon_training_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_batchnorm_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_example_training_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_batchnorm_example_training_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_bernoulli_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_double_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_bernoulli_double_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_double_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_bernoulli_double_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_bernoulli_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_seed_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_bernoulli_seed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_seed_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_bernoulli_seed_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_left_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_bitshift_left_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_bitshift_left_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_bitshift_left_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_right_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_bitshift_right_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_bitshift_right_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_bitshift_right_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cast_BFLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_DOUBLE_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_DOUBLE_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_DOUBLE_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_DOUBLE_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT16_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT16_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cast_FLOAT_to_BFLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_cast_FLOAT_to_STRING_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_STRING_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_STRING_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_BFLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_BFLOAT16_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_BFLOAT16_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT16_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_DOUBLE_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_DOUBLE_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_FLOAT_to_BFLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_BFLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_FLOAT_to_BFLOAT16_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_DOUBLE_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_DOUBLE_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_FLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_FLOAT16_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_castlike_FLOAT_to_STRING_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_STRING_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_castlike_FLOAT_to_STRING_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_STRING_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_STRING_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_STRING_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_STRING_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ceil_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_ceil_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ceil_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_ceil_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_celu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_celu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_celu_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_celu_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_inbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_inbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_int8_inbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_int8_inbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_int8_max_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_int8_max_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_int8_min_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_default_int8_min_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_max_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_max_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_min_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_default_min_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_inbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_inbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_outbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_outbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_splitbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_splitbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_1d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_1d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_1d_axis_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_1d_axis_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_negative_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_negative_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_negative_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_negative_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_negative_3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_negative_3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constant_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_constant_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constant_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constant_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constantofshape_float_ones_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constantofshape_float_ones_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constantofshape_int_shape_zero_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_constantofshape_int_shape_zero_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constantofshape_int_zeros_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constantofshape_int_zeros_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_autopad_same_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_autopad_same_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_strides_and_asymmetric_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_strides_and_asymmetric_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_strides_no_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_strides_no_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_strides_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_strides_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convinteger_with_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convinteger_with_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convinteger_without_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convinteger_without_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_autopad_same_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_convtranspose_autopad_same_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_dilations_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_dilations_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_kernel_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_kernel_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_output_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_output_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_with_kernel_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_with_kernel_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cos_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cos_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cosh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cosh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cosh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cosh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_exclusive_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_1d_exclusive_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_reverse_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_1d_reverse_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_reverse_exclusive_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_1d_reverse_exclusive_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_2d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_2d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_2d_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_2d_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_2d_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_2d_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_crd_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_depthtospace_crd_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_crd_mode_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_depthtospace_crd_mode_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_dcr_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_depthtospace_dcr_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_depthtospace_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dequantizelinear_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dequantizelinear_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dequantizelinear_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dequantizelinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_det_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_det_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_det_nd_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_det_nd_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_div_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_mask_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_mask_ratio_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_mask_ratio_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_old_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_old_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_ratio_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_ratio_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_random_old_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_random_old_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dynamicquantizelinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dynamicquantizelinear_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_max_adjusted_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dynamicquantizelinear_max_adjusted_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_max_adjusted_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dynamicquantizelinear_max_adjusted_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_min_adjusted_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dynamicquantizelinear_min_adjusted_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_min_adjusted_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dynamicquantizelinear_min_adjusted_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_edge_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_edge_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_batch_diagonal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_batch_diagonal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_batch_matmul_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_batch_matmul_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_inner_prod_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_inner_prod_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_transpose_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_transpose_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_elu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_elu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_elu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_elu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_elu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_elu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_equal_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_equal_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_equal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_equal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_erf_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_erf_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_exp_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_exp_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_exp_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_exp_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_dim_changed_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_expand_dim_changed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_dim_unchanged_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_expand_dim_unchanged_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_eyelike_populate_off_main_diagonal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_eyelike_populate_off_main_diagonal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_eyelike_with_dtype_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_eyelike_with_dtype_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_eyelike_without_dtype_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_eyelike_without_dtype_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis4_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis4_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_floor_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_floor_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_floor_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_floor_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_2d_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_2d_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_elements_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_elements_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_elements_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_elements_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_elements_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_elements_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gathernd_example_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gathernd_example_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gathernd_example_int32_batch_dim1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gathernd_example_int32_batch_dim1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gathernd_example_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gathernd_example_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_all_attributes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_all_attributes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_alpha_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_alpha_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_beta_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_beta_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_matrix_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_matrix_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_no_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_no_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_scalar_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_scalar_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_single_elem_vector_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_single_elem_vector_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_vector_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_vector_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_zero_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_zero_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_transposeA_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_transposeA_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_transposeB_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_transposeB_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalaveragepool_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_globalaveragepool_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalaveragepool_precomputed_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_globalaveragepool_precomputed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalmaxpool_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_globalmaxpool_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalmaxpool_precomputed_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_globalmaxpool_precomputed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_bcast_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_bcast_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_aligncorners_true_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_aligncorners_true_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_bicubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_bicubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_bilinear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_bilinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_border_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_border_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_reflection_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_reflection_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_zeros_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_zeros_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_batchwise_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gru_batchwise_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_defaults_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gru_defaults_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_seq_length_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gru_seq_length_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gru_with_initial_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_one_hot_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardmax_one_hot_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardsigmoid_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardsigmoid_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardsigmoid_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardsigmoid_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardsigmoid_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardsigmoid_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardswish_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardswish_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardswish_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_hardswish_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_identity_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_identity_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_identity_opt_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_identity_opt_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_identity_sequence_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_identity_sequence_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_if_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_if_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_if_opt_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_if_opt_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_if_seq_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_if_seq_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_instancenorm_epsilon_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_instancenorm_epsilon_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_instancenorm_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_instancenorm_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isinf_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_isinf_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isinf_negative_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_isinf_negative_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isinf_positive_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_isinf_positive_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isnan_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_isnan_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_leakyrelu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_leakyrelu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_leakyrelu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_leakyrelu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_leakyrelu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_leakyrelu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_bcast_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_bcast_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_log_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_log_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_0_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_0_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_2_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_2_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_default_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_default_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_example_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_example_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_example_1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_example_1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_large_number_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_large_number_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_large_number_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_large_number_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_negative_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_negative_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_loop11_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_loop11_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_loop13_seq_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_loop13_seq_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_loop16_seq_none_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_loop16_seq_none_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lrn_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_lrn_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lrn_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_lrn_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_batchwise_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lstm_batchwise_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_defaults_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_lstm_defaults_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_lstm_with_initial_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_with_peepholes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_lstm_with_peepholes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmul_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmul_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmul_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmul_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmul_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmul_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmulinteger_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmulinteger_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_float16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_float16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_float64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_float64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_max_int16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_max_int8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_max_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_max_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_1d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_1d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_ceil_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_ceil_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_dilations_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_dilations_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_precomputed_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_precomputed_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_precomputed_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_precomputed_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_precomputed_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_precomputed_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_same_lower_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_same_lower_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_3d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_3d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_with_argmax_2d_precomputed_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_with_argmax_2d_precomputed_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_with_argmax_2d_precomputed_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_with_argmax_2d_precomputed_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxunpool_export_with_output_shape_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_maxunpool_export_with_output_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxunpool_export_without_output_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxunpool_export_without_output_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mean_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mean_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mean_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mean_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mean_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mean_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_float16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_float16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_float64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_float64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_min_int16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_min_int8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_min_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_min_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_broadcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_broadcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_int64_fmod_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_int64_fmod_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_float16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_float16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_float64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_float64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_momentum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_momentum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_momentum_multiple_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_momentum_multiple_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_mul_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mvn_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mvn_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mvn_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mvn_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_neg_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_neg_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nesterov_momentum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nesterov_momentum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NC_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NC_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NC_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NC_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_mean_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_mean_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_mean_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_mean_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_weight_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_weight_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_mean_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_reduction_mean_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_reduction_mean_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_reduction_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_reduction_sum_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_mean_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_mean_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_mean_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_sum_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_sum_weight_high_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3_sum_weight_high_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_sum_weight_high_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3_sum_weight_high_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_mean_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3d4d5_mean_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3d4d5_mean_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_none_no_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3d4d5_none_no_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_none_no_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3d4d5_none_no_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_center_point_box_format_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_center_point_box_format_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_flipped_coordinates_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_flipped_coordinates_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_identical_boxes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_identical_boxes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_limit_output_size_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_limit_output_size_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_single_box_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_single_box_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_suppress_by_IOU_and_scores_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_suppress_by_IOU_and_scores_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_suppress_by_IOU_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_suppress_by_IOU_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_two_batches_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_two_batches_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_two_classes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonmaxsuppression_two_classes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonzero_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nonzero_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_not_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_not_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_not_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_not_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_not_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_not_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_onehot_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_onehot_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_with_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_onehot_with_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_onehot_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_get_element_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_get_element_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_get_element_sequence_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_get_element_sequence_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_has_element_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_has_element_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_has_element_empty_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_has_element_empty_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast3v1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast3v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast4v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast4v3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast4v4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_bcast_array_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_bcast_array_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_bcast_scalar_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_bcast_scalar_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float32_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float32_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_pow_types_float32_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_pow_types_float32_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int32_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int32_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int32_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int32_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int64_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int64_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int64_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int64_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_prelu_broadcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_prelu_broadcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_prelu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_prelu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_qlinearconv_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_qlinearconv_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_qlinearmatmul_2D_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_qlinearmatmul_2D_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_qlinearmatmul_3D_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_qlinearmatmul_3D_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_quantizelinear_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_quantizelinear_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_quantizelinear_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_quantizelinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_float_type_positive_delta_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_range_float_type_positive_delta_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_float_type_positive_delta_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_range_float_type_positive_delta_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_int32_type_negative_delta_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_range_int32_type_negative_delta_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_int32_type_negative_delta_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_range_int32_type_negative_delta_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reciprocal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reciprocal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reciprocal_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reciprocal_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_negative_axes_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_negative_axes_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_negative_axes_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_negative_axes_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_negative_axes_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_negative_axes_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_negative_axes_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_negative_axes_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_asc_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_asc_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_desc_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_desc_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_default_axes_keepdim_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_default_axes_keepdim_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_empty_axes_input_noop_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_empty_axes_input_noop_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_empty_axes_input_noop_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_empty_axes_input_noop_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reflect_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reflect_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_relu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_relu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_allowzero_reordered_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reshape_allowzero_reordered_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_extended_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_extended_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_negative_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_negative_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_negative_extended_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_negative_extended_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_one_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_one_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_reduced_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_reduced_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_reordered_all_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_reordered_all_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_reordered_last_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_reordered_last_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_zero_and_negative_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_zero_and_negative_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_zero_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_zero_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_cubic_A_n0p5_exclude_outside_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_cubic_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_linear_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_linear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_linear_pytorch_half_pixel_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_linear_pytorch_half_pixel_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_tf_crop_and_resize_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_tf_crop_and_resize_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_A_n0p5_exclude_outside_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_asymmetric_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_asymmetric_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_linear_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_linear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_ceil_half_pixel_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_ceil_half_pixel_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_floor_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_floor_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reversesequence_batch_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reversesequence_batch_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reversesequence_time_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reversesequence_time_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_rnn_seq_length_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_rnn_seq_length_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_roialign_aligned_false_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_roialign_aligned_false_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_roialign_aligned_true_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_roialign_aligned_true_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_round_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_round_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scan9_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scan9_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scan_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scan_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_elements_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_with_duplicate_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatter_elements_with_duplicate_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_with_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_elements_with_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_elements_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatternd_add_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatternd_add_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatternd_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatternd_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatternd_multiply_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatternd_multiply_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1_mean_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1_mean_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1_mean_weight_negative_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1_mean_weight_negative_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_none_no_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_none_no_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_sum_weight_high_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_sum_weight_high_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_mean_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_mean_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_none_no_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_none_no_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_3d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_3d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_3d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_3d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_3d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_3d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_4d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_4d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_4d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_no_weight_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_3d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_3d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_3d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_4d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_4d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_4d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_mean_weight_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_weights_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_weights_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_weights_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_none_weights_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_sum_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_sum_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sce_sum_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_selu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_selu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_selu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_selu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_selu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_selu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_insert_at_back_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sequence_insert_at_back_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_insert_at_front_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sequence_insert_at_front_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_clip_end_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_clip_end_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_clip_start_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_clip_start_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_end_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_end_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_end_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_end_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_start_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_1_end_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_start_1_end_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_1_end_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_start_1_end_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_start_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shrink_hard_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shrink_hard_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shrink_soft_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shrink_soft_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sigmoid_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sigmoid_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sigmoid_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sigmoid_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sign_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sign_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_simple_rnn_batchwise_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_simple_rnn_batchwise_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_simple_rnn_defaults_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_simple_rnn_defaults_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_simple_rnn_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_simple_rnn_with_initial_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sin_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sin_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sin_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sin_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sinh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sinh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sinh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sinh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_size_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_size_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_size_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_size_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_default_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_default_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_default_steps_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_default_steps_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_end_out_of_bounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_end_out_of_bounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_neg_steps_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_neg_steps_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_start_out_of_bounds_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_slice_start_out_of_bounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_0_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_0_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_2_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_2_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_default_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_default_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_example_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_example_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_large_number_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_large_number_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_large_number_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_large_number_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_negative_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_negative_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softplus_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softplus_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softplus_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softplus_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softsign_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softsign_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softsign_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softsign_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_spacetodepth_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_spacetodepth_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_spacetodepth_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_spacetodepth_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_equal_parts_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_equal_parts_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_equal_parts_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_equal_parts_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_equal_parts_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_equal_parts_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_variable_parts_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_variable_parts_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_variable_parts_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_variable_parts_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_variable_parts_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_variable_parts_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_zero_size_splits_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_split_zero_size_splits_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sqrt_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sqrt_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sqrt_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sqrt_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_squeeze_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_squeeze_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_squeeze_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_squeeze_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_casesensintive_lower_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_export_monday_casesensintive_lower_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_casesensintive_nochangecase_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_export_monday_casesensintive_nochangecase_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_casesensintive_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_export_monday_casesensintive_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_empty_output_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_export_monday_empty_output_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_insensintive_upper_twodim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_export_monday_insensintive_upper_twodim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_nostopwords_nochangecase_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_nostopwords_nochangecase_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sub_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sum_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sum_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sum_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sum_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sum_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sum_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tan_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tan_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tan_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tan_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tanh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tanh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tanh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tanh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_batch_onlybigrams_skip0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_batch_onlybigrams_skip0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_batch_onlybigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_batch_onlybigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_batch_uniandbigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_batch_uniandbigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_only_bigrams_skip0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_only_bigrams_skip0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_onlybigrams_levelempty_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_onlybigrams_levelempty_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_onlybigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_onlybigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_uniandbigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_uniandbigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_thresholdedrelu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_thresholdedrelu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_thresholdedrelu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_thresholdedrelu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_thresholdedrelu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_thresholdedrelu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tile_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tile_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tile_precomputed_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tile_precomputed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_top_k_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_top_k_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_top_k_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_top_k_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_top_k_smallest_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_top_k_smallest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_training_dropout_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_default_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_training_dropout_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_default_mask_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_training_dropout_default_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_mask_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_training_dropout_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_zero_ratio_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_zero_ratio_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_zero_ratio_mask_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_zero_ratio_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_4_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_4_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_one_row_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_one_row_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_out_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_out_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_out_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_out_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_square_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_square_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_square_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tril_square_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_zero_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_zero_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_one_row_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_one_row_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_out_neg_out_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_out_neg_out_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_out_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_out_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_square_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_square_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_square_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_triu_square_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_zero_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_zero_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_not_sorted_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unique_not_sorted_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_with_axis_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unique_sorted_with_axis_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unique_sorted_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_with_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unique_sorted_with_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unique_sorted_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_three_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_three_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_two_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_two_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_unsorted_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_unsorted_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_upsample_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_upsample_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_where_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_where_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_where_long_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_where_long_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor_bcast3v1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor_bcast3v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor_bcast4v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor_bcast4v3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_xor_bcast4v4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool1d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool2d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool2d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool3d_stride1_pad0_gpu_input_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool3d_stride1_pad0_gpu_input_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_AvgPool3d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm1d_3d_input_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_BatchNorm1d_3d_input_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm2d_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_BatchNorm2d_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm2d_momentum_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_BatchNorm2d_momentum_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm3d_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_BatchNorm3d_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm3d_momentum_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_BatchNorm3d_momentum_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ConstantPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ConstantPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_dilated_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_dilated_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_groups_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_groups_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad1_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad1_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad1size1_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad1size1_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad2_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad2_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad2size1_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad2size1_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_padded_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_padded_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_strided_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_strided_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_with_multiplier_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_with_multiplier_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_dilated_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_dilated_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_groups_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_groups_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_groups_thnn_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_groups_thnn_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_padding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_padding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_strided_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_strided_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_dilated_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_dilated_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_dilated_strided_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_dilated_strided_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_groups_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_groups_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_stride_padding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_stride_padding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ConvTranspose2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ConvTranspose2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ConvTranspose2d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ConvTranspose2d_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ELU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ELU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Embedding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Embedding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Embedding_sparse_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Embedding_sparse_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_GLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_GLU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_GLU_dim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_GLU_dim_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_LeakyReLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_LeakyReLU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_LeakyReLU_with_negval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_LeakyReLU_with_negval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Linear_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_Linear_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Linear_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Linear_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_LogSoftmax_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_LogSoftmax_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool1d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool1d_stride_padding_dilation_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool1d_stride_padding_dilation_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool2d_stride_padding_dilation_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool2d_stride_padding_dilation_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool3d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool3d_stride_padding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool3d_stride_padding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_1d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_1d_multiparam_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_2d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_2d_multiparam_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_3d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_3d_multiparam_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PixelShuffle_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_PixelShuffle_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PoissonNLLLLoss_no_reduce_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PoissonNLLLLoss_no_reduce_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ReLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ReLU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ReflectionPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ReflectionPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ReplicationPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ReplicationPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_SELU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_SELU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Sigmoid_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Sigmoid_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softmax_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Softmax_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softmin_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Softmin_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softplus_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Softplus_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softsign_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_Softsign_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Tanh_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Tanh_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ZeroPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ZeroPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_softmax_dim3_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_log_softmax_dim3_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_softmax_lastdim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_log_softmax_lastdim_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_functional_dim3_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_softmax_functional_dim3_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_lastdim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_softmax_lastdim_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_add_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_size1_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_add_size1_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_size1_right_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_add_size1_right_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_size1_singleton_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_add_size1_singleton_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_addconstant_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_addconstant_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_addmm_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_addmm_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_basic_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_basic_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_chunk_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_chunk_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_clip_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_clip_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_concat2_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_concat2_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_conv_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_conv_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_convtranspose_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_convtranspose_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_exp_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_exp_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_flatten_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_flatten_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_index_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_index_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_max_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_max_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_maxpool_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_maxpool_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_min_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_min_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_mm_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_mm_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_non_float_params_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_non_float_params_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_pad_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_pad_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_params_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_params_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_permute2_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_permute2_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_pow_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_pow_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_mean_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_mean_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_mean_keepdim_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_mean_keepdim_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_sum_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_sum_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_sum_keepdim_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_sum_keepdim_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_repeat_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_repeat_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_repeat_dim_overflow_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_repeat_dim_overflow_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_selu_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_selu_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_sqrt_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_sqrt_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_symbolic_override_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_symbolic_override_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_symbolic_override_nested_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_symbolic_override_nested_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_view_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_view_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bvlc_alexnet_cpu (__main__.OnnxBackendRealModelTest) ... ok
    test_bvlc_alexnet_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;no matched include pattern&#39;
    test_densenet121_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_densenet121_.*&quot;&#39;
    test_densenet121_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_densenet121_.*&quot;&#39;
    test_inception_v1_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_inception_v1_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_inception_v2_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_inception_v2_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_resnet50_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_resnet50_.*&quot;&#39;
    test_resnet50_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_resnet50_.*&quot;&#39;
    test_shufflenet_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_shufflenet_.*&quot;&#39;
    test_shufflenet_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_shufflenet_.*&quot;&#39;
    test_squeezenet_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_squeezenet_.*&quot;&#39;
    test_squeezenet_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_squeezenet_.*&quot;&#39;
    test_vgg19_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_vgg19_.*&quot;&#39;
    test_vgg19_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_vgg19_.*&quot;&#39;
    test_zfnet512_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_zfnet512_.*&quot;&#39;
    test_zfnet512_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_zfnet512_.*&quot;&#39;
    test_expand_shape_model1_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model1_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_shape_model2_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model2_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_shape_model3_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model3_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_shape_model4_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model4_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gradient_of_add_and_mul_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_gradient_of_add_and_mul_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gradient_of_add_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_gradient_of_add_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model1_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model1_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model2_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model2_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model3_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model3_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model4_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model4_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model5_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model5_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model6_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model6_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model7_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model7_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model8_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model8_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shrink_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_shrink_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sign_model_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sign_model_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_single_relu_model_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_single_relu_model_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_casesensintive_lower_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_monday_casesensintive_lower_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_casesensintive_nochangecase_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_monday_casesensintive_nochangecase_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_casesensintive_upper_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_monday_casesensintive_upper_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_empty_output_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_monday_empty_output_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_insensintive_upper_twodim_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_monday_insensintive_upper_twodim_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_nostopwords_nochangecase_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_nostopwords_nochangecase_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    
    ======================================================================
    ERROR: test_adagrad_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Adagrad is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Adagrad is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X&quot;
        input: &quot;G&quot;
        input: &quot;H&quot;
        output: &quot;X_new&quot;
        output: &quot;H_new&quot;
        op_type: &quot;Adagrad&quot;
        attribute {
          name: &quot;decay_factor&quot;
          f: 0.10000000149011612
          type: FLOAT
        }
        attribute {
          name: &quot;epsilon&quot;
          f: 9.999999747378752e-06
          type: FLOAT
        }
        attribute {
          name: &quot;norm_coefficient&quot;
          f: 0.0010000000474974513
          type: FLOAT
        }
        domain
    [...]
        dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;X_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;H_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_adagrad_multiple_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Adagrad is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Adagrad is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X1&quot;
        input: &quot;X2&quot;
        input: &quot;G1&quot;
        input: &quot;G2&quot;
        input: &quot;H1&quot;
        input: &quot;H2&quot;
        output: &quot;X1_new&quot;
        output: &quot;X2_new&quot;
        output: &quot;H1_new&quot;
        output: &quot;H2_new&quot;
        op_type: &quot;Adagrad&quot;
        attribute {
          name: &quot;decay_factor&quot;
          f: 0.10000000149011612
          type: FLOAT
        }
        attribute {
          name: &quot;epsilon&quot;
          f: 9.999999747378752e-06
          type: FLOAT
        }
        attribute {
     
    [...]
      dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;H1_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;H2_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_adam_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Adam is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Adam is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X&quot;
        input: &quot;G&quot;
        input: &quot;V&quot;
        input: &quot;H&quot;
        output: &quot;X_new&quot;
        output: &quot;V_new&quot;
        output: &quot;H_new&quot;
        op_type: &quot;Adam&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 0.949999988079071
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 0.10000000149011612
          type: FLOAT
        }
        attribute {
          name: &quot;epsilon&quot;
          f: 1.0000000116860974e-07
          type: FLOAT
        }
    
    [...]
        dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;V_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;H_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_adam_multiple_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Adam is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Adam is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X1&quot;
        input: &quot;X2&quot;
        input: &quot;G1&quot;
        input: &quot;G2&quot;
        input: &quot;V1&quot;
        input: &quot;V2&quot;
        input: &quot;H1&quot;
        input: &quot;H2&quot;
        output: &quot;X1_new&quot;
        output: &quot;X2_new&quot;
        output: &quot;V1_new&quot;
        output: &quot;V2_new&quot;
        output: &quot;H1_new&quot;
        output: &quot;H2_new&quot;
        op_type: &quot;Adam&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 0.949999988079071
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 0.85
    [...]
      dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;H1_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;H2_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_add_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(14) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(14) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;sum&quot;
        op_type: &quot;Add&quot;
      }
      name: &quot;test_add_uint8&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            e
    [...]
            dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;sum&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_bitshift_left_uint16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BitShift(11) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BitShift(11) node with name &#39;&#39;&#39;
    ir_version: 5
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;BitShift&quot;
        attribute {
          name: &quot;direction&quot;
          s: &quot;LEFT&quot;
          type: STRING
        }
      }
      name: &quot;test_bitshift_left_uint16&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 11
    }
    .
    
    ======================================================================
    ERROR: test_bitshift_right_uint16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BitShift(11) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BitShift(11) node with name &#39;&#39;&#39;
    ir_version: 5
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;BitShift&quot;
        attribute {
          name: &quot;direction&quot;
          s: &quot;RIGHT&quot;
          type: STRING
        }
      }
      name: &quot;test_bitshift_right_uint16&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 11
    }
    .
    
    ======================================================================
    ERROR: test_cast_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_cast_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 583, in to_sequence
        outputs[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_BFLOAT16_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_FLOAT_to_BFLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_clip_default_inbounds_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , ) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , ) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;float32&#39;) shape=(3,)
    Clip(x, , ) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;float32&#39;) shape=(3,)
    ---
    
    
    ======================================================================
    ERROR: test_clip_default_int8_inbounds_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , ) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , ) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;int8&#39;) shape=(3,)
    Clip(x, , ) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;int8&#39;) shape=(3,)
    ---
    
    
    ======================================================================
    ERROR: test_clip_default_int8_max_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;max&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;max&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (&#39;?&#39;,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , max) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    ((&#39;max&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , max) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;int8&#39;) shape=(3, 4, 5)
    input: name=&#39;max&#39; type=dtype(&#39;int8&#39;) shape=()
    Clip(x, , max) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;int8&#39;) shape=(3, 4, 5)
    ---
    
    
    ======================================================================
    ERROR: test_clip_default_max_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;max&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;max&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (&#39;?&#39;,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , max) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    ((&#39;max&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , max) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;float32&#39;) shape=(3, 4, 5)
    input: name=&#39;max&#39; type=dtype(&#39;float32&#39;) shape=()
    Clip(x, , max) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;float32&#39;) shape=(3, 4, 5)
    ---
    
    
    ======================================================================
    ERROR: test_constant_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1232, in _run_whole_runtime
        res = self._whole.run(inputs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 105, in run
        v = next(iter(inputs.values()))
    StopIteration
    
    ======================================================================
    ERROR: test_constantofshape_int_shape_zero_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;y&quot;
    type {
      tensor_type {
        elem_type: 6
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_div_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Div(14) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Div(14) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;Div&quot;
      }
      name: &quot;test_div_uint8&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            ele
    [...]
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_aligncorners_true_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;align_corners&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;mode&quot;
          s: &quot;bilinear&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_aligncorners_true&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim 
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_bicubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;mode&quot;
          s: &quot;bicubic&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_bicubic&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 3
            
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_bilinear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;mode&quot;
          s: &quot;bilinear&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_bilinear&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 3
          
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_border_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;padding_mode&quot;
          s: &quot;border&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_border_padding&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_val
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;align_corners&quot;
          i: 0
          type: INT
        }
        attribute {
          name: &quot;mode&quot;
          s: &quot;bilinear&quot;
          type: STRING
        }
        attribute {
          name: &quot;padding_mode&quot;
          s: &quot;zeros&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
    
    [...]
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;mode&quot;
          s: &quot;nearest&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_nearest&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 3
            
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_reflection_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;padding_mode&quot;
          s: &quot;reflection&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_reflection_padding&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
               
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gridsample_zeros_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;Grid&quot;
        output: &quot;Y&quot;
        op_type: &quot;GridSample&quot;
        attribute {
          name: &quot;padding_mode&quot;
          s: &quot;zeros&quot;
          type: STRING
        }
      }
      name: &quot;test_gridsample_zeros_padding&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value
    [...]
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_gru_batchwise_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/deep_cpu_gru.h:55 onnxruntime::DeepCpuGruOp::DeepCpuGruOp(const onnxruntime::OpKernelInfo&amp;) layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/deep_cpu_gru.h:55 onnxruntime::DeepCpuGruOp::DeepCpuGruOp(const onnxruntime::OpKernelInfo&amp;) layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    &#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;W&quot;
        input: &quot;R&quot;
        output: &quot;Y&quot;
        output: &quot;Y_h&quot;
        op_type: &quot;GRU&quot;
        attribute {
          name: &quot;hidden_size&quot;
          i: 6
          type: INT
        }
        attribute {
          name: &quot;layout&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;test_gru_batchwise&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
      
    [...]
            dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
      output {
        name: &quot;Y_h&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_identity_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        output: &quot;y&quot;
        op_type: &quot;Identity&quot;
      }
      name: &quot;test_identity&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_identity_opt_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;opt_in&quot;
        output: &quot;opt_out&quot;
        op_type: &quot;Identity&quot;
      }
      name: &quot;test_identity_opt&quot;
      input {
        name: &quot;opt_in&quot;
        type {
          optional_type {
            elem_type {
              sequence_type {
                elem_type {
                  tensor_type {
                    elem_type: 1
                    shape {
                      dim {
                        dim_value: 5
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
      output {
        name: &quot;opt_out&quot;
        type {
          optional_type {
            elem_type {
              sequence_type {
                elem_type {
                  tensor_type {
                    elem_type: 1
                    shape {
                      dim {
                        dim_value: 5
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;&quot;
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_identity_sequence_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        output: &quot;y&quot;
        op_type: &quot;Identity&quot;
      }
      name: &quot;test_identity_sequence&quot;
      input {
        name: &quot;x&quot;
        type {
          sequence_type {
            elem_type {
              tensor_type {
                elem_type: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;y&quot;
        type {
          sequence_type {
            elem_type {
              tensor_type {
                elem_type: 1
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_if_opt_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;cond&quot;
        output: &quot;sequence&quot;
        op_type: &quot;If&quot;
        attribute {
          name: &quot;else_branch&quot;
          g {
            node {
              output: &quot;x&quot;
              op_type: &quot;Constant&quot;
              attribute {
                name: &quot;value&quot;
                t {
                  dims: 5
                  data_type: 1
                  raw_data: &quot;\000\000\200?\000\000\000@\000\000@@\000\000\200@\000\000\240@&quot;
                }
                type: TENSOR
              }
            
    [...]
    _type {
            elem_type: 9
            shape {
            }
          }
        }
      }
      output {
        name: &quot;sequence&quot;
        type {
          optional_type {
            elem_type {
              sequence_type {
                elem_type {
                  tensor_type {
                    elem_type: 1
                    shape {
                      dim {
                        dim_value: 5
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;&quot;
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_loop16_seq_none_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;trip_count&quot;
        input: &quot;cond&quot;
        input: &quot;opt_seq&quot;
        output: &quot;seq_res&quot;
        op_type: &quot;Loop&quot;
        attribute {
          name: &quot;body&quot;
          g {
            node {
              input: &quot;cond_in&quot;
              output: &quot;cond_out&quot;
              op_type: &quot;Identity&quot;
            }
            node {
              input: &quot;opt_seq_in&quot;
              output: &quot;optional_has_elem&quot;
              op_type: &quot;OptionalHasElement&quot;
            }
            node {
              input: &quot;optional_has
    [...]
    opt_seq&quot;
        type {
          optional_type {
            elem_type {
              sequence_type {
                elem_type {
                  tensor_type {
                    elem_type: 1
                    shape {
                    }
                  }
                }
              }
            }
          }
        }
      }
      output {
        name: &quot;seq_res&quot;
        type {
          sequence_type {
            elem_type {
              tensor_type {
                elem_type: 1
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;&quot;
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_lstm_batchwise_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/lstm_base.h:52 onnxruntime::LSTMBase::LSTMBase(const onnxruntime::OpKernelInfo&amp;) layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/lstm_base.h:52 onnxruntime::LSTMBase::LSTMBase(const onnxruntime::OpKernelInfo&amp;) layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    &#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;W&quot;
        input: &quot;R&quot;
        output: &quot;Y&quot;
        output: &quot;Y_h&quot;
        op_type: &quot;LSTM&quot;
        attribute {
          name: &quot;hidden_size&quot;
          i: 7
          type: INT
        }
        attribute {
          name: &quot;layout&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;test_lstm_batchwise&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
    
    [...]
            dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 7
              }
            }
          }
        }
      }
      output {
        name: &quot;Y_h&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 7
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_max_int16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Max&quot;
      }
      name: &quot;test_max_int16&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 5
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 5
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 5
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_max_int8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Max&quot;
      }
      name: &quot;test_max_int8&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 3
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 3
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 3
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_max_uint16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Max&quot;
      }
      name: &quot;test_max_uint16&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_max_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Max(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Max&quot;
      }
      name: &quot;test_max_uint8&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_min_int16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Min&quot;
      }
      name: &quot;test_min_int16&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 5
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 5
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 5
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_min_int8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Min&quot;
      }
      name: &quot;test_min_int8&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 3
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 3
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 3
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_min_uint16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Min&quot;
      }
      name: &quot;test_min_uint16&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 4
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_min_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Min(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data_0&quot;
        input: &quot;data_1&quot;
        output: &quot;result&quot;
        op_type: &quot;Min&quot;
      }
      name: &quot;test_min_uint8&quot;
      input {
        name: &quot;data_0&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;data_1&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;result&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_momentum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Momentum is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Momentum is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X&quot;
        input: &quot;G&quot;
        input: &quot;V&quot;
        output: &quot;X_new&quot;
        output: &quot;V_new&quot;
        op_type: &quot;Momentum&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 0.949999988079071
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 0.10000000149011612
          type: FLOAT
        }
        attribute {
          name: &quot;mode&quot;
          s: &quot;standard&quot;
          type: STRING
        }
        attribute {
          name: &quot;norm_coefficient
    [...]
        dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;X_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;V_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_momentum_multiple_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Momentum is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Momentum is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X1&quot;
        input: &quot;X2&quot;
        input: &quot;G1&quot;
        input: &quot;G2&quot;
        input: &quot;H1&quot;
        input: &quot;H2&quot;
        output: &quot;X1_new&quot;
        output: &quot;X2_new&quot;
        output: &quot;V1_new&quot;
        output: &quot;V2_new&quot;
        op_type: &quot;Momentum&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 0.949999988079071
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 0.8500000238418579
          type: FLOAT
        }
        attribute {
          name: &quot;mo
    [...]
      dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;V1_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;V2_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_mul_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(14) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(14) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;Mul&quot;
      }
      name: &quot;test_mul_uint8&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            ele
    [...]
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_nesterov_momentum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Momentum is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Momentum is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;R&quot;
        input: &quot;T&quot;
        input: &quot;X&quot;
        input: &quot;G&quot;
        input: &quot;V&quot;
        output: &quot;X_new&quot;
        output: &quot;V_new&quot;
        op_type: &quot;Momentum&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 0.949999988079071
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 1.0
          type: FLOAT
        }
        attribute {
          name: &quot;mode&quot;
          s: &quot;nesterov&quot;
          type: STRING
        }
        attribute {
          name: &quot;norm_coefficient&quot;
          f: 0.009
    [...]
        dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;X_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;V_new&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_optional_get_element_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(tensor(float)) is not currently registered or supported
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(tensor(float)) is not currently registered or supported&#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;optional_input&quot;
        output: &quot;output&quot;
        op_type: &quot;OptionalGetElement&quot;
      }
      name: &quot;test_optional_get_element&quot;
      input {
        name: &quot;optional_input&quot;
        type {
          optional_type {
            elem_type {
              tensor_type {
                elem_type: 1
                shape {
                  dim {
                    dim_value: 4
                  }
                }
              }
            }
          }
        }
      }
      output {
        name: &quot;output&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 15
    }
    .
    
    ======================================================================
    ERROR: test_optional_get_element_sequence_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(seq(tensor(int32))) is not currently registered or supported
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(seq(tensor(int32))) is not currently registered or supported&#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;optional_input&quot;
        output: &quot;output&quot;
        op_type: &quot;OptionalGetElement&quot;
      }
      name: &quot;test_optional_get_element_sequence&quot;
      input {
        name: &quot;optional_input&quot;
        type {
          optional_type {
            elem_type {
              sequence_type {
                elem_type {
                  tensor_type {
                    elem_type: 6
                    shape {
                      dim {
                        dim_value: 4
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
      output {
        name: &quot;output&quot;
        type {
          sequence_type {
            elem_type {
              tensor_type {
                elem_type: 6
                shape {
                  dim {
                    dim_value: 4
                  }
                }
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 15
    }
    .
    
    ======================================================================
    ERROR: test_optional_has_element_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(tensor(float)) is not currently registered or supported
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(tensor(float)) is not currently registered or supported&#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;optional_input&quot;
        output: &quot;output&quot;
        op_type: &quot;OptionalHasElement&quot;
      }
      name: &quot;test_optional_has_element&quot;
      input {
        name: &quot;optional_input&quot;
        type {
          optional_type {
            elem_type {
              tensor_type {
                elem_type: 1
                shape {
                  dim {
                    dim_value: 4
                  }
                }
              }
            }
          }
        }
      }
      output {
        name: &quot;output&quot;
        type {
          tensor_type {
            elem_type: 9
            shape {
            }
          }
        }
      }
    }
    opset_import {
      version: 15
    }
    .
    
    ======================================================================
    ERROR: test_optional_has_element_empty_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(tensor(int32)) is not currently registered or supported
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Exception during loading: MLDataType for: optional(tensor(int32)) is not currently registered or supported&#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;optional_input&quot;
        output: &quot;output&quot;
        op_type: &quot;OptionalHasElement&quot;
      }
      name: &quot;test_optional_has_element_empty&quot;
      input {
        name: &quot;optional_input&quot;
        type {
          optional_type {
            elem_type {
              tensor_type {
                elem_type: 6
                shape {
                }
              }
            }
          }
        }
      }
      output {
        name: &quot;output&quot;
        type {
          tensor_type {
            elem_type: 9
            shape {
            }
          }
        }
      }
    }
    opset_import {
      version: 15
    }
    .
    
    ======================================================================
    ERROR: test_pow_types_float32_uint32_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Pow(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Pow(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;Pow&quot;
      }
      name: &quot;test_pow_types_float32_uint32&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 12
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_pow_types_float32_uint64_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Pow(12) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Pow(12) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;Pow&quot;
      }
      name: &quot;test_pow_types_float32_uint64&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 13
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 12
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_empty_axes_input_noop_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_empty_axes_input_noop_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reshape_allowzero_reordered_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;data&quot;
    type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 3
          }
          dim {
            dim_value: 4
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_resize_downsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales, cubic_coeff_a=-0.50, exclude_outside=1) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 2)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 2)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 2)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_linear_pytorch_half_pixel_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 1)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 1)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_tf_crop_and_resize_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;roi&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;roi&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (8,)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, roi, , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;roi&#39;, 0), 1)
    ((&#39;sizes&#39;, 0), 2)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, roi, , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;roi&#39; type=dtype(&#39;float32&#39;) shape=(8,)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, roi, , sizes, extrapolation_value=10.00) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales, cubic_coeff_a=-0.50, exclude_outside=1) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_asymmetric_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 6)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 6)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 9, 10)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 9, 10)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_ceil_half_pixel_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 7, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 7, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_floor_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_roialign_aligned_false_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;rois&quot;
        input: &quot;batch_indices&quot;
        output: &quot;Y&quot;
        op_type: &quot;RoiAlign&quot;
        attribute {
          name: &quot;coordinate_transformation_mode&quot;
          s: &quot;output_half_pixel&quot;
          type: STRING
        }
        attribute {
          name: &quot;output_height&quot;
          i: 5
          type: INT
        }
        attribute {
          name: &quot;output_width&quot;
          i: 5
          type: INT
        }
        attribute {
          name: &quot;sampling_ratio&quot;
          i: 2
          type: INT
    [...]
      tensor_type {
            elem_type: 7
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_roialign_aligned_true_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;rois&quot;
        input: &quot;batch_indices&quot;
        output: &quot;Y&quot;
        op_type: &quot;RoiAlign&quot;
        attribute {
          name: &quot;coordinate_transformation_mode&quot;
          s: &quot;half_pixel&quot;
          type: STRING
        }
        attribute {
          name: &quot;output_height&quot;
          i: 5
          type: INT
        }
        attribute {
          name: &quot;output_width&quot;
          i: 5
          type: INT
        }
        attribute {
          name: &quot;sampling_ratio&quot;
          i: 2
          type: INT
        }
    
    [...]
      tensor_type {
            elem_type: 7
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;Y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_scan_sum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;initial&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;initial&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 2)}}))
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 3, 2)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 2)}}))
    ((&#39;z&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;z&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 3, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Scan(, initial, x) -&gt; y, z))
    --order--
    ((&#39;initial&#39;, 0), 0)
    ((&#39;x&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Scan(, initial, x) -&gt; y, z)
    --ONNX--
    opset: domain=&#39;&#39; version=8
    input: name=&#39;initial&#39; type=dtype(&#39;float32&#39;) shape=(1, 2)
    input: name=&#39;x&#39; type=dtype(&#39;float32&#39;) shape=(1, 3, 2)
    Scan(, initial, x, num_scan_inputs=1) -&gt; y, z
    output: name=&#39;y&#39; type=dtype(&#39;float32&#39;) shape=(1, 2)
    output: name=&#39;z&#39; type=dtype(&#39;float32&#39;) shape=(1, 3, 2)
    ----- subgraph ---- Scan -  - att.body=
    input: name=&#39;sum_in&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    input: name=&#39;next&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    Add(sum_in, next) -&gt; sum_out
      Identity(sum_out) -&gt; scan_out
    output: name=&#39;sum_out&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    output: name=&#39;scan_out&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    ---
    
    
    ======================================================================
    ERROR: test_scatter_elements_with_duplicate_indices_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data&quot;
        input: &quot;indices&quot;
        input: &quot;updates&quot;
        output: &quot;y&quot;
        op_type: &quot;ScatterElements&quot;
        attribute {
          name: &quot;axis&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;reduction&quot;
          s: &quot;add&quot;
          type: STRING
        }
      }
      name: &quot;test_scatter_elements_with_duplicate_indices&quot;
      input {
        name: &quot;data&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                di
    [...]
      }
      }
      input {
        name: &quot;updates&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_scatternd_add_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data&quot;
        input: &quot;indices&quot;
        input: &quot;updates&quot;
        output: &quot;y&quot;
        op_type: &quot;ScatterND&quot;
        attribute {
          name: &quot;reduction&quot;
          s: &quot;add&quot;
          type: STRING
        }
      }
      name: &quot;test_scatternd_add&quot;
      input {
        name: &quot;data&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
              dim {
            
    [...]
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_scatternd_multiply_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map&lt;std::__cxx11::basic_string&lt;char&gt;, int&gt;&amp;, const onnxruntime::logging::Logger&amp;, bool, const string&amp;, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 16 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 15.
    &#39;
    ir_version: 8
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;data&quot;
        input: &quot;indices&quot;
        input: &quot;updates&quot;
        output: &quot;y&quot;
        op_type: &quot;ScatterND&quot;
        attribute {
          name: &quot;reduction&quot;
          s: &quot;mul&quot;
          type: STRING
        }
      }
      name: &quot;test_scatternd_multiply&quot;
      input {
        name: &quot;data&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
              dim {
       
    [...]
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;y&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 16
    }
    .
    
    ======================================================================
    ERROR: test_sequence_insert_at_back_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 117, in run
        return self.sess._sess.run_with_ort_values(
    RuntimeError: Unable to cast Python instance to C++ type (compile in debug mode for details)
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1232, in _run_whole_runtime
        res = self._whole.run(inputs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 121, in run
        {k: v._get_c_value() for k, v in inputs.items()},
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 121, in &lt;dictcomp&gt;
        {k: v._get_c_value() for k, v in inputs.items()},
    AttributeError: &#39;list&#39; object has no attribute &#39;_get_c_value&#39;
    
    ======================================================================
    ERROR: test_sequence_insert_at_front_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 117, in run
        return self.sess._sess.run_with_ort_values(
    RuntimeError: Unable to cast Python instance to C++ type (compile in debug mode for details)
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1232, in _run_whole_runtime
        res = self._whole.run(inputs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 121, in run
        {k: v._get_c_value() for k, v in inputs.items()},
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 121, in &lt;dictcomp&gt;
        {k: v._get_c_value() for k, v in inputs.items()},
    AttributeError: &#39;list&#39; object has no attribute &#39;_get_c_value&#39;
    
    ======================================================================
    ERROR: test_simple_rnn_batchwise_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/rnn.h:45 onnxruntime::RNN&lt;T&gt;::RNN(const onnxruntime::OpKernelInfo&amp;) [with T = float] layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/rnn.h:45 onnxruntime::RNN&lt;T&gt;::RNN(const onnxruntime::OpKernelInfo&amp;) [with T = float] layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    &#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;X&quot;
        input: &quot;W&quot;
        input: &quot;R&quot;
        output: &quot;Y&quot;
        output: &quot;Y_h&quot;
        op_type: &quot;RNN&quot;
        attribute {
          name: &quot;hidden_size&quot;
          i: 4
          type: INT
        }
        attribute {
          name: &quot;layout&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;test_simple_rnn_batchwise&quot;
      input {
        name: &quot;X&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              d
    [...]
            dim {
                dim_value: 1
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;Y_h&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_slice_start_out_of_bounds_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;y&quot;
    type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 20
          }
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 5
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_split_zero_size_splits_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;input&quot;
    type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_sub_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Sub(14) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Sub(14) node with name &#39;&#39;&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;x&quot;
        input: &quot;y&quot;
        output: &quot;z&quot;
        op_type: &quot;Sub&quot;
      }
      name: &quot;test_sub_uint8&quot;
      input {
        name: &quot;x&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      input {
        name: &quot;y&quot;
        type {
          tensor_type {
            ele
    [...]
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;z&quot;
        type {
          tensor_type {
            elem_type: 2
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 14
    }
    .
    
    ======================================================================
    ERROR: test_tril_zero_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;x&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 3
          }
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 5
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_triu_zero_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;x&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 5
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;Unsqueeze&quot;
        attribute {
          name: &quot;axes&quot;
          ints: 3
          type: INTS
        }
      }
      node {
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 2
          ints: 1
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute
    [...]
    
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
      output {
        name: &quot;3&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;Unsqueeze&quot;
        attribute {
          name: &quot;axes&quot;
          ints: 3
          type: INTS
        }
      }
      node {
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 2
          ints: 1
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute
    [...]
    
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
      output {
        name: &quot;3&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 2
          ints: 2
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute {
          name: &quot;strides&quot;
          ints: 2
          ints: 2
          type: INTS
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
    [...]
    
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
      output {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool2d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 2
          ints: 2
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute {
          name: &quot;strides&quot;
          ints: 2
          ints: 2
          type: INTS
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
    [...]
    
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
      output {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 2
          ints: 2
          ints: 2
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute {
          name: &quot;strides&quot;
          ints: 2
          ints: 2
          ints: 2
          type: INTS
        }
      }
      nam
    [...]
    
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool3d_stride1_pad0_gpu_input_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 3
          ints: 3
          ints: 3
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute {
          name: &quot;strides&quot;
          ints: 1
          ints: 1
          ints: 1
          type: INTS
        }
      }
      nam
    [...]
    
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_AvgPool3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for AveragePool(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;AveragePool&quot;
        attribute {
          name: &quot;kernel_shape&quot;
          ints: 2
          ints: 2
          ints: 2
          type: INTS
        }
        attribute {
          name: &quot;pads&quot;
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          ints: 0
          type: INTS
        }
        attribute {
          name: &quot;strides&quot;
          ints: 2
          ints: 2
          ints: 2
          type: INTS
        }
      }
      nam
    [...]
    
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_BatchNorm1d_3d_input_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        input: &quot;3&quot;
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;BatchNormalization&quot;
        attribute {
          name: &quot;epsilon&quot;
          f: 9.999999747378752e-06
          type: FLOAT
        }
        attribute {
          name: &quot;is_test&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;momentum&quot;
          f: 0.8999999761581421
          type: FLOAT
        }
      }
      name: &quot;torch-jit-export&quot;
      initial
    [...]
       }
        }
      }
      input {
        name: &quot;4&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;5&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_BatchNorm2d_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        input: &quot;3&quot;
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;BatchNormalization&quot;
        attribute {
          name: &quot;epsilon&quot;
          f: 9.999999747378752e-06
          type: FLOAT
        }
        attribute {
          name: &quot;is_test&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;momentum&quot;
          f: 0.8999999761581421
          type: FLOAT
        }
      }
      name: &quot;torch-jit-export&quot;
      initial
    [...]
       tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;5&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_BatchNorm2d_momentum_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        input: &quot;3&quot;
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;BatchNormalization&quot;
        attribute {
          name: &quot;epsilon&quot;
          f: 0.0010000000474974513
          type: FLOAT
        }
        attribute {
          name: &quot;is_test&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;momentum&quot;
          f: 0.20000000298023224
          type: FLOAT
        }
      }
      name: &quot;torch-jit-export&quot;
      initia
    [...]
       tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;5&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_BatchNorm3d_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        input: &quot;3&quot;
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;BatchNormalization&quot;
        attribute {
          name: &quot;epsilon&quot;
          f: 9.999999747378752e-06
          type: FLOAT
        }
        attribute {
          name: &quot;is_test&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;momentum&quot;
          f: 0.8999999761581421
          type: FLOAT
        }
      }
      name: &quot;torch-jit-export&quot;
      initial
    [...]
    
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;5&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_BatchNorm3d_momentum_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for BatchNormalization(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        input: &quot;3&quot;
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;BatchNormalization&quot;
        attribute {
          name: &quot;epsilon&quot;
          f: 0.0010000000474974513
          type: FLOAT
        }
        attribute {
          name: &quot;is_test&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;momentum&quot;
          f: 0.30000001192092896
          type: FLOAT
        }
      }
      name: &quot;torch-jit-export&quot;
      initia
    [...]
    
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;5&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_GLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Split&quot;
        attribute {
          name: &quot;axis&quot;
          i: -1
          type: INT
        }
      }
      node {
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Sigmoid&quot;
      }
      node {
        input: &quot;1&quot;
        input: &quot;3&quot;
        output: &quot;4&quot;
        op_type: &quot;Mul&quot;
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
      output {
        name: &quot;4&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_GLU_dim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Split&quot;
        attribute {
          name: &quot;axis&quot;
          i: 1
          type: INT
        }
      }
      node {
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Sigmoid&quot;
      }
      node {
        input: &quot;1&quot;
        input: &quot;3&quot;
        output: &quot;4&quot;
        op_type: &quot;Mul&quot;
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 6
              }
              dim {
                dim_value: 7
              }
            }
          }
        }
      }
      output {
        name: &quot;4&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 7
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_Linear_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Gemm&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 1.0
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 1.0
          type: FLOAT
        }
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;transB&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        
    [...]
    {
                dim_value: 10
              }
            }
          }
        }
      }
      input {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 8
              }
            }
          }
        }
      }
      output {
        name: &quot;3&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 8
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PReLU_1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;PRelu&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 1
        data_type: 1
        name: &quot;1&quot;
        raw_data: &quot;\000\000\200&gt;&quot;
      }
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim
    [...]
       }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PReLU_1d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;PRelu&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 3
        data_type: 1
        name: &quot;1&quot;
        raw_data: &quot;\000\000\200&gt;\000\000\200&gt;\000\000\200&gt;&quot;
      }
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
         
    [...]
       }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PReLU_2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;PRelu&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 1
        data_type: 1
        name: &quot;1&quot;
        raw_data: &quot;\000\000\200&gt;&quot;
      }
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim
    [...]
       tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PReLU_2d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;PRelu&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 3
        data_type: 1
        name: &quot;1&quot;
        raw_data: &quot;\000\000\200&gt;\000\000\200&gt;\000\000\200&gt;&quot;
      }
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
         
    [...]
       tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PReLU_3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;PRelu&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 1
        data_type: 1
        name: &quot;1&quot;
        raw_data: &quot;\000\000\200&gt;&quot;
      }
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim
    [...]
    
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PReLU_3d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for PRelu(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;PRelu&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 3
        data_type: 1
        name: &quot;1&quot;
        raw_data: &quot;\000\000\200&gt;\000\000\200&gt;\000\000\200&gt;&quot;
      }
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
         
    [...]
    
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
              dim {
                dim_value: 5
              }
              dim {
                dim_value: 6
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_PoissonNLLLLoss_no_reduce_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Mul(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        output: &quot;1&quot;
        op_type: &quot;Constant&quot;
        attribute {
          name: &quot;value&quot;
          t {
            dims: 10
            dims: 10
            data_type: 1
            raw_data: &quot;g\253\261\277.\034\211\276G`\000\276Q|\300\277\347G\250\276\203!=?\025\220\313?\360hG\276.\235\214\276\355\242\260\276\351X7\277\354\376\023?)\324\302&gt;R\217-\277u\026\001\276\333\246\326\277\350XM?Z\311\263\276\2209\211\275i\240Z\275\306\337T\277\016\374t?y\r\
    [...]
    it-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 10
              }
              dim {
                dim_value: 10
              }
            }
          }
        }
      }
      output {
        name: &quot;4&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 10
              }
              dim {
                dim_value: 10
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_Softsign_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        output: &quot;1&quot;
        op_type: &quot;Abs&quot;
      }
      node {
        output: &quot;2&quot;
        op_type: &quot;Constant&quot;
        attribute {
          name: &quot;value&quot;
          t {
            data_type: 1
            raw_data: &quot;\000\000\200?&quot;
          }
          type: TENSOR
        }
      }
      node {
        input: &quot;1&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Add&quot;
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
      }
      node {
        input: &quot;0&quot;
        
    [...]
    
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: &quot;4&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_add_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;axis&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_add_size1_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;axis&quot;
          i: 0
          type: INT
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value
    [...]
     }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_add_size1_right_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;axis&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_add_size1_singleton_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
        attribute {
          name: &quot;axis&quot;
          i: 0
          type: INT
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value
    [...]
     }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_addconstant_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        output: &quot;1&quot;
        op_type: &quot;Constant&quot;
        attribute {
          name: &quot;value&quot;
          t {
            data_type: 11
            raw_data: &quot;\000\000\000\000\000\000\360?&quot;
          }
          type: TENSOR
        }
      }
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 11
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_addmm_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Gemm&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 1.0
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 1.0
          type: FLOAT
        }
        attribute {
          name: &quot;broadcast&quot;
          i: 1
          type: INT
        }
      }
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;3&quot;
        output: &quot;4&quot;
        op_type: &quot;Gemm&quot;
        attribute {
          name:
    [...]
     {
                dim_value: 4
              }
            }
          }
        }
      }
      input {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;4&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_basic_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
      }
      node {
        input: &quot;0&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Mul&quot;
      }
      node {
        input: &quot;3&quot;
        output: &quot;4&quot;
        op_type: &quot;Tanh&quot;
      }
      node {
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;Sigmoid&quot;
      }
      node {
        input: &quot;5&quot;
        output: &quot;6&quot;
        op_type: &quot;Neg&quot;
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
      output {
        name: &quot;6&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_mm_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Gemm(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        output: &quot;2&quot;
        op_type: &quot;Constant&quot;
        attribute {
          name: &quot;value&quot;
          t {
            dims: 1
            data_type: 1
            raw_data: &quot;\000\000\000\000&quot;
          }
          type: TENSOR
        }
      }
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Gemm&quot;
        attribute {
          name: &quot;alpha&quot;
          f: 1.0
          type: FLOAT
        }
        attribute {
          name: &quot;beta&quot;
          f: 0.0
          type: 
    [...]
       }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;3&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_non_float_params_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
      }
      node {
        input: &quot;0&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Mul&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 2
        dims: 2
        data_type: 7
        name: &quot;1&quot;
        raw_data: &quot;\001\000\000\000\000\000\000\000\002\000\000\000\000\000\000\000\003\000\000\000\000\000\000\000\004\000\000\000\000\000\000\000&quot;
      }
      input {
        name: &quot;0&quot;
      
    [...]
       }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 7
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;3&quot;
        type {
          tensor_type {
            elem_type: 7
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_params_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Add(6) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Add&quot;
      }
      node {
        input: &quot;0&quot;
        input: &quot;2&quot;
        output: &quot;3&quot;
        op_type: &quot;Mul&quot;
      }
      node {
        input: &quot;3&quot;
        output: &quot;4&quot;
        op_type: &quot;Tanh&quot;
      }
      node {
        input: &quot;4&quot;
        output: &quot;5&quot;
        op_type: &quot;Sigmoid&quot;
      }
      node {
        input: &quot;5&quot;
        output: &quot;6&quot;
        op_type: &quot;Neg&quot;
      }
      name: &quot;torch-jit-export&quot;
      initializer {
        dims: 2
        dims: 2
        data_typ
    [...]
       }
        }
      }
      input {
        name: &quot;1&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
      output {
        name: &quot;6&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 2
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_operator_pow_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 371, in _create_inference_session
        sess.initialize_session(providers, provider_options, disabled_optimizers)
    onnxruntime.capi.onnxruntime_pybind11_state.NotImplemented: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Pow(1) node with name &#39;&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for Pow(1) node with name &#39;&#39;&#39;
    ir_version: 3
    producer_name: &quot;pytorch&quot;
    producer_version: &quot;0.3&quot;
    graph {
      node {
        input: &quot;0&quot;
        input: &quot;1&quot;
        output: &quot;2&quot;
        op_type: &quot;Pow&quot;
      }
      name: &quot;torch-jit-export&quot;
      input {
        name: &quot;0&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
            }
          }
      
    [...]
    
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
      output {
        name: &quot;2&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
                dim_value: 1
              }
              dim {
                dim_value: 2
              }
              dim {
                dim_value: 3
              }
              dim {
                dim_value: 4
              }
            }
          }
        }
      }
    }
    opset_import {
      version: 6
    }
    .
    
    ======================================================================
    ERROR: test_gradient_of_add_and_mul_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Gradient is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Gradient is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;a&quot;
        input: &quot;b&quot;
        output: &quot;c&quot;
        name: &quot;my_add&quot;
        op_type: &quot;Add&quot;
      }
      node {
        input: &quot;c&quot;
        input: &quot;a&quot;
        output: &quot;d&quot;
        name: &quot;my_mul&quot;
        op_type: &quot;Mul&quot;
      }
      node {
        input: &quot;a&quot;
        input: &quot;b&quot;
        output: &quot;dd_da&quot;
        output: &quot;dd_db&quot;
        name: &quot;my_gradient&quot;
        op_type: &quot;Gradient&quot;
        attribute {
          name: &quot;xs&quot;
          strings: &quot;a&quot;
          strings: &quot;b&quot;
          type: STRINGS
        }
        attribute {
          name: &quot;y
    [...]
        }
        }
      }
      output {
        name: &quot;d&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
            }
          }
        }
      }
      output {
        name: &quot;dd_da&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
            }
          }
        }
      }
      output {
        name: &quot;dd_db&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;&quot;
      version: 12
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    ERROR: test_gradient_of_add_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 85, in __init__
        self.sess = InferenceSession(onnx_data, sess_options=sess_options,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 326, in __init__
        self._create_inference_session(providers, provider_options, disabled_optimizers)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_venv/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 362, in _create_inference_session
        sess = C.InferenceSession(session_options, self._model_bytes, False, self._read_config_from_model)
    onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: Gradient is not a registered function/op
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 214, in create_inference_session
        return OnnxInference(model, runtime=&#39;onnxruntime1&#39;)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 227, in _init
        self._whole = OnnxWholeSession(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_whole/session.py&quot;, line 90, in __init__
        raise RuntimeError(
    RuntimeError: Unable to create InferenceSession due to &#39;[ONNXRuntimeError] : 1 : FAIL : Fatal error: Gradient is not a registered function/op&#39;
    ir_version: 7
    producer_name: &quot;backend-test&quot;
    graph {
      node {
        input: &quot;a&quot;
        input: &quot;b&quot;
        output: &quot;c&quot;
        name: &quot;my_add&quot;
        op_type: &quot;Add&quot;
      }
      node {
        input: &quot;a&quot;
        input: &quot;b&quot;
        output: &quot;dc_da&quot;
        output: &quot;dc_db&quot;
        name: &quot;my_gradient&quot;
        op_type: &quot;Gradient&quot;
        attribute {
          name: &quot;xs&quot;
          strings: &quot;a&quot;
          strings: &quot;b&quot;
          type: STRINGS
        }
        attribute {
          name: &quot;y&quot;
          s: &quot;c&quot;
          type: STRING
        }
        domain: &quot;ai.onnx.preview.training&quot;
      }
      name: &quot;Gradi
    [...]
        }
        }
      }
      output {
        name: &quot;c&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
            }
          }
        }
      }
      output {
        name: &quot;dc_da&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
            }
          }
        }
      }
      output {
        name: &quot;dc_db&quot;
        type {
          tensor_type {
            elem_type: 1
            shape {
            }
          }
        }
      }
    }
    opset_import {
      domain: &quot;&quot;
      version: 12
    }
    opset_import {
      domain: &quot;ai.onnx.preview.training&quot;
      version: 1
    }
    .
    
    ======================================================================
    FAIL: test_bernoulli_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 7 / 10 (70%)
    Max absolute difference: 1.
    Max relative difference: 1.
     x: array([1., 0., 0., 1., 0., 0., 1., 0., 1., 1.])
     y: array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.])
    
    ======================================================================
    FAIL: test_bernoulli_double_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 7 / 10 (70%)
    Max absolute difference: 1.
    Max relative difference: 1.
     x: array([1., 0., 0., 0., 1., 0., 0., 0., 0., 1.])
     y: array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.])
    
    ======================================================================
    FAIL: test_bernoulli_double_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 6 / 10 (60%)
    Max absolute difference: 1.
    Max relative difference: 1.
     x: array([1., 1., 1., 0., 1., 0., 1., 0., 0., 1.])
     y: array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.])
    
    ======================================================================
    FAIL: test_bernoulli_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 8 / 10 (80%)
    Max absolute difference: 1.
    Max relative difference: 1.
     x: array([1., 1., 0., 1., 1., 0., 1., 0., 0., 1.])
     y: array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.])
    
    ======================================================================
    FAIL: test_bernoulli_seed_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 5 / 10 (50%)
    Max absolute difference: 1.
    Max relative difference: 1.
     x: array([0., 0., 1., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)
     y: array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.], dtype=float32)
    
    ======================================================================
    FAIL: test_bernoulli_seed_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 5 / 10 (50%)
    Max absolute difference: 1.
    Max relative difference: 1.
     x: array([0., 0., 1., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)
     y: array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.], dtype=float32)
    
    ======================================================================
    FAIL: test_cast_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    Mismatched elements: 3 / 12 (25%)
     x: array([[&#39;0.9767611&#39;, &#39;0.60484552&#39;, &#39;0.73926359&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.29614019&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
     y: array([[&#39;0.9767611&#39;, &#39;0.6048455&#39;, &#39;0.7392636&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.2961402&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
    
    ======================================================================
    FAIL: test_castlike_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    Mismatched elements: 3 / 12 (25%)
     x: array([[&#39;0.9767611&#39;, &#39;0.60484552&#39;, &#39;0.73926359&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.29614019&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
     y: array([[&#39;0.9767611&#39;, &#39;0.6048455&#39;, &#39;0.7392636&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.2961402&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
    
    ======================================================================
    FAIL: test_castlike_FLOAT_to_STRING_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    Mismatched elements: 3 / 12 (25%)
     x: array([[&#39;0.9767611&#39;, &#39;0.60484552&#39;, &#39;0.73926359&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.29614019&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
     y: array([[&#39;0.9767611&#39;, &#39;0.6048455&#39;, &#39;0.7392636&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.2961402&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
    
    ======================================================================
    FAIL: test_convtranspose_autopad_same_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 60 / 72 (83.3%)
    Max absolute difference: 20.
    Max relative difference: 11.
     x: array([[[[ 0.,  1.,  1.,  3.,  2.,  2.],
             [ 3.,  8.,  5., 12.,  7.,  7.],
             [ 3.,  7.,  4.,  9.,  5.,  5.],...
     y: array([[[[ 0.,  0.,  1.,  1.,  3.,  2.],
             [ 0.,  0.,  1.,  1.,  3.,  2.],
             [ 3.,  3.,  8.,  5., 12.,  7.],...
    
    ======================================================================
    FAIL: test_maxunpool_export_with_output_shape_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 8 / 25 (32%)
    Max absolute difference: 8.
    Max relative difference: 1.
     x: array([[[[0., 0., 0., 0., 0.],
             [5., 0., 6., 0., 0.],
             [0., 0., 0., 7., 0.],...
     y: array([[[[0., 0., 0., 0., 0.],
             [0., 5., 0., 6., 0.],
             [0., 0., 0., 0., 0.],...
    
    ======================================================================
    FAIL: test_training_dropout_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 23 / 60 (38.3%)
    Max absolute difference: 10.212
    Max relative difference: 1.
     x: array([[[ 0.      ,  0.      ,  3.914952,  0.      ,  0.      ],
            [-0.      ,  0.      , -0.      , -0.      ,  1.642394],
            [ 0.      ,  0.      ,  3.044151,  0.      ,  0.      ],...
     y: array([[[  0.      ,   0.      ,   0.      ,   0.      ,   0.      ],
            [ -0.      ,   0.      ,  -0.605429,  -0.412875,   0.      ],
            [  0.576174,   0.      ,   0.      ,   0.4867  ,   0.      ],...
    
    ======================================================================
    FAIL: test_training_dropout_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 32 / 60 (53.3%)
    Max absolute difference: 5.106
    Max relative difference: 1.
     x: array([[[ 0.      ,  0.      ,  1.957476,  0.      ,  3.735116],
            [-0.      ,  0.      , -0.302714, -0.206438,  0.821197],
            [ 0.      ,  2.908547,  1.522075,  0.      ,  0.      ],...
     y: array([[[ 3.528105,  0.800314,  1.957476,  4.481786,  0.      ],
            [-1.954556,  0.      , -0.302714, -0.206438,  0.      ],
            [ 0.288087,  2.908547,  1.522075,  0.24335 ,  0.      ],...
    
    ======================================================================
    FAIL: test_training_dropout_default_mask_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 32 / 60 (53.3%)
    Max absolute difference: 5.106
    Max relative difference: 1.
     x: array([[[ 0.      ,  0.      ,  1.957476,  0.      ,  3.735116],
            [-0.      ,  0.      , -0.302714, -0.206438,  0.821197],
            [ 0.      ,  2.908547,  1.522075,  0.      ,  0.      ],...
     y: array([[[ 3.528105,  0.800314,  1.957476,  4.481786,  0.      ],
            [-1.954556,  0.      , -0.302714, -0.206438,  0.      ],
            [ 0.288087,  2.908547,  1.522075,  0.24335 ,  0.      ],...
    
    ======================================================================
    FAIL: test_training_dropout_mask_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 23 / 60 (38.3%)
    Max absolute difference: 10.212
    Max relative difference: 1.
     x: array([[[ 0.      ,  0.      ,  3.914952,  0.      ,  0.      ],
            [-0.      ,  0.      , -0.      , -0.      ,  1.642394],
            [ 0.      ,  0.      ,  3.044151,  0.      ,  0.      ],...
     y: array([[[  0.      ,   0.      ,   0.      ,   0.      ,   0.      ],
            [ -0.      ,   0.      ,  -0.605429,  -0.412875,   0.      ],
            [  0.576174,   0.      ,   0.      ,   0.4867  ,   0.      ],...
    
    ----------------------------------------------------------------------
    Ran 2026 tests in 46.594s
    
    FAILED (failures=15, errors=131, skipped=1021)
    [runpythonerror]
    2022-03-10 10:30:46.789759367 [E:onnxruntime:, inference_session.cc:1463 operator()] Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/deep_cpu_gru.h:55 onnxruntime::DeepCpuGruOp::DeepCpuGruOp(const onnxruntime::OpKernelInfo&amp;) layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    
    2022-03-10 10:30:47.614908630 [W:onnxruntime:, constant_folding.cc:202 ApplyImpl] Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can&#39;t constant fold SequenceConstruct node &#39;&#39;
    2022-03-10 10:30:47.615006589 [W:onnxruntime:, constant_folding.cc:202 ApplyImpl] Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can&#39;t constant fold SequenceConstruct node &#39;&#39;
    2022-03-10 10:30:47.615175937 [W:onnxruntime:, constant_folding.cc:202 ApplyImpl] Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can&#39;t constant fold SequenceConstruct node &#39;&#39;
    2022-03-10 10:30:47.615244317 [W:onnxruntime:, constant_folding.cc:202 ApplyImpl] Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can&#39;t constant fold SequenceConstruct node &#39;&#39;
    2022-03-10 10:30:49.095137428 [E:onnxruntime:, inference_session.cc:1463 operator()] Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/lstm_base.h:52 onnxruntime::LSTMBase::LSTMBase(const onnxruntime::OpKernelInfo&amp;) layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    
    2022-03-10 10:30:57.171746435 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:04.908429952 [E:onnxruntime:, inference_session.cc:1463 operator()] Exception during initialization: /var/lib/jenkins/workspace/_automation/_automation_FORK_onnxruntime-jenkins_39_std/onnxruntime/onnxruntime/core/providers/cpu/rnn/rnn.h:45 onnxruntime::RNN&lt;T&gt;::RNN(const onnxruntime::OpKernelInfo&amp;) [with T = float] layout_ == 0 was false. Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
    
    2022-03-10 10:31:10.711233890 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:10.909070391 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.107777662 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.277886700 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.366464913 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.372151724 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.377876665 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.391855600 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.398910067 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.405945064 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.413026971 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.420115378 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.427943206 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.440528026 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.453013947 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.497635415 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.506198956 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.515428510 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.537103685 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.545869935 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.555136429 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.563838249 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.573494969 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.582534695 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.592179225 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.610760033 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.621204495 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.630721446 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.639412546 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.648267734 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.657071193 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.666497156 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.675395033 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.683939665 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:11.692472997 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.268873107 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.287038748 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.296406051 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.312197788 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.328337961 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.346629771 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.354192893 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.362311759 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.369843060 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.377977086 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.472556127 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.584256000 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.591464175 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.599506612 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.696868624 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.704974459 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.712452212 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.720553078 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.771899036 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.866841973 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.874414564 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.886718927 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:12.903538063 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.291918760 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.297728050 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.303562140 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.309372199 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.315131990 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.324919758 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.337747826 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.360285792 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.375608763 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.383407913 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.399173019 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.413539631 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.426271829 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.444510540 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.452174530 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.460726462 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.556192013 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.563709565 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.571030719 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.578772709 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.585994504 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.593747134 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.607576191 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.752023165 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:13.900439627 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.014022891 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.100539595 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.130874981 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.137392393 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.144524049 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.157861701 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.165206775 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.173712727 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.295898311 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.303159296 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.310193903 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.410258747 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.418995676 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.427493318 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.436109689 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.444489492 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.500810699 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.564555749 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.573295938 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.631939471 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.639443313 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.676463050 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.684514556 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.691950869 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.699947907 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.707470949 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.715508125 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.723724050 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.730702788 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.739079851 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.748889210 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:14.760474170 [W:onnxruntime:, model.cc:152 Model] ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain &#39;ai.onnx&#39;. Please upgrade your model to opset 7 or higher. For now, this opset 6 model may run depending upon legacy support of some older opset version operators.
    2022-03-10 10:31:21.220393790 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.220451599 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_at appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.221084323 [W:onnxruntime:, constant_folding.cc:202 ApplyImpl] Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can&#39;t constant fold SequenceEmpty node &#39;&#39;
    2022-03-10 10:31:21.221267511 [W:onnxruntime:, constant_folding.cc:202 ApplyImpl] Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can&#39;t constant fold SequenceEmpty node &#39;&#39;
    2022-03-10 10:31:21.237269076 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_erase appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.237326805 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_at appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.251415199 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_erase appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.251475068 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_insert appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.251504918 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_at appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
    2022-03-10 10:31:21.315456736 [W:onnxruntime:, graph.cc:1137 Graph] Initializer pos_at appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
</pre></div>
</div>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="backend_python.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">ONNX Backends for Python/Numpy runtime</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../onnx_bench.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">scikit-learn Converters and Benchmarks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>