
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>ONNX Backends for Python/Numpy runtime &#8212; Python Runtime for ONNX</title>
    
    <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script src="../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="shortcut icon" href="../_static/project_ico.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ONNX Backends for onnxruntime1" href="backend_onnxruntime1.html" />
    <link rel="prev" title="ONNX Backends" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../installation.html">
  Installation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorial/index.html">
  Tutorial
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/index.html">
  API
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../onnx.html">
  ONNX, Runtime, Backends
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../onnx_bench.html">
  scikit-learn Converters and Benchmarks
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_cmd.html">
  Command lines
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_ex.html">
  Examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_index.html">
  FAQ, code, …
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../gyexamples/index.html">
  Gallery of examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../all_notebooks.html">
  Notebook Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../HISTORY.html">
  History
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../onnx_runtime.html">
   Runtimes for ONNX
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   ONNX Backends
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     ONNX Backends for Python/Numpy runtime
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="backend_onnxruntime1.html">
     ONNX Backends for onnxruntime1
    </a>
   </li>
  </ul>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="onnx-backends-for-python-numpy-runtime">
<h1>ONNX Backends for Python/Numpy runtime<a class="headerlink" href="#onnx-backends-for-python-numpy-runtime" title="Permalink to this headline">¶</a></h1>
<p>Backend class: <a class="reference internal" href="../mlprodict/onnxrt/backend.html#mlprodict.onnxrt.backend.OnnxInferenceBackend" title="mlprodict.onnxrt.backend.OnnxInferenceBackend"><code class="xref py py-class docutils literal notranslate"><span class="pre">OnnxInferenceBackend</span></code></a>.</p>
<p>&lt;&lt;&lt;</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">redirect_stdout</span><span class="p">,</span> <span class="n">redirect_stderr</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="kn">from</span> <span class="nn">onnx.backend.test</span> <span class="kn">import</span> <span class="n">BackendTest</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">onnx_version</span>
<span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">ort_version</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">npy_version</span>
<span class="kn">import</span> <span class="nn">mlprodict.onnxrt.backend_py</span> <span class="k">as</span> <span class="nn">backend</span>

<span class="n">back_test</span> <span class="o">=</span> <span class="n">BackendTest</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">include</span><span class="p">(</span><span class="s1">&#39;.*_cpu&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_blvc_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_densenet_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_densenet121_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_inception_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_resnet50_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_shufflenet_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_squeezenet_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_vgg19_.*&#39;</span><span class="p">)</span>
<span class="n">back_test</span><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s1">&#39;.*_zfnet512_.*&#39;</span><span class="p">)</span>
<span class="nb">globals</span><span class="p">()</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">back_test</span><span class="o">.</span><span class="n">enable_report</span><span class="p">()</span><span class="o">.</span><span class="n">test_cases</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;python&#39;</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;onnx&#39;</span><span class="p">,</span> <span class="n">onnx_version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;onnxruntime&#39;</span><span class="p">,</span> <span class="n">ort_version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="n">npy_version</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">(),</span> <span class="s2">&quot;BEGIN&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>

<span class="n">buffer</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span>
<span class="k">if</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">redirect_stdout</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">redirect_stderr</span><span class="p">(</span><span class="n">buffer</span><span class="p">):</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">exit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">exit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">testsRun</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">testsRun</span>
<span class="n">errors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
<span class="n">skipped</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">skipped</span><span class="p">)</span>
<span class="n">unexpectedSuccesses</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">unexpectedSuccesses</span><span class="p">)</span>
<span class="n">expectedFailures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">expectedFailures</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">(),</span> <span class="s2">&quot;END&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;testsRun=</span><span class="si">%d</span><span class="s2"> errors=</span><span class="si">%d</span><span class="s2"> skipped=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">testsRun</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="n">skipped</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;unexpectedSuccesses=</span><span class="si">%d</span><span class="s2"> expectedFailures=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
    <span class="n">unexpectedSuccesses</span><span class="p">,</span> <span class="n">expectedFailures</span><span class="p">))</span>
<span class="n">ran</span> <span class="o">=</span> <span class="n">testsRun</span> <span class="o">-</span> <span class="n">skipped</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ratio=</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">errors</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">ran</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>
</pre></div>
</div>
<p>&gt;&gt;&gt;</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    ---------------------------------
    python 3.9.1 (default, Jan 18 2021, 16:35:58) 
    [GCC 8.3.0]
    onnx 1.11.0
    onnxruntime 1.10.91
    numpy 1.21.5
    ---------------------------------
    2022-03-10 10:31:24.650371 BEGIN
    ---------------------------------
    ---------------------------------
    2022-03-10 10:31:51.193329 END
    ---------------------------------
    testsRun=2026 errors=376 skipped=1021
    unexpectedSuccesses=0 expectedFailures=0
    ratio=0.625871
    ---------------------------------
    test_abs_cpu (__main__.OnnxBackendNodeModelTest) ... /var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op_numpy_helper.py:8: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.
      from scipy.sparse.coo import coo_matrix
    /usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py:188: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      if ref_outputs[i].dtype == np.object:
    ok
    test_abs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acos_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acos_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acosh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acosh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_acosh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_acosh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adagrad_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adagrad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adagrad_multiple_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adagrad_multiple_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adam_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adam_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_adam_multiple_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_adam_multiple_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_add_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_add_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_add_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_add_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_add_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_add_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast3v1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast3v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast4v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast4v3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_and_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_and_bcast4v4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_default_axis_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_default_axis_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmax_negative_axis_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmax_negative_axis_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmax_negative_axis_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_negative_axis_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmax_negative_axis_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmax_no_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmax_no_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_default_axis_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_default_axis_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmin_negative_axis_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmin_negative_axis_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmin_negative_axis_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_negative_axis_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_argmin_negative_axis_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_example_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_argmin_no_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_argmin_no_keepdims_random_select_last_index_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asin_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asin_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asin_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asin_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asinh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asinh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_asinh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_asinh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atan_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atan_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atan_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atan_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atanh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atanh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_atanh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_atanh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_1d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_1d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_ceil_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_averagepool_2d_ceil_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_pads_count_include_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_pads_count_include_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_pads_count_include_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_pads_count_include_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_precomputed_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_precomputed_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_same_lower_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_same_lower_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_2d_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_2d_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_averagepool_3d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_averagepool_3d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_basic_conv_with_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_basic_conv_with_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_basic_conv_without_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_basic_conv_without_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_basic_convinteger_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_basic_convinteger_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_epsilon_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_batchnorm_epsilon_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_epsilon_training_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_batchnorm_epsilon_training_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_batchnorm_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_batchnorm_example_training_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_batchnorm_example_training_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bernoulli_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_double_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bernoulli_double_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_double_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bernoulli_double_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bernoulli_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_seed_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bernoulli_seed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bernoulli_seed_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bernoulli_seed_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_left_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_left_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_left_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_left_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_left_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_right_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_right_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_right_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bitshift_right_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_bitshift_right_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cast_BFLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_DOUBLE_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_DOUBLE_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_DOUBLE_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_DOUBLE_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT16_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT16_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cast_FLOAT_to_BFLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_FLOAT_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_cast_FLOAT_to_STRING_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cast_STRING_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cast_STRING_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_BFLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_BFLOAT16_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_BFLOAT16_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT16_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_DOUBLE_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_DOUBLE_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_DOUBLE_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_DOUBLE_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT16_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT16_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_FLOAT_to_BFLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_BFLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_castlike_FLOAT_to_BFLOAT16_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_DOUBLE_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_DOUBLE_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_DOUBLE_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_DOUBLE_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_FLOAT16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_FLOAT16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_FLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_FLOAT_to_FLOAT16_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_castlike_FLOAT_to_STRING_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_FLOAT_to_STRING_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_castlike_FLOAT_to_STRING_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_STRING_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_STRING_to_FLOAT_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_castlike_STRING_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_castlike_STRING_to_FLOAT_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ceil_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_ceil_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ceil_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_ceil_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_celu_cpu (__main__.OnnxBackendNodeModelTest) ... /var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_celu.py:47: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      lambda x: pycelu(x, self.alpha), otypes=[numpy.float])
    ok
    test_celu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_celu_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_celu_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_inbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_inbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_int8_inbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_int8_inbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_int8_max_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_int8_max_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_int8_min_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_default_int8_min_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_max_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_clip_default_max_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_default_min_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_default_min_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_inbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_inbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_outbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_outbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_clip_splitbounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_clip_splitbounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_compress_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_compress_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_1d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_1d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_1d_axis_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_1d_axis_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_2d_axis_negative_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_2d_axis_negative_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_negative_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_negative_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_concat_3d_axis_negative_3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_concat_3d_axis_negative_3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constant_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constant_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constant_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constant_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constantofshape_float_ones_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constantofshape_float_ones_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constantofshape_int_shape_zero_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_constantofshape_int_shape_zero_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_constantofshape_int_zeros_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_constantofshape_int_zeros_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_autopad_same_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_autopad_same_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_strides_and_asymmetric_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_strides_and_asymmetric_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_strides_no_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_strides_no_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_conv_with_strides_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_conv_with_strides_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convinteger_with_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_convinteger_with_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convinteger_without_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_convinteger_without_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_autopad_same_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_convtranspose_autopad_same_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_dilations_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_dilations_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_kernel_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_kernel_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_output_shape_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_convtranspose_output_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_convtranspose_with_kernel_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_convtranspose_with_kernel_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cos_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cos_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cos_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cosh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cosh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cosh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cosh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_exclusive_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cumsum_1d_exclusive_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_reverse_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cumsum_1d_reverse_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_1d_reverse_exclusive_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_cumsum_1d_reverse_exclusive_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_2d_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_2d_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_2d_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_2d_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_cumsum_2d_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_cumsum_2d_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_crd_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_depthtospace_crd_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_crd_mode_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_depthtospace_crd_mode_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_dcr_mode_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_depthtospace_dcr_mode_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_depthtospace_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_depthtospace_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dequantizelinear_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dequantizelinear_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dequantizelinear_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dequantizelinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_det_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_det_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_det_nd_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_det_nd_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_div_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_div_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_mask_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_mask_ratio_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_mask_ratio_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_old_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_old_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_default_ratio_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_default_ratio_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dropout_random_old_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_dropout_random_old_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_dynamicquantizelinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_dynamicquantizelinear_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_max_adjusted_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_dynamicquantizelinear_max_adjusted_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_max_adjusted_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_dynamicquantizelinear_max_adjusted_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_min_adjusted_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_dynamicquantizelinear_min_adjusted_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_dynamicquantizelinear_min_adjusted_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_dynamicquantizelinear_min_adjusted_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_edge_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_edge_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_batch_diagonal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_batch_diagonal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_batch_matmul_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_batch_matmul_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_inner_prod_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_einsum_inner_prod_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_einsum_transpose_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_einsum_transpose_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_elu_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_elu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_elu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_elu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_elu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_elu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_equal_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_equal_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_equal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_equal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_erf_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_erf_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_exp_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_exp_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_exp_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_exp_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_dim_changed_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_expand_dim_changed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_dim_unchanged_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_expand_dim_unchanged_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_eyelike_populate_off_main_diagonal_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_eyelike_populate_off_main_diagonal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_eyelike_with_dtype_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_eyelike_with_dtype_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_eyelike_without_dtype_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_eyelike_without_dtype_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_axis3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_axis3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_flatten_negative_axis4_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_flatten_negative_axis4_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_floor_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_floor_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_floor_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_floor_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_2d_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_2d_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_elements_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_elements_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_elements_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_elements_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_elements_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gather_elements_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gather_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gather_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gathernd_example_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gathernd_example_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gathernd_example_int32_batch_dim1_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gathernd_example_int32_batch_dim1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gathernd_example_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gathernd_example_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_all_attributes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_all_attributes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_alpha_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_alpha_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_beta_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_beta_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_matrix_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_matrix_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_no_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_no_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_scalar_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_scalar_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_single_elem_vector_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_single_elem_vector_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_vector_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_vector_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_default_zero_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_default_zero_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_transposeA_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_transposeA_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gemm_transposeB_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_gemm_transposeB_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalaveragepool_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_globalaveragepool_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalaveragepool_precomputed_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_globalaveragepool_precomputed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalmaxpool_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_globalmaxpool_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_globalmaxpool_precomputed_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_globalmaxpool_precomputed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_bcast_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_bcast_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_greater_equal_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_greater_equal_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_aligncorners_true_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_aligncorners_true_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_bicubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_bicubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_bilinear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_bilinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_border_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_border_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_reflection_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_reflection_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gridsample_zeros_padding_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gridsample_zeros_padding_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_batchwise_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gru_batchwise_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_defaults_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gru_defaults_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_seq_length_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gru_seq_length_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gru_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_gru_with_initial_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardmax_one_hot_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardmax_one_hot_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardsigmoid_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardsigmoid_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardsigmoid_default_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardsigmoid_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardsigmoid_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardsigmoid_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardswish_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardswish_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_hardswish_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_hardswish_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_identity_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_identity_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_identity_opt_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_identity_opt_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_identity_sequence_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_identity_sequence_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_if_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_if_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_if_opt_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_if_opt_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_if_seq_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_if_seq_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_instancenorm_epsilon_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_instancenorm_epsilon_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_instancenorm_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_instancenorm_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isinf_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_isinf_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isinf_negative_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_isinf_negative_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isinf_positive_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_isinf_positive_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_isnan_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_isnan_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_leakyrelu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_leakyrelu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_leakyrelu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_leakyrelu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_leakyrelu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_leakyrelu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_bcast_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_bcast_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_less_equal_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_less_equal_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_log_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_log_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_0_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_0_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_axis_2_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_axis_2_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_default_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_default_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_example_1_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_example_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_example_1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_example_1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_large_number_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_large_number_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_large_number_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_large_number_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_logsoftmax_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_logsoftmax_negative_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_logsoftmax_negative_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_loop11_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_loop11_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_loop13_seq_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_loop13_seq_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_loop16_seq_none_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_loop16_seq_none_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lrn_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lrn_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lrn_default_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lrn_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_batchwise_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lstm_batchwise_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_defaults_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lstm_defaults_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lstm_with_initial_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_lstm_with_peepholes_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_lstm_with_peepholes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmul_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmul_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmul_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmul_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmul_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_matmul_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_matmulinteger_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_matmulinteger_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_max_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_float16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_float16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_float64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_float64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_int16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_int8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_int8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_max_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_max_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_max_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_1d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_1d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_ceil_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_ceil_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_dilations_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_dilations_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_precomputed_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_precomputed_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_precomputed_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_precomputed_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_precomputed_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_precomputed_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_same_lower_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_same_lower_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_same_upper_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_same_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_2d_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_2d_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_maxpool_2d_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_3d_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_3d_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_with_argmax_2d_precomputed_pads_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_with_argmax_2d_precomputed_pads_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxpool_with_argmax_2d_precomputed_strides_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_maxpool_with_argmax_2d_precomputed_strides_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxunpool_export_with_output_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_maxunpool_export_with_output_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_maxunpool_export_without_output_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_maxunpool_export_without_output_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mean_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mean_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mean_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mean_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mean_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mean_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_min_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_float16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_float16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_float64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_float64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_int16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_int8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_int8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_min_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_min_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_min_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_broadcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_broadcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_int64_fmod_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_mod_int64_fmod_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_float16_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_mod_mixed_sign_float16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_float32_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_mod_mixed_sign_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_float64_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_mod_mixed_sign_float64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_mixed_sign_int8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_mixed_sign_int8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint16_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint16_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mod_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mod_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_momentum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_momentum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_momentum_multiple_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_momentum_multiple_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mul_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mul_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mvn_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_mvn_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_mvn_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_mvn_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_neg_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_neg_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nesterov_momentum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nesterov_momentum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NC_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NC_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NC_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NC_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_mean_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1_mean_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_mean_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_mean_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1_weight_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1_weight_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_no_weight_reduction_mean_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_mean_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_reduction_mean_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_reduction_mean_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_reduction_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_reduction_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_reduction_sum_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_with_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_mean_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_with_weight_reduction_mean_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_mean_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_with_weight_reduction_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_sum_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2_with_weight_reduction_sum_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3_none_no_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_sum_weight_high_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2d3_sum_weight_high_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3_sum_weight_high_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3_sum_weight_high_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_mean_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2d3d4d5_mean_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3d4d5_mean_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_none_no_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nllloss_NCd1d2d3d4d5_none_no_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nllloss_NCd1d2d3d4d5_none_no_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_nllloss_NCd1d2d3d4d5_none_no_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_center_point_box_format_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_center_point_box_format_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_flipped_coordinates_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_flipped_coordinates_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_identical_boxes_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_identical_boxes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_limit_output_size_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_limit_output_size_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_single_box_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_single_box_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_suppress_by_IOU_and_scores_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_suppress_by_IOU_and_scores_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_suppress_by_IOU_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_suppress_by_IOU_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_two_batches_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_two_batches_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonmaxsuppression_two_classes_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonmaxsuppression_two_classes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_nonzero_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_nonzero_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_not_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_not_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_not_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_not_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_not_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_not_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_onehot_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_onehot_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_with_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_onehot_with_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_onehot_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_onehot_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_get_element_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_get_element_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_get_element_sequence_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_get_element_sequence_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_has_element_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_has_element_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_optional_has_element_empty_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_optional_has_element_empty_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast3v1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast3v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast4v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast4v3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_or_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_or_bcast4v4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_bcast_array_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_bcast_array_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_bcast_scalar_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_bcast_scalar_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float32_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float32_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_uint32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float32_uint32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float32_uint64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float32_uint64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_float_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_float_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int32_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int32_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int32_int32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int32_int32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int64_float32_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int64_float32_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int64_int64_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int64_int64_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_pow_types_int_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_pow_types_int_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_prelu_broadcast_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_prelu_broadcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_prelu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_prelu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_qlinearconv_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_qlinearconv_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_qlinearmatmul_2D_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_qlinearmatmul_2D_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_qlinearmatmul_3D_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_qlinearmatmul_3D_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_quantizelinear_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_quantizelinear_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_quantizelinear_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_quantizelinear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_float_type_positive_delta_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_range_float_type_positive_delta_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_float_type_positive_delta_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_range_float_type_positive_delta_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_int32_type_negative_delta_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_range_int32_type_negative_delta_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_range_int32_type_negative_delta_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_range_int32_type_negative_delta_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reciprocal_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reciprocal_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reciprocal_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reciprocal_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_negative_axes_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_negative_axes_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l1_negative_axes_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l1_negative_axes_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_negative_axes_keep_dims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_negative_axes_keep_dims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_l2_negative_axes_keep_dims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_l2_negative_axes_keep_dims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_asc_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_log_sum_asc_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_log_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_default_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_log_sum_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_desc_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_log_sum_desc_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_exp_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_log_sum_exp_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_log_sum_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_log_sum_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_default_axes_keepdim_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_default_axes_keepdim_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_max_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_max_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_mean_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_mean_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_min_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_min_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_prod_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_prod_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_empty_axes_input_noop_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_empty_axes_input_noop_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_empty_axes_input_noop_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_empty_axes_input_noop_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reduce_sum_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_default_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_default_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_do_not_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_do_not_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_do_not_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_do_not_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_negative_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_negative_axes_keepdims_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reduce_sum_square_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reduce_sum_square_negative_axes_keepdims_random_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reflect_pad_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reflect_pad_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_relu_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_relu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_allowzero_reordered_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reshape_allowzero_reordered_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_extended_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_extended_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_negative_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_negative_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_negative_extended_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_negative_extended_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_one_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_one_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_reduced_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_reduced_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_reordered_all_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_reordered_all_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_reordered_last_dims_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_reordered_last_dims_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_zero_and_negative_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_zero_and_negative_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reshape_zero_dim_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_reshape_zero_dim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_cubic_A_n0p5_exclude_outside_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_cubic_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_linear_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_linear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_scales_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_linear_pytorch_half_pixel_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_linear_pytorch_half_pixel_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_tf_crop_and_resize_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_tf_crop_and_resize_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_A_n0p5_exclude_outside_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_asymmetric_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_asymmetric_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_linear_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_linear_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_scales_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_cubic_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_ceil_half_pixel_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_ceil_half_pixel_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_floor_align_corners_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_floor_align_corners_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reversesequence_batch_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reversesequence_batch_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_reversesequence_time_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_reversesequence_time_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_rnn_seq_length_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_rnn_seq_length_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_roialign_aligned_false_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_roialign_aligned_false_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_roialign_aligned_true_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_roialign_aligned_true_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_round_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_round_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scan9_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scan9_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scan_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scan_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_elements_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_with_duplicate_indices_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_scatter_elements_with_duplicate_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_with_negative_indices_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_scatter_elements_with_negative_indices_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_elements_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatter_elements_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatter_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatter_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatter_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatternd_add_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatternd_add_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatternd_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatternd_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_scatternd_multiply_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_scatternd_multiply_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1_mean_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1_mean_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1_mean_weight_negative_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1_mean_weight_negative_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1_mean_weight_negative_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_none_no_weight_negative_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_none_no_weight_negative_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_sum_weight_high_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_sum_weight_high_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_mean_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_mean_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_mean_weight_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_none_no_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_none_no_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_3d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_3d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_3d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_3d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_3d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_3d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_4d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_4d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_4d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_4d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_no_weight_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_no_weight_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_3d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_3d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_3d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_4d_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_4d_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_4d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_4d_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_ii_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_mean_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_mean_weight_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_weights_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_weights_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_weights_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_none_weights_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_none_weights_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_sum_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_sum_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_log_prob_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_sum_log_prob_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sce_sum_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_sce_sum_log_prob_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_selu_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_selu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_selu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_selu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_selu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_selu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_insert_at_back_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sequence_insert_at_back_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_insert_at_front_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sequence_insert_at_front_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_clip_end_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_clip_end_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_clip_start_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_clip_start_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_end_1_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_shape_end_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_end_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_shape_end_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_shape_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_1_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_shape_start_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_1_end_2_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_shape_start_1_end_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_1_end_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_shape_start_1_end_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shape_start_negative_1_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_shape_start_negative_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shrink_hard_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_shrink_hard_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shrink_soft_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_shrink_soft_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sigmoid_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sigmoid_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sigmoid_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sigmoid_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sign_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sign_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_simple_rnn_batchwise_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_simple_rnn_batchwise_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_simple_rnn_defaults_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_simple_rnn_defaults_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_simple_rnn_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_simple_rnn_with_initial_bias_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sin_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sin_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sin_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sin_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sinh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sinh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sinh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sinh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_size_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_size_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_size_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_size_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_default_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_default_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_default_steps_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_default_steps_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_end_out_of_bounds_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_end_out_of_bounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_neg_steps_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_neg_steps_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_slice_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_slice_start_out_of_bounds_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_slice_start_out_of_bounds_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softmax_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_0_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_0_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_1_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_1_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softmax_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_axis_2_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_axis_2_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_softmax_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_default_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_default_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_example_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_example_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_large_number_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_large_number_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_large_number_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_large_number_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softmax_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_negative_axis_expanded_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_softmax_negative_axis_expanded_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softplus_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softplus_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softplus_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softplus_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softsign_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softsign_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softsign_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_softsign_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_spacetodepth_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_spacetodepth_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_spacetodepth_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_spacetodepth_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_equal_parts_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_equal_parts_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_equal_parts_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_equal_parts_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_equal_parts_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_equal_parts_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_variable_parts_1d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_variable_parts_1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_variable_parts_2d_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_variable_parts_2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_variable_parts_default_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_split_variable_parts_default_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_split_zero_size_splits_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_split_zero_size_splits_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sqrt_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sqrt_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sqrt_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sqrt_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_squeeze_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_squeeze_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_squeeze_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_squeeze_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_casesensintive_lower_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_strnormalizer_export_monday_casesensintive_lower_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_casesensintive_nochangecase_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_strnormalizer_export_monday_casesensintive_nochangecase_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_casesensintive_upper_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_strnormalizer_export_monday_casesensintive_upper_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_empty_output_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_strnormalizer_export_monday_empty_output_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_export_monday_insensintive_upper_twodim_cpu (__main__.OnnxBackendNodeModelTest) ... FAIL
    test_strnormalizer_export_monday_insensintive_upper_twodim_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnormalizer_nostopwords_nochangecase_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_strnormalizer_nostopwords_nochangecase_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_bcast_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_bcast_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sub_uint8_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sub_uint8_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sum_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sum_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sum_one_input_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sum_one_input_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sum_two_inputs_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_sum_two_inputs_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tan_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tan_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tan_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tan_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tanh_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tanh_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tanh_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tanh_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_batch_onlybigrams_skip0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_batch_onlybigrams_skip0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_batch_onlybigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_batch_onlybigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_batch_uniandbigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_tfidfvectorizer_tf_batch_uniandbigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_only_bigrams_skip0_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tfidfvectorizer_tf_only_bigrams_skip0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_onlybigrams_levelempty_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tfidfvectorizer_tf_onlybigrams_levelempty_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_onlybigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tfidfvectorizer_tf_onlybigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tfidfvectorizer_tf_uniandbigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tfidfvectorizer_tf_uniandbigrams_skip5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_thresholdedrelu_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_thresholdedrelu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_thresholdedrelu_default_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_thresholdedrelu_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_thresholdedrelu_example_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_thresholdedrelu_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tile_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tile_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tile_precomputed_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tile_precomputed_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_top_k_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_top_k_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_top_k_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_top_k_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_top_k_smallest_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_top_k_smallest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_default_mask_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_default_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_mask_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_zero_ratio_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_zero_ratio_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_training_dropout_zero_ratio_mask_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_training_dropout_zero_ratio_mask_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_4_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_4_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_all_permutations_5_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_all_permutations_5_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_transpose_default_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_transpose_default_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_one_row_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_one_row_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_out_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_out_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_out_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_out_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_square_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_square_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_square_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_square_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_tril_zero_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_tril_zero_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_one_row_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_one_row_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_out_neg_out_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_out_neg_out_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_out_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_out_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_pos_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_pos_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_square_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_square_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_square_neg_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_square_neg_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_triu_zero_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_triu_zero_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_not_sorted_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_unique_not_sorted_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_with_axis_3d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_unique_sorted_with_axis_3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_with_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_unique_sorted_with_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_with_negative_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_unique_sorted_with_negative_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unique_sorted_without_axis_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_unique_sorted_without_axis_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_0_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_0_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_1_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_1_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_2_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_2_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_axis_3_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_axis_3_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_negative_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_negative_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_three_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_three_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_two_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_two_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_unsqueeze_unsorted_axes_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_unsqueeze_unsorted_axes_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_upsample_nearest_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_upsample_nearest_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_where_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_where_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_where_long_example_cpu (__main__.OnnxBackendNodeModelTest) ... ok
    test_where_long_example_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor2d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor3d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor4d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor_bcast3v1d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor_bcast3v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor_bcast4v2d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor_bcast4v3d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_xor_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest) ... ERROR
    test_xor_bcast4v4d_cuda (__main__.OnnxBackendNodeModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool1d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool2d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool2d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool3d_stride1_pad0_gpu_input_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool3d_stride1_pad0_gpu_input_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_AvgPool3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_AvgPool3d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm1d_3d_input_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_BatchNorm1d_3d_input_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm2d_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_BatchNorm2d_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm2d_momentum_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_BatchNorm2d_momentum_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm3d_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_BatchNorm3d_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_BatchNorm3d_momentum_eval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_BatchNorm3d_momentum_eval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ConstantPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_ConstantPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_dilated_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_dilated_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_groups_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_groups_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad1_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad1_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad1size1_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad1size1_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad2_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad2_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_pad2size1_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_pad2size1_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv1d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_padded_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_padded_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_strided_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_strided_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_depthwise_with_multiplier_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_depthwise_with_multiplier_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_dilated_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_dilated_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_groups_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_groups_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_groups_thnn_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_groups_thnn_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_padding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_padding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv2d_strided_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv2d_strided_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_dilated_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_dilated_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_dilated_strided_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_dilated_strided_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_groups_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_groups_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Conv3d_stride_padding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Conv3d_stride_padding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ConvTranspose2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... FAIL
    test_ConvTranspose2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ConvTranspose2d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... FAIL
    test_ConvTranspose2d_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ELU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_ELU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Embedding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Embedding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Embedding_sparse_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Embedding_sparse_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_GLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_GLU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_GLU_dim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_GLU_dim_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_LeakyReLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_LeakyReLU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_LeakyReLU_with_negval_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_LeakyReLU_with_negval_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Linear_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Linear_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Linear_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Linear_no_bias_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_LogSoftmax_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_LogSoftmax_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool1d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool1d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool1d_stride_padding_dilation_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool1d_stride_padding_dilation_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool2d_stride_padding_dilation_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool2d_stride_padding_dilation_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool3d_stride_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool3d_stride_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_MaxPool3d_stride_padding_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_MaxPool3d_stride_padding_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_1d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_1d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_1d_multiparam_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_2d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_2d_multiparam_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_3d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PReLU_3d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_PReLU_3d_multiparam_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PixelShuffle_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_PixelShuffle_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_PoissonNLLLLoss_no_reduce_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_PoissonNLLLLoss_no_reduce_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ReLU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_ReLU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ReflectionPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_ReflectionPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ReplicationPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_ReplicationPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_SELU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_SELU_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Sigmoid_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Sigmoid_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softmax_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Softmax_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softmin_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Softmin_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softplus_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_Softplus_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Softsign_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Softsign_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_Tanh_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_Tanh_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_ZeroPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_ZeroPad2d_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_softmax_dim3_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_log_softmax_dim3_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_log_softmax_lastdim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_log_softmax_lastdim_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_functional_dim3_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ERROR
    test_softmax_functional_dim3_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_softmax_lastdim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest) ... ok
    test_softmax_lastdim_cuda (__main__.OnnxBackendPyTorchConvertedModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_add_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_size1_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_add_size1_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_size1_right_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_add_size1_right_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_add_size1_singleton_broadcast_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_add_size1_singleton_broadcast_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_addconstant_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_addconstant_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_addmm_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_addmm_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_basic_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_basic_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_chunk_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_chunk_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_clip_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_clip_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_concat2_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_concat2_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_conv_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_conv_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_convtranspose_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... FAIL
    test_operator_convtranspose_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_exp_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_exp_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_flatten_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_flatten_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_index_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_index_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_max_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_max_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_maxpool_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_maxpool_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_min_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_min_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_mm_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_mm_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_non_float_params_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_non_float_params_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_pad_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_pad_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_params_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_params_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_permute2_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_permute2_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_pow_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... /var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_pow.py:19: RuntimeWarning: invalid value encountered in power
      return (numpy.power(a, b).astype(a.dtype), )
    ok
    test_operator_pow_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_mean_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_mean_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_mean_keepdim_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_mean_keepdim_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_sum_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_sum_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_reduced_sum_keepdim_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_reduced_sum_keepdim_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_repeat_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_repeat_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_repeat_dim_overflow_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_repeat_dim_overflow_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_selu_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_selu_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_sqrt_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... /var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_sqrt.py:22: RuntimeWarning: invalid value encountered in sqrt
      return (numpy.sqrt(x), )
    ok
    test_operator_sqrt_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_symbolic_override_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ERROR
    test_operator_symbolic_override_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_symbolic_override_nested_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_symbolic_override_nested_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_operator_view_cpu (__main__.OnnxBackendPyTorchOperatorModelTest) ... ok
    test_operator_view_cuda (__main__.OnnxBackendPyTorchOperatorModelTest) ... skipped &#39;no matched include pattern&#39;
    test_bvlc_alexnet_cpu (__main__.OnnxBackendRealModelTest) ... ERROR
    test_bvlc_alexnet_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;no matched include pattern&#39;
    test_densenet121_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_densenet121_.*&quot;&#39;
    test_densenet121_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_densenet121_.*&quot;&#39;
    test_inception_v1_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_inception_v1_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_inception_v2_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_inception_v2_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_inception_.*&quot;&#39;
    test_resnet50_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_resnet50_.*&quot;&#39;
    test_resnet50_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_resnet50_.*&quot;&#39;
    test_shufflenet_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_shufflenet_.*&quot;&#39;
    test_shufflenet_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_shufflenet_.*&quot;&#39;
    test_squeezenet_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_squeezenet_.*&quot;&#39;
    test_squeezenet_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_squeezenet_.*&quot;&#39;
    test_vgg19_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_vgg19_.*&quot;&#39;
    test_vgg19_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_vgg19_.*&quot;&#39;
    test_zfnet512_cpu (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_zfnet512_.*&quot;&#39;
    test_zfnet512_cuda (__main__.OnnxBackendRealModelTest) ... skipped &#39;matched exclude pattern &quot;.*_zfnet512_.*&quot;&#39;
    test_expand_shape_model1_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model1_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_shape_model2_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model2_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_shape_model3_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model3_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_expand_shape_model4_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_expand_shape_model4_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gradient_of_add_and_mul_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_gradient_of_add_and_mul_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_gradient_of_add_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_gradient_of_add_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model1_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_sequence_model1_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model2_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_sequence_model2_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model3_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_sequence_model3_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model4_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model4_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model5_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sequence_model5_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model6_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_sequence_model6_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model7_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_sequence_model7_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sequence_model8_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_sequence_model8_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_shrink_cpu (__main__.OnnxBackendSimpleModelTest) ... ERROR
    test_shrink_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_sign_model_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_sign_model_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_single_relu_model_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_single_relu_model_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_casesensintive_lower_cpu (__main__.OnnxBackendSimpleModelTest) ... FAIL
    test_strnorm_model_monday_casesensintive_lower_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_casesensintive_nochangecase_cpu (__main__.OnnxBackendSimpleModelTest) ... FAIL
    test_strnorm_model_monday_casesensintive_nochangecase_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_casesensintive_upper_cpu (__main__.OnnxBackendSimpleModelTest) ... FAIL
    test_strnorm_model_monday_casesensintive_upper_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_empty_output_cpu (__main__.OnnxBackendSimpleModelTest) ... FAIL
    test_strnorm_model_monday_empty_output_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_monday_insensintive_upper_twodim_cpu (__main__.OnnxBackendSimpleModelTest) ... FAIL
    test_strnorm_model_monday_insensintive_upper_twodim_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    test_strnorm_model_nostopwords_nochangecase_cpu (__main__.OnnxBackendSimpleModelTest) ... ok
    test_strnorm_model_nostopwords_nochangecase_cuda (__main__.OnnxBackendSimpleModelTest) ... skipped &#39;no matched include pattern&#39;
    
    ======================================================================
    ERROR: test_adagrad_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Adagrad&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_adagrad_multiple_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Adagrad&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_adam_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Adam&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_adam_multiple_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Adam&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_argmax_negative_axis_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 2-tuple and equal to ShapeObject((2, 2), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMax(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmax_negative_axis_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 2-tuple and equal to ShapeObject((2, 2), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMax(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmax_negative_axis_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 3-tuple and equal to ShapeObject((2, 3, 4), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMax(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmax_negative_axis_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 3-tuple and equal to ShapeObject((2, 3, 4), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMax(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmin_negative_axis_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 2-tuple and equal to ShapeObject((2, 2), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMin(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmin_negative_axis_keepdims_example_select_last_index_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 2-tuple and equal to ShapeObject((2, 2), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMin(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmin_negative_axis_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 3-tuple and equal to ShapeObject((2, 3, 4), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMin(data) -&gt; result
    
    ======================================================================
    ERROR: test_argmin_negative_axis_keepdims_random_select_last_index_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 484, in infer_shapes
        return self._infer_shapes(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 560, in _infer_shapes
        sh = x.reduce(self.axis, self.keepdims,  # pylint: disable=E1101
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 817, in reduce
        raise IndexError(&quot;axis={} is wrong, shape is {}-tuple and equal to &quot;
    IndexError: axis=-1 is wrong, shape is 3-tuple and equal to ShapeObject((2, 3, 4), dtype=numpy.float32, name=&#39;data&#39;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-ArgMin(data) -&gt; result
    
    ======================================================================
    ERROR: test_averagepool_2d_ceil_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_average_pool.py&quot;, line 117, in _run
        raise RuntimeError(
    RuntimeError: ceil_mode != 0, runtime not implemented yet.
    
    ======================================================================
    ERROR: test_basic_convinteger_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ConvInteger&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_batchnorm_epsilon_training_mode_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 335, in _set_shape_inference_runtime
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Mismatch number of outputs got 5 != 3 for names [&#39;y&#39;, &#39;output_mean&#39;, &#39;output_var&#39;] (node=&#39;BatchNormalization_14&#39;).
    {&#39;atts&#39;: {&#39;epsilon&#39;: {...}, &#39;training_mode&#39;: {...}},
     &#39;domain&#39;: &#39;&#39;,
     &#39;name&#39;: &#39;&#39;,
     &#39;op_type&#39;: &#39;BatchNormalization&#39;}
    
    ======================================================================
    ERROR: test_batchnorm_example_training_mode_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 335, in _set_shape_inference_runtime
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Mismatch number of outputs got 5 != 3 for names [&#39;y&#39;, &#39;output_mean&#39;, &#39;output_var&#39;] (node=&#39;BatchNormalization_14&#39;).
    {&#39;atts&#39;: {&#39;training_mode&#39;: {...}},
     &#39;domain&#39;: &#39;&#39;,
     &#39;name&#39;: &#39;&#39;,
     &#39;op_type&#39;: &#39;BatchNormalization&#39;}
    
    ======================================================================
    ERROR: test_bernoulli_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Bernoulli&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bernoulli_double_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Bernoulli&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bernoulli_double_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;RandomUniformLike&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bernoulli_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;RandomUniformLike&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bernoulli_seed_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Bernoulli&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bernoulli_seed_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;RandomUniformLike&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_left_uint16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_left_uint32_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_left_uint64_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_left_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_right_uint16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_right_uint32_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_right_uint64_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bitshift_right_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;BitShift&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_cast_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_cast_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 583, in to_sequence
        outputs[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_BFLOAT16_to_FLOAT_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_BFLOAT16_to_FLOAT_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_FLOAT_to_BFLOAT16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_castlike_FLOAT_to_BFLOAT16_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 337, in _var_as_dict
        elem_type = _elem_type_as_str(t.elem_type)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 280, in _elem_type_as_str
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: elem_type &#39;16&#39; is unknown
    fields:
    [&#39;__abs__&#39;,
     &#39;__add__&#39;,
     &#39;__and__&#39;,
     &#39;__bool__&#39;,
     &#39;__ceil__&#39;,
     &#39;__class__&#39;,
     &#39;__delattr__&#39;,
     &#39;__dir__&#39;,
     &#39;__divmod__&#39;,
     &#39;__doc__&#39;,
     &#39;__eq__&#39;,
     &#39;__float__&#39;,
     &#39;__floor__&#39;,
     &#39;__floordiv__&#39;,
     &#39;__format__&#39;,
     &#39;__ge__&#39;,
     &#39;__getattribute__&#39;,
     &#39;__getnewargs__&#39;,
     &#39;__gt__&#39;,
     &#39;__hash__&#39;,
     &#39;__index__&#39;,
     &#39;__init__&#39;,
     &#39;__init_subclass__&#39;,
     &#39;__int__&#39;,
     &#39;__invert__&#39;,
     &#39;__le__&#39;,
     &#39;__lshift__&#39;,
     &#39;__lt__&#39;,
     &#39;__mod__&#39;,
     &#39;__mul__&#39;,
     &#39;__ne__&#39;,
     &#39;__neg__&#39;,
     &#39;__new__&#39;,
     &#39;__or__&#39;,
     &#39;__pos__&#39;,
     &#39;__pow__&#39;,
     &#39;__radd__&#39;,
     &#39;__rand__&#39;,
     &#39;__rdivmod__&#39;,
     &#39;__reduce__&#39;,
     &#39;__reduce_ex__&#39;,
     &#39;__repr__&#39;,
     &#39;__rfloordiv__&#39;,
     &#39;__rlshift__&#39;,
     &#39;__rmod__&#39;,
     &#39;__rmul__&#39;,
     &#39;__ror__&#39;,
     &#39;__round__&#39;,
     &#39;__rpow__&#39;,
     &#39;__rrshift__&#39;,
     &#39;__rshift__&#39;,
     &#39;__rsub__&#39;,
     &#39;__rtruediv__&#39;,
     &#39;__rxor__&#39;,
     &#39;__setattr__&#39;,
     &#39;__sizeof__&#39;,
     &#39;__str__&#39;,
     &#39;__sub__&#39;,
     &#39;__subclasshook__&#39;,
     &#39;__truediv__&#39;,
     &#39;__trunc__&#39;,
     &#39;__xor__&#39;,
     &#39;as_integer_ratio&#39;,
     &#39;bit_length&#39;,
     &#39;conjugate&#39;,
     &#39;denominator&#39;,
     &#39;from_bytes&#39;,
     &#39;imag&#39;,
     &#39;numerator&#39;,
     &#39;real&#39;,
     &#39;to_bytes&#39;]
    -----
    &lt;class &#39;int&#39;&gt;.
    
    ======================================================================
    ERROR: test_celu_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Elu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_clip_default_inbounds_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , ) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , ) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;float32&#39;) shape=(3,)
    Clip(x, , ) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;float32&#39;) shape=(3,)
    ---
    
    
    ======================================================================
    ERROR: test_clip_default_int8_inbounds_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3,)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , ) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , ) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;int8&#39;) shape=(3,)
    Clip(x, , ) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;int8&#39;) shape=(3,)
    ---
    
    
    ======================================================================
    ERROR: test_clip_default_int8_max_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;max&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;max&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (&#39;?&#39;,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int8&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , max) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    ((&#39;max&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , max) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;int8&#39;) shape=(3, 4, 5)
    input: name=&#39;max&#39; type=dtype(&#39;int8&#39;) shape=()
    Clip(x, , max) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;int8&#39;) shape=(3, 4, 5)
    ---
    
    
    ======================================================================
    ERROR: test_clip_default_max_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;max&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;max&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (&#39;?&#39;,)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (3, 4, 5)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Clip(x, , max) -&gt; y))
    --order--
    ((&#39;x&#39;, 0), 0)
    ((&#39;max&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Clip(x, , max) -&gt; y)
    --ONNX--
    opset: domain=&#39;&#39; version=12
    input: name=&#39;x&#39; type=dtype(&#39;float32&#39;) shape=(3, 4, 5)
    input: name=&#39;max&#39; type=dtype(&#39;float32&#39;) shape=()
    Clip(x, , max) -&gt; y
    output: name=&#39;y&#39; type=dtype(&#39;float32&#39;) shape=(3, 4, 5)
    ---
    
    
    ======================================================================
    ERROR: test_constantofshape_int_shape_zero_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;y&quot;
    type {
      tensor_type {
        elem_type: 6
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_convinteger_with_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ConvInteger&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_convinteger_without_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ConvInteger&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_cumsum_1d_exclusive_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_cum_sum.py&quot;, line 41, in _run
        raise NotImplementedError(
    NotImplementedError: reverse=1 or exclusive=1 not implemented
    
    ======================================================================
    ERROR: test_cumsum_1d_reverse_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_cum_sum.py&quot;, line 41, in _run
        raise NotImplementedError(
    NotImplementedError: reverse=1 or exclusive=1 not implemented
    
    ======================================================================
    ERROR: test_cumsum_1d_reverse_exclusive_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_cum_sum.py&quot;, line 41, in _run
        raise NotImplementedError(
    NotImplementedError: reverse=1 or exclusive=1 not implemented
    
    ======================================================================
    ERROR: test_depthtospace_crd_mode_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DepthToSpace&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_depthtospace_crd_mode_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DepthToSpace&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_depthtospace_dcr_mode_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DepthToSpace&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_depthtospace_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DepthToSpace&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_dynamicquantizelinear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DynamicQuantizeLinear&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_dynamicquantizelinear_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 57, in _wrapfunc
        return bound(*args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 159, in _clip
        return _clip_dep_invoke_with_casting(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 113, in _clip_dep_invoke_with_casting
        return ufunc(*args, out=out, **kwargs)
    TypeError: return arrays must be of ArrayType
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 60, in run
        res = self._run(x, *minmax)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 69, in _run
        return self._run_inplace(data, *minmax)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 80, in _run_inplace
        res = numpy.clip(data, amin, amax, out=data)
      File &quot;&lt;__array_function__ internals&gt;&quot;, line 5, in clip
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 2115, in clip
        return _wrapfunc(a, &#39;clip&#39;, a_min, a_max, out=out, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 66, in _wrapfunc
        return _wrapit(obj, method, *args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 43, in _wrapit
        result = getattr(asarray(obj), method)(*args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 159, in _clip
        return _clip_dep_invoke_with_casting(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 113, in _clip_dep_invoke_with_casting
        return ufunc(*args, out=out, **kwargs)
    TypeError: return arrays must be of ArrayType
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 62, in run
        raise TypeError(&quot;Issues with types {} (binary operator {}).&quot;.format(
    TypeError: Issues with types &lt;class &#39;numpy.float32&#39;&gt; (binary operator Clip_11).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_clip.Clip_11&#39;&gt;, inputs=[&#39;DynamicQuantizeLinear_test_dynamicquantizelinear_expanded_functionInitial_ZeroPoint_FP&#39;, &#39;DynamicQuantizeLinear_test_dynamicquantizelinear_expanded_functionQ_Min&#39;, &#39;DynamicQuantizeLinear_test_dynamicquantizelinear_expanded_functionQ_Max&#39;].
    
    ======================================================================
    ERROR: test_dynamicquantizelinear_max_adjusted_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DynamicQuantizeLinear&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_dynamicquantizelinear_max_adjusted_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 57, in _wrapfunc
        return bound(*args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 159, in _clip
        return _clip_dep_invoke_with_casting(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 113, in _clip_dep_invoke_with_casting
        return ufunc(*args, out=out, **kwargs)
    TypeError: return arrays must be of ArrayType
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 60, in run
        res = self._run(x, *minmax)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 69, in _run
        return self._run_inplace(data, *minmax)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 80, in _run_inplace
        res = numpy.clip(data, amin, amax, out=data)
      File &quot;&lt;__array_function__ internals&gt;&quot;, line 5, in clip
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 2115, in clip
        return _wrapfunc(a, &#39;clip&#39;, a_min, a_max, out=out, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 66, in _wrapfunc
        return _wrapit(obj, method, *args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 43, in _wrapit
        result = getattr(asarray(obj), method)(*args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 159, in _clip
        return _clip_dep_invoke_with_casting(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 113, in _clip_dep_invoke_with_casting
        return ufunc(*args, out=out, **kwargs)
    TypeError: return arrays must be of ArrayType
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 62, in run
        raise TypeError(&quot;Issues with types {} (binary operator {}).&quot;.format(
    TypeError: Issues with types &lt;class &#39;numpy.float32&#39;&gt; (binary operator Clip_11).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_clip.Clip_11&#39;&gt;, inputs=[&#39;DynamicQuantizeLinear_test_dynamicquantizelinear_max_adjusted_expanded_functionInitial_ZeroPoint_FP&#39;, &#39;DynamicQuantizeLinear_test_dynamicquantizelinear_max_adjusted_expanded_functionQ_Min&#39;, &#39;DynamicQuantizeLinear_test_dynamicquantizelinear_max_adjusted_expanded_functionQ_Max&#39;].
    
    ======================================================================
    ERROR: test_dynamicquantizelinear_min_adjusted_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;DynamicQuantizeLinear&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_dynamicquantizelinear_min_adjusted_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 57, in _wrapfunc
        return bound(*args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 159, in _clip
        return _clip_dep_invoke_with_casting(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 113, in _clip_dep_invoke_with_casting
        return ufunc(*args, out=out, **kwargs)
    TypeError: return arrays must be of ArrayType
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 60, in run
        res = self._run(x, *minmax)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 69, in _run
        return self._run_inplace(data, *minmax)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 80, in _run_inplace
        res = numpy.clip(data, amin, amax, out=data)
      File &quot;&lt;__array_function__ internals&gt;&quot;, line 5, in clip
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 2115, in clip
        return _wrapfunc(a, &#39;clip&#39;, a_min, a_max, out=out, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 66, in _wrapfunc
        return _wrapit(obj, method, *args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 43, in _wrapit
        result = getattr(asarray(obj), method)(*args, **kwds)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 159, in _clip
        return _clip_dep_invoke_with_casting(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py&quot;, line 113, in _clip_dep_invoke_with_casting
        return ufunc(*args, out=out, **kwargs)
    TypeError: return arrays must be of ArrayType
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_clip.py&quot;, line 62, in run
        raise TypeError(&quot;Issues with types {} (binary operator {}).&quot;.format(
    TypeError: Issues with types &lt;class &#39;numpy.float32&#39;&gt; (binary operator Clip_11).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_clip.Clip_11&#39;&gt;, inputs=[&#39;DynamicQuantizeLinear_test_dynamicquantizelinear_min_adjusted_expanded_functionInitial_ZeroPoint_FP&#39;, &#39;DynamicQuantizeLinear_test_dynamicquantizelinear_min_adjusted_expanded_functionQ_Min&#39;, &#39;DynamicQuantizeLinear_test_dynamicquantizelinear_min_adjusted_expanded_functionQ_Max&#39;].
    
    ======================================================================
    ERROR: test_einsum_inner_prod_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_einsum.py&quot;, line 38, in _infer_shapes
        return (ShapeObject.einsum_shape(self.equation, *args), )
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 1114, in einsum_shape
        inp, out = [_.strip() for _ in equation.split(b&quot;-&gt;&quot;)]
    ValueError: not enough values to unpack (expected 2, got 1)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 2 arguments for class &#39;Einsum&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_einsum.Einsum object at 0x7f11803c6f70&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Einsum(x, y) -&gt; z
    
    ======================================================================
    ERROR: test_elu_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Elu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_elu_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Elu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_elu_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Elu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_eyelike_populate_off_main_diagonal_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_eyelike.py&quot;, line 26, in _run
        return (numpy.eye(*shape, k=self.k, dtype=self.dtype_), )
    TypeError: eye() got multiple values for argument &#39;k&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 192, in run
        raise TypeError(  # pragma: no cover
    TypeError: Issues with types &lt;class &#39;numpy.ndarray&#39;&gt; (operator EyeLike).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_eyelike.EyeLike&#39;&gt;, inputs=[&#39;x&#39;].
    
    ======================================================================
    ERROR: test_eyelike_with_dtype_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_eyelike.py&quot;, line 26, in _run
        return (numpy.eye(*shape, k=self.k, dtype=self.dtype_), )
    TypeError: eye() got multiple values for argument &#39;k&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 192, in run
        raise TypeError(  # pragma: no cover
    TypeError: Issues with types &lt;class &#39;numpy.ndarray&#39;&gt; (operator EyeLike).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_eyelike.EyeLike&#39;&gt;, inputs=[&#39;x&#39;].
    
    ======================================================================
    ERROR: test_eyelike_without_dtype_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_eyelike.py&quot;, line 26, in _run
        return (numpy.eye(*shape, k=self.k, dtype=self.dtype_), )
    TypeError: eye() got multiple values for argument &#39;k&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 192, in run
        raise TypeError(  # pragma: no cover
    TypeError: Issues with types &lt;class &#39;numpy.ndarray&#39;&gt; (operator EyeLike).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_eyelike.EyeLike&#39;&gt;, inputs=[&#39;x&#39;].
    
    ======================================================================
    ERROR: test_gather_elements_negative_indices_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_gather_elements.py&quot;, line 54, in gather_numpy
        gathered = numpy.choose(index_swaped, data_swaped)
      File &quot;&lt;__array_function__ internals&gt;&quot;, line 5, in choose
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 429, in choose
        return _wrapfunc(a, &#39;choose&#39;, choices, out=out, mode=mode)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py&quot;, line 57, in _wrapfunc
        return bound(*args, **kwds)
    ValueError: invalid entry in choice array
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_gather_elements.py&quot;, line 75, in _run
        y = gather_numpy(data, self.axis, indices)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_gather_elements.py&quot;, line 57, in gather_numpy
        return gather_numpy_2(self, dim, index)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_gather_elements.py&quot;, line 18, in gather_numpy_2
        res = numpy.array(
    ValueError: cannot reshape array of size 2 into shape (2,3)
    
    ======================================================================
    ERROR: test_gathernd_example_float32_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GatherND&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gathernd_example_int32_batch_dim1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GatherND&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gathernd_example_int32_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GatherND&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_globalmaxpool_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GlobalMaxPool&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_globalmaxpool_precomputed_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GlobalMaxPool&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_aligncorners_true_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_bicubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_bilinear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_border_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_reflection_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gridsample_zeros_padding_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GridSample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gru_batchwise_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GRU&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gru_defaults_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GRU&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gru_seq_length_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GRU&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gru_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;GRU&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardmax_one_hot_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Hardmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardsigmoid_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;HardSigmoid&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardsigmoid_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;HardSigmoid&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardsigmoid_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;HardSigmoid&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardswish_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;HardSwish&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_hardswish_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;HardSigmoid&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_identity_opt_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1362, in _set_shape_inference_runtime
        values[k] = ShapeObject(v, use_n1=True, name=k)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 660, in __init__
        _dtype_again()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/shape_object.py&quot;, line 657, in _dtype_again
        raise ValueError(  # pragma: no cover
    ValueError: dtype has an unexpected value: &#39;unk&#39;.
    
    ======================================================================
    ERROR: test_if_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_if.py&quot;, line 65, in _run
        if all(cond):
    TypeError: iteration over a 0-d array
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 192, in run
        raise TypeError(  # pragma: no cover
    TypeError: Issues with types &lt;class &#39;numpy.ndarray&#39;&gt; (operator If).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_if.If&#39;&gt;, inputs=[&#39;cond&#39;].
    
    ======================================================================
    ERROR: test_if_opt_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 202, in preprocess_parameters
        sess = rt_class(v[&#39;value&#39;], runtime=runtime,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Optional&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 122, in setup_runtime
        self.preprocess_parameters(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 208, in preprocess_parameters
        raise RuntimeError(
    RuntimeError: Unable to instantiate a node of type &#39;If&#39; and name &#39;&#39;.
    
    ======================================================================
    ERROR: test_if_seq_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_if.py&quot;, line 65, in _run
        if all(cond):
    TypeError: iteration over a 0-d array
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 192, in run
        raise TypeError(  # pragma: no cover
    TypeError: Issues with types &lt;class &#39;numpy.ndarray&#39;&gt; (operator If).
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 253, in run
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unable to run operator &lt;class &#39;mlprodict.onnxrt.ops_cpu.op_if.If&#39;&gt;, inputs=[&#39;cond&#39;].
    
    ======================================================================
    ERROR: test_instancenorm_epsilon_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;InstanceNormalization&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_instancenorm_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;InstanceNormalization&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_isinf_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;IsInf&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_isinf_negative_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;IsInf&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_isinf_positive_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;IsInf&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_axis_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_example_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_large_number_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_logsoftmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_loop16_seq_none_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 202, in preprocess_parameters
        sess = rt_class(v[&#39;value&#39;], runtime=runtime,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OptionalHasElement&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 122, in setup_runtime
        self.preprocess_parameters(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 208, in preprocess_parameters
        raise RuntimeError(
    RuntimeError: Unable to instantiate a node of type &#39;Loop&#39; and name &#39;&#39;.
    
    ======================================================================
    ERROR: test_lrn_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LRN&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_lrn_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LRN&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_lstm_batchwise_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LSTM&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_lstm_defaults_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LSTM&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_lstm_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LSTM&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_lstm_with_peepholes_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LSTM&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_matmulinteger_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;MatMulInteger&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_max_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() takes 3 positional arguments but 4 were given
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Max&#39;) and shapes
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;)
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_1&#39;)
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_2&#39;)
    ----args
    (ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;),
     ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_1&#39;),
     ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_2&#39;))
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 3 arguments for class &#39;Max&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_max.Max object at 0x7f11954d6a90&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Max(data_0, data_1, data_2) -&gt; result
    
    ======================================================================
    ERROR: test_max_one_input_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;y&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Max&#39;) and shapes
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;)
    ----args
    (ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Max&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_max.Max object at 0x7f11803c6b50&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Max(data_0) -&gt; result
    
    ======================================================================
    ERROR: test_maxunpool_export_with_output_shape_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;MaxUnpool&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_maxunpool_export_without_output_shape_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;MaxUnpool&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_min_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() takes 3 positional arguments but 4 were given
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Min&#39;) and shapes
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;)
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_1&#39;)
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_2&#39;)
    ----args
    (ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;),
     ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_1&#39;),
     ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_2&#39;))
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 3 arguments for class &#39;Min&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_min.Min object at 0x7f11803c6ee0&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Min(data_0, data_1, data_2) -&gt; result
    
    ======================================================================
    ERROR: test_min_one_input_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;y&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Min&#39;) and shapes
    ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;)
    ----args
    (ShapeObject((3, ), dtype=numpy.float32, name=&#39;data_0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Min&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_min.Min object at 0x7f11803658b0&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Min(data_0) -&gt; result
    
    ======================================================================
    ERROR: test_momentum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Momentum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_momentum_multiple_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Momentum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_mvn_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;MeanVarianceNormalization&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nesterov_momentum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Momentum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NC_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1_mean_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1_weight_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_no_weight_reduction_mean_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_reduction_mean_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_reduction_sum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_with_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_with_weight_reduction_mean_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_with_weight_reduction_sum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2_with_weight_reduction_sum_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2d3_none_no_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2d3_sum_weight_high_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2d3d4d5_mean_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nllloss_NCd1d2d3d4d5_none_no_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NegativeLogLikelihoodLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_center_point_box_format_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_flipped_coordinates_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_identical_boxes_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_limit_output_size_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_single_box_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_suppress_by_IOU_and_scores_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_suppress_by_IOU_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_two_batches_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonmaxsuppression_two_classes_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonMaxSuppression&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_nonzero_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;NonZero&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_onehot_negative_indices_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OneHot&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_onehot_with_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OneHot&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_onehot_with_negative_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OneHot&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_onehot_without_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OneHot&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_optional_get_element_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OptionalGetElement&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_optional_get_element_sequence_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OptionalGetElement&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_optional_has_element_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OptionalHasElement&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_optional_has_element_empty_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;OptionalHasElement&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_prelu_broadcast_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_prelu_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_qlinearmatmul_2D_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;QLinearMatMul&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_qlinearmatmul_3D_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;QLinearMatMul&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_range_float_type_positive_delta_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 202, in preprocess_parameters
        sess = rt_class(v[&#39;value&#39;], runtime=runtime,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 439, in _var_as_dict
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: Unable to guess which object it is type is &lt;class &#39;onnx.onnx_ml_pb2.ValueInfoProto&#39;&gt; value is name: &quot;prev&quot;
    .
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 122, in setup_runtime
        self.preprocess_parameters(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 208, in preprocess_parameters
        raise RuntimeError(
    RuntimeError: Unable to instantiate a node of type &#39;Loop&#39; and name &#39;&#39;.
    
    ======================================================================
    ERROR: test_range_int32_type_negative_delta_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 202, in preprocess_parameters
        sess = rt_class(v[&#39;value&#39;], runtime=runtime,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 573, in to_sequence
        variables[obj.name] = _var_as_dict(obj)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnx_tools/onnx2py_helper.py&quot;, line 439, in _var_as_dict
        raise NotImplementedError(  # pragma: no cover
    NotImplementedError: Unable to guess which object it is type is &lt;class &#39;onnx.onnx_ml_pb2.ValueInfoProto&#39;&gt; value is name: &quot;prev&quot;
    .
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 122, in setup_runtime
        self.preprocess_parameters(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 208, in preprocess_parameters
        raise RuntimeError(
    RuntimeError: Unable to instantiate a node of type &#39;Loop&#39; and name &#39;&#39;.
    
    ======================================================================
    ERROR: test_reduce_log_sum_asc_axes_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReduceLogSum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_reduce_log_sum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReduceLogSum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_reduce_log_sum_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReduceLogSum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_reduce_log_sum_desc_axes_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReduceLogSum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_reduce_log_sum_negative_axes_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReduceLogSum&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_reduce_sum_default_axes_keepdims_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_default_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_empty_axes_input_noop_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_empty_axes_input_noop_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reduce_sum_negative_axes_keepdims_random_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;axes&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_reshape_allowzero_reordered_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;data&quot;
    type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 3
          }
          dim {
            dim_value: 4
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_resize_downsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales, cubic_coeff_a=-0.50, exclude_outside=1) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 2)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 2)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 2)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_linear_pytorch_half_pixel_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 1)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 1)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 1, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 1, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Resize&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_resize_tf_crop_and_resize_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;roi&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;roi&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (8,)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 3, 3)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, roi, , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;roi&#39;, 0), 1)
    ((&#39;sizes&#39;, 0), 2)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, roi, , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;roi&#39; type=dtype(&#39;float32&#39;) shape=(8,)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, roi, , sizes, extrapolation_value=10.00) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 3, 3)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_A_n0p5_exclude_outside_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales, cubic_coeff_a=-0.50, exclude_outside=1) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_asymmetric_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_linear_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_linear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_scales_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;scales&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;scales&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 6)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , scales) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;scales&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , scales) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;scales&#39; type=dtype(&#39;float32&#39;) shape=(4,)
    Resize(X, , scales) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 6)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_cubic_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 9, 10)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 9, 10)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_ceil_half_pixel_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 2, 2)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 7, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 2, 2)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 7, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_floor_align_corners_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;X&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;X&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 4, 4)}}))
    ((&#39;sizes&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;sizes&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;int64&#39;, &#39;shape&#39;: (4,)}}))
    ((&#39;Y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;Y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 1, 8, 8)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Resize(X, , , sizes) -&gt; Y))
    --order--
    ((&#39;X&#39;, 0), 0)
    ((&#39;sizes&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Resize(X, , , sizes) -&gt; Y)
    --ONNX--
    opset: domain=&#39;&#39; version=13
    input: name=&#39;X&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 4, 4)
    input: name=&#39;sizes&#39; type=dtype(&#39;int64&#39;) shape=(4,)
    Resize(X, , , sizes) -&gt; Y
    output: name=&#39;Y&#39; type=dtype(&#39;float32&#39;) shape=(1, 1, 8, 8)
    ---
    
    
    ======================================================================
    ERROR: test_reversesequence_batch_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReverseSequence&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_reversesequence_time_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ReverseSequence&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_rnn_seq_length_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 113, in load_op
        return cl(onnx_node, desc=desc, **options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 156, in __init__
        CommonRNN.__init__(self, onnx_node, desc=desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 40, in __init__
        self.f1 = self.choose_act(self.activations[0],
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 57, in choose_act
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unknown activation function &#39;tanh&#39;.
    
    ======================================================================
    ERROR: test_roialign_aligned_false_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;RoiAlign&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_roialign_aligned_true_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;RoiAlign&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_scan_sum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 187, in _init
        self.graph_ = self.to_sequence()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 720, in to_sequence
        raise RuntimeError(  # pragma: no cover
    RuntimeError: No runnable nodes was found in the ONNX graph
    --rev--
    ((&#39;initial&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;initial&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 2)}}))
    ((&#39;x&#39;, 0), (&#39;I&#39;, {&#39;name&#39;: &#39;x&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 3, 2)}}))
    ((&#39;y&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;y&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 2)}}))
    ((&#39;z&#39;, 0), (&#39;O&#39;, {&#39;name&#39;: &#39;z&#39;, &#39;type&#39;: {&#39;kind&#39;: &#39;tensor&#39;, &#39;elem&#39;: &#39;float&#39;, &#39;shape&#39;: (1, 3, 2)}}))
    ((&#39;&#39;, 1), (&#39;N&#39;, Onnx-Scan(, initial, x) -&gt; y, z))
    --order--
    ((&#39;initial&#39;, 0), 0)
    ((&#39;x&#39;, 0), 1)
    --nodes--
    (&#39;&#39;, Onnx-Scan(, initial, x) -&gt; y, z)
    --ONNX--
    opset: domain=&#39;&#39; version=8
    input: name=&#39;initial&#39; type=dtype(&#39;float32&#39;) shape=(1, 2)
    input: name=&#39;x&#39; type=dtype(&#39;float32&#39;) shape=(1, 3, 2)
    Scan(, initial, x, num_scan_inputs=1) -&gt; y, z
    output: name=&#39;y&#39; type=dtype(&#39;float32&#39;) shape=(1, 2)
    output: name=&#39;z&#39; type=dtype(&#39;float32&#39;) shape=(1, 3, 2)
    ----- subgraph ---- Scan -  - att.body=
    input: name=&#39;sum_in&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    input: name=&#39;next&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    Add(sum_in, next) -&gt; sum_out
      Identity(sum_out) -&gt; scan_out
    output: name=&#39;sum_out&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    output: name=&#39;scan_out&#39; type=dtype(&#39;float32&#39;) shape=(2,)
    ---
    
    
    ======================================================================
    ERROR: test_scatter_elements_without_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 190, in run
        res = self._run(*args, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_scatter_elements.py&quot;, line 78, in _run
        res = scatter_elements(data, indices, updates, axis=self.axis)
    AttributeError: &#39;ScatterElements&#39; object has no attribute &#39;axis&#39;
    
    ======================================================================
    ERROR: test_scatter_with_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Scatter&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_scatter_without_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Scatter&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_scatternd_add_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ScatterND&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_scatternd_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ScatterND&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_scatternd_multiply_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ScatterND&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1_mean_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1_mean_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1_mean_weight_negative_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1_mean_weight_negative_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_none_no_weight_negative_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_none_no_weight_negative_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_none_no_weight_negative_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_sum_weight_high_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_sum_weight_high_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3_sum_weight_high_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_mean_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_mean_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_mean_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_none_no_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_none_no_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_NCd1d2d3d4d5_none_no_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_3d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_3d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_4d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_4d_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_4d_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_4d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_no_weight_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_3d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_3d_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_3d_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_3d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_4d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_4d_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_4d_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_4d_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_ii_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_mean_weight_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_weights_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_weights_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_weights_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_none_weights_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_sum_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_sum_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_sum_log_prob_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SoftmaxCrossEntropyLoss&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sce_sum_log_prob_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_selu_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Selu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_selu_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Selu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_selu_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Selu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_shrink_hard_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Shrink&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_shrink_soft_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Shrink&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_simple_rnn_batchwise_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 113, in load_op
        return cl(onnx_node, desc=desc, **options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 156, in __init__
        CommonRNN.__init__(self, onnx_node, desc=desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 40, in __init__
        self.f1 = self.choose_act(self.activations[0],
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 57, in choose_act
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unknown activation function &#39;tanh&#39;.
    
    ======================================================================
    ERROR: test_simple_rnn_defaults_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 113, in load_op
        return cl(onnx_node, desc=desc, **options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 156, in __init__
        CommonRNN.__init__(self, onnx_node, desc=desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 40, in __init__
        self.f1 = self.choose_act(self.activations[0],
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 57, in choose_act
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unknown activation function &#39;tanh&#39;.
    
    ======================================================================
    ERROR: test_simple_rnn_with_initial_bias_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 113, in load_op
        return cl(onnx_node, desc=desc, **options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 156, in __init__
        CommonRNN.__init__(self, onnx_node, desc=desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 40, in __init__
        self.f1 = self.choose_act(self.activations[0],
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_rnn.py&quot;, line 57, in choose_act
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Unknown activation function &#39;tanh&#39;.
    
    ======================================================================
    ERROR: test_slice_start_out_of_bounds_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;y&quot;
    type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 20
          }
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 5
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_softmax_axis_0_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 594, in run
        res = OpRunUnary.run(self, x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_softmax.py&quot;, line 26, in _run
        tmp = X - X.max(axis=self.axis)[:, numpy.newaxis]
    ValueError: operands could not be broadcast together with shapes (3,4,5) (4,1,5) 
    
    ======================================================================
    ERROR: test_softmax_axis_2_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 594, in run
        res = OpRunUnary.run(self, x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_softmax.py&quot;, line 26, in _run
        tmp = X - X.max(axis=self.axis)[:, numpy.newaxis]
    ValueError: operands could not be broadcast together with shapes (3,4,5) (3,1,4) 
    
    ======================================================================
    ERROR: test_softmax_negative_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 594, in run
        res = OpRunUnary.run(self, x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_softmax.py&quot;, line 26, in _run
        tmp = X - X.max(axis=self.axis)[:, numpy.newaxis]
    ValueError: operands could not be broadcast together with shapes (3,4,5) (3,1,4) 
    
    ======================================================================
    ERROR: test_softplus_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Softplus&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_softplus_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Softplus&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_softsign_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Softsign&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_softsign_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Softsign&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_spacetodepth_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SpaceToDepth&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_spacetodepth_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SpaceToDepth&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_split_zero_size_splits_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;input&quot;
    type {
      tensor_type {
        elem_type: 1
        shape {
          dim {
            dim_value: 0
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_tfidfvectorizer_tf_only_bigrams_skip0_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_tfidfvectorizer.py&quot;, line 54, in _run
        return (res.reshape((x.shape[0], -1)), )
    ValueError: cannot reshape array of size 7 into shape (12,newaxis)
    
    ======================================================================
    ERROR: test_tfidfvectorizer_tf_onlybigrams_levelempty_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_tfidfvectorizer.py&quot;, line 54, in _run
        return (res.reshape((x.shape[0], -1)), )
    ValueError: cannot reshape array of size 3 into shape (12,newaxis)
    
    ======================================================================
    ERROR: test_tfidfvectorizer_tf_onlybigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_tfidfvectorizer.py&quot;, line 54, in _run
        return (res.reshape((x.shape[0], -1)), )
    ValueError: cannot reshape array of size 7 into shape (12,newaxis)
    
    ======================================================================
    ERROR: test_tfidfvectorizer_tf_uniandbigrams_skip5_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_tfidfvectorizer.py&quot;, line 54, in _run
        return (res.reshape((x.shape[0], -1)), )
    ValueError: cannot reshape array of size 7 into shape (12,newaxis)
    
    ======================================================================
    ERROR: test_thresholdedrelu_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ThresholdedRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_thresholdedrelu_default_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ThresholdedRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_thresholdedrelu_example_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;ThresholdedRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tile_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Tile&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tile_precomputed_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Tile&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_neg_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_one_row_neg_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_out_neg_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_out_pos_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_pos_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_square_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_square_neg_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_tril_zero_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;x&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 3
          }
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 5
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_triu_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_neg_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_one_row_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_out_neg_out_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_out_pos_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_pos_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_square_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_square_neg_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Trilu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_triu_zero_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 210, in _init
        raise RuntimeError(  # pragma: no cover
    RuntimeError: Wrong ONNX file, one input or output has an empty shape: name: &quot;x&quot;
    type {
      tensor_type {
        elem_type: 7
        shape {
          dim {
            dim_value: 0
          }
          dim {
            dim_value: 5
          }
        }
      }
    }
    .
    
    ======================================================================
    ERROR: test_unique_not_sorted_without_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Unique&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_unique_sorted_with_axis_3d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Unique&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_unique_sorted_with_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Unique&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_unique_sorted_with_negative_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Unique&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_unique_sorted_without_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Unique&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_upsample_nearest_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Upsample&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor2d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor3d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor4d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor_bcast3v1d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor_bcast3v2d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor_bcast4v2d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor_bcast4v3d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_xor_bcast4v4d_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Xor&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_ConstantPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;pads&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Pad&#39;) and shapes
    ShapeObject((2, 3, 4, 4), dtype=numpy.float32, name=&#39;0&#39;)
    ----args
    (ShapeObject((2, 3, 4, 4), dtype=numpy.float32, name=&#39;0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Pad&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_pad.Pad object at 0x7f11954b2850&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Pad(0) -&gt; 1
    
    ======================================================================
    ERROR: test_ELU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Elu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_LogSoftmax_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_PReLU_1d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_PReLU_1d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_PReLU_2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_PReLU_2d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_PReLU_3d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_PReLU_3d_multiparam_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;PRelu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_ReflectionPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;pads&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Pad&#39;) and shapes
    ShapeObject((2, 3, 8, 8), dtype=numpy.float32, name=&#39;0&#39;)
    ----args
    (ShapeObject((2, 3, 8, 8), dtype=numpy.float32, name=&#39;0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Pad&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_pad.Pad object at 0x7f119540e220&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Pad(0) -&gt; 1
    
    ======================================================================
    ERROR: test_ReplicationPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;pads&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Pad&#39;) and shapes
    ShapeObject((2, 3, 4, 4), dtype=numpy.float32, name=&#39;0&#39;)
    ----args
    (ShapeObject((2, 3, 4, 4), dtype=numpy.float32, name=&#39;0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Pad&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_pad.Pad object at 0x7f119540e640&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Pad(0) -&gt; 1
    
    ======================================================================
    ERROR: test_SELU_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Selu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_Softplus_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Softplus&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_ZeroPad2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;pads&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Pad&#39;) and shapes
    ShapeObject((2, 3, 4, 4), dtype=numpy.float32, name=&#39;0&#39;)
    ----args
    (ShapeObject((2, 3, 4, 4), dtype=numpy.float32, name=&#39;0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Pad&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_pad.Pad object at 0x7f1195498070&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Pad(0) -&gt; 1
    
    ======================================================================
    ERROR: test_log_softmax_dim3_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_log_softmax_lastdim_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LogSoftmax&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_softmax_functional_dim3_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 320, in run
        outputs = list(prepared_model.run(inputs))
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 52, in run
        outs = self._session.run(feeds)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 853, in run
        return self._run(inputs, clean_right_away=False,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 999, in _run_sequence_runtime
        node.run(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 251, in run
        res = self.ops_.run(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 594, in run
        res = OpRunUnary.run(self, x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 474, in run
        res = self._run(x)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/op_softmax.py&quot;, line 26, in _run
        tmp = X - X.max(axis=self.axis)[:, numpy.newaxis]
    ValueError: operands could not be broadcast together with shapes (2,3,4,5) (2,1,3,4) 
    
    ======================================================================
    ERROR: test_operator_pad_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 235, in infer_shapes
        res = self._infer_shapes(*args, **kwargs)
    TypeError: _infer_shapes() missing 1 required positional argument: &#39;pads&#39;
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 323, in _set_shape_inference_runtime
        res = self.ops_.infer_shapes(*args)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/_op.py&quot;, line 237, in infer_shapes
        raise TypeError(  # pragma: no cover
    TypeError: Issues with (operator &#39;Pad&#39;) and shapes
    ShapeObject((1, 1, 2, 4), dtype=numpy.float32, name=&#39;0&#39;)
    ----args
    (ShapeObject((1, 1, 2, 4), dtype=numpy.float32, name=&#39;0&#39;),)
    ------kwargs
    {}
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1383, in _set_shape_inference_runtime
        s = node._set_shape_inference_runtime(values)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 325, in _set_shape_inference_runtime
        raise TypeError(
    TypeError: Unable to call infer_shapes with 1 arguments for class &#39;Pad&#39; (&lt;bound method OpRun.infer_shapes of &lt;mlprodict.onnxrt.ops_cpu.op_pad.Pad object at 0x7f11806ac820&gt;&gt;)
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 264, in _init
        self.shapes_ = self._set_shape_inference_runtime()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 1394, in _set_shape_inference_runtime
        raise RuntimeError(&quot;Unable to infer shape of node {}\n{}&quot;.format(
    RuntimeError: Unable to infer shape of node 0
    0 --&gt; Onnx-Pad(0) -&gt; 1
    
    ======================================================================
    ERROR: test_operator_repeat_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Tile&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_operator_repeat_dim_overflow_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Tile&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_operator_selu_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Selu&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_operator_symbolic_override_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;InstanceNormalization&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_bvlc_alexnet_cpu (__main__.OnnxBackendRealModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;LRN&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gradient_of_add_and_mul_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Gradient&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_gradient_of_add_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Gradient&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sequence_model1_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SequenceEmpty&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sequence_model2_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SequenceErase&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sequence_model3_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SequenceErase&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sequence_model6_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SplitToSequence&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sequence_model7_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SplitToSequence&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_sequence_model8_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;SplitToSequence&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    ERROR: test_shrink_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 295, in run
        prepared_model = self.backend.prepare(model, device)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 179, in prepare
        return cls.prepare(binm, device, **kwargs)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 164, in prepare
        inf = cls.create_inference_session(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/backend.py&quot;, line 144, in create_inference_session
        return OnnxInference(model)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 140, in __init__
        self._init()
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference.py&quot;, line 250, in _init
        node.setup_runtime(
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/onnx_inference_node.py&quot;, line 146, in setup_runtime
        self.ops_ = load_op(self.onnx_node, desc=self.desc,
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops.py&quot;, line 35, in load_op
        return lo(onnx_node, desc=desc, options=options)
      File &quot;/var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/sphinxdoc/source/mlprodict/onnxrt/ops_cpu/__init__.py&quot;, line 83, in load_op
        raise MissingOperatorError(  # pragma no cover
    mlprodict.onnxrt.excs.MissingOperatorError: Operator &#39;Shrink&#39; has no runtime yet. Available list:
    
    --- +
    Abs
    Acos
    Acosh
    Add
    And
    ArgMax
    ArgMin
    ArrayFeatureExtractor
    Asin
    Asinh
    Atan
    Atanh
    AveragePool
    BatchNormalization
    Binarizer
    BroadcastGradientArgs
    CDist
    Cast
    CastLike
    CategoryMapper
    Ceil
    Celu
    Clip
    ComplexAbs
    Compress
    Concat
    ConcatFromSequence
    Constant
    ConstantOfShape
    Conv
    ConvTranspose
    Cos
    Cosh
    CumSum
    DEBUG
    DequantizeLinear
    Det
    DictVectorizer
    Div
    Dropout
    Einsum
    Equal
    Erf
    Exp
    Expand
    EyeLike
    FFT
    FFT2D
    FeatureVectorizer
    Flatten
    Floor
    FusedMatMul
    Gather
    GatherElements
    Gemm
    GlobalAveragePool
    Greater
    GreaterOrEqual
    Identity
    If
    Imputer
    IsNaN
    LabelEncoder
    LeakyRelu
    Less
    LessOrEqual
    LinearClassifier
    LinearRegressor
    Log
    Loop
    LpNormalization
    MatMul
    Max
    MaxPool
    Mean
    Min
    Mod
    Mul
    Neg
    Normalizer
    Not
    OneHotEncoder
    OpRun
    Or
    Pad
    Pow
    QLinearConv
    QuantizeLinear
    RFFT
    RNN
    Range
    Reciprocal
    ReduceL1
    ReduceL2
    ReduceLogSumExp
    ReduceMax
    ReduceMean
    ReduceMin
    ReduceProd
    ReduceSum
    ReduceSumSquare
    Relu
    Reshape
    Round
    SVMClassifier
    SVMClassifierDouble
    SVMRegressor
    SVMRegressorDouble
    Scaler
    Scan
    ScatterElements
    SequenceAt
    SequenceConstruct
    SequenceInsert
    Shape
    Sigmoid
    Sign
    Sin
    Sinh
    Size
    Slice
    Softmax
    SoftmaxGrad
    Solve
    Split
    Sqrt
    Squeeze
    StringNormalizer
    Sub
    Sum
    Tan
    Tanh
    TfIdfVectorizer
    Tokenizer
    TopK
    Transpose
    TreeEnsembleClassifier
    TreeEnsembleClassifierDouble
    TreeEnsembleRegressor
    TreeEnsembleRegressorDouble
    Unsqueeze
    Where
    YieldOp
    ZipMap
    
    ======================================================================
    FAIL: test_cast_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 187, in assert_similar_outputs
        np.testing.assert_equal(outputs[i].dtype, ref_outputs[i].dtype)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 425, in assert_equal
        raise AssertionError(msg)
    AssertionError: 
    Items are not equal:
     ACTUAL: dtype(&#39;&lt;U32&#39;)
     DESIRED: dtype(&#39;O&#39;)
    
    ======================================================================
    FAIL: test_castlike_FLOAT_to_STRING_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    Mismatched elements: 12 / 12 (100%)
     x: array([[0.9767611026763916, 0.6048455238342285, 0.7392635941505432,
            0.03918779268860817],
           [0.28280696272850037, 0.12019655853509903, 0.296140193939209,...
     y: array([[&#39;0.9767611&#39;, &#39;0.6048455&#39;, &#39;0.7392636&#39;, &#39;0.039187793&#39;],
           [&#39;0.28280696&#39;, &#39;0.12019656&#39;, &#39;0.2961402&#39;, &#39;0.11872772&#39;],
           [&#39;0.31798318&#39;, &#39;0.41426298&#39;, &#39;0.064147495&#39;, &#39;0.6924721&#39;]],
          dtype=object)
    
    ======================================================================
    FAIL: test_castlike_FLOAT_to_STRING_expanded_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 187, in assert_similar_outputs
        np.testing.assert_equal(outputs[i].dtype, ref_outputs[i].dtype)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 425, in assert_equal
        raise AssertionError(msg)
    AssertionError: 
    Items are not equal:
     ACTUAL: dtype(&#39;&lt;U32&#39;)
     DESIRED: dtype(&#39;O&#39;)
    
    ======================================================================
    FAIL: test_convtranspose_autopad_same_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (1, 2, 7, 7), (1, 2, 6, 6) mismatch)
     x: array([[[[ 0.,  0.,  1.,  1.,  3.,  2.,  2.],
             [ 0.,  0.,  1.,  1.,  3.,  2.,  2.],
             [ 3.,  3.,  8.,  5., 12.,  7.,  7.],...
     y: array([[[[ 0.,  0.,  1.,  1.,  3.,  2.],
             [ 0.,  0.,  1.,  1.,  3.,  2.],
             [ 3.,  3.,  8.,  5., 12.,  7.],...
    
    ======================================================================
    FAIL: test_convtranspose_output_shape_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (1, 2, 9, 7), (1, 2, 10, 8) mismatch)
     x: array([[[[ 0.,  0.,  1.,  1.,  3.,  2.,  2.],
             [ 0.,  0.,  1.,  1.,  3.,  2.,  2.],
             [ 0.,  0.,  1.,  1.,  3.,  2.,  2.],...
     y: array([[[[ 0.,  0.,  1.,  1.,  3.,  2.,  2.,  0.],
             [ 0.,  0.,  1.,  1.,  3.,  2.,  2.,  0.],
             [ 0.,  0.,  1.,  1.,  3.,  2.,  2.,  0.],...
    
    ======================================================================
    FAIL: test_loop11_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (1,), (5, 1) mismatch)
     x: array([13.], dtype=float32)
     y: array([[-1.],
           [ 1.],
           [ 4.],...
    
    ======================================================================
    FAIL: test_maxpool_2d_uint8_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 187, in assert_similar_outputs
        np.testing.assert_equal(outputs[i].dtype, ref_outputs[i].dtype)
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 425, in assert_equal
        raise AssertionError(msg)
    AssertionError: 
    Items are not equal:
     ACTUAL: dtype(&#39;float64&#39;)
     DESIRED: dtype(&#39;uint8&#39;)
    
    ======================================================================
    FAIL: test_mod_int64_fmod_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 2 / 6 (33.3%)
    Max absolute difference: 3
    Max relative difference: 3.
     x: array([ 0, -2,  5,  0,  2,  3])
     y: array([ 0,  1,  5,  0, -1,  3])
    
    ======================================================================
    FAIL: test_mod_mixed_sign_float16_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 4 / 6 (66.7%)
    Max absolute difference: 3.4
    Max relative difference: 20.67
     x: array([ 1.998, -3.002,  5.   , -1.998,  3.002,  3.   ], dtype=float16)
     y: array([-0.10156,  0.3984 ,  5.     ,  0.10156, -0.3984 ,  3.     ],
          dtype=float16)
    
    ======================================================================
    FAIL: test_mod_mixed_sign_float32_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 4 / 6 (66.7%)
    Max absolute difference: 3.4
    Max relative difference: 21.
     x: array([ 2., -3.,  5., -2.,  3.,  3.], dtype=float32)
     y: array([-0.1,  0.4,  5. ,  0.1, -0.4,  3. ], dtype=float32)
    
    ======================================================================
    FAIL: test_mod_mixed_sign_float64_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 4 / 6 (66.7%)
    Max absolute difference: 3.4
    Max relative difference: 21.
     x: array([ 2., -3.,  5., -2.,  3.,  3.])
     y: array([-0.1,  0.4,  5. ,  0.1, -0.4,  3. ])
    
    ======================================================================
    FAIL: test_quantizelinear_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 1 / 6 (16.7%)
    Max absolute difference: 255
    Max relative difference: 1.962
     x: array([128, 129, 129, 255,   1,   0], dtype=uint8)
     y: array([128, 129, 130, 255,   1,   0], dtype=uint8)
    
    ======================================================================
    FAIL: test_scatter_elements_with_duplicate_indices_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 1 / 5 (20%)
    Max absolute difference: 3.1
    Max relative difference: 0.596
     x: array([[1. , 2.1, 3. , 4. , 5. ]], dtype=float32)
     y: array([[1. , 5.2, 3. , 4. , 5. ]], dtype=float32)
    
    ======================================================================
    FAIL: test_shape_end_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (3,), (1,) mismatch)
     x: array([3, 4, 5])
     y: array([3])
    
    ======================================================================
    FAIL: test_shape_end_negative_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (3,), (2,) mismatch)
     x: array([3, 4, 5])
     y: array([3, 4])
    
    ======================================================================
    FAIL: test_shape_start_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (3,), (2,) mismatch)
     x: array([3, 4, 5])
     y: array([4, 5])
    
    ======================================================================
    FAIL: test_shape_start_1_end_2_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (3,), (1,) mismatch)
     x: array([3, 4, 5])
     y: array([4])
    
    ======================================================================
    FAIL: test_shape_start_1_end_negative_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (3,), (1,) mismatch)
     x: array([3, 4, 5])
     y: array([4])
    
    ======================================================================
    FAIL: test_shape_start_negative_1_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    (shapes (3,), (1,) mismatch)
     x: array([3, 4, 5])
     y: array([5])
    
    ======================================================================
    FAIL: test_softmax_default_axis_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 60 / 60 (100%)
    Max absolute difference: 0.359
    Max relative difference: 2.949
     x: array([[[0.528422, 0.116434, 0.369971, 0.722795, 0.544447],
            [0.240596, 0.201796, 0.161748, 0.085238, 0.126825],
            [0.104573, 0.334101, 0.297592, 0.086826, 0.131115],...
     y: array([[[0.225649, 0.05769 , 0.10289 , 0.363515, 0.250256],
            [0.294493, 0.286594, 0.128938, 0.122878, 0.167097],
            [0.112513, 0.417088, 0.208526, 0.110024, 0.151849],...
    
    ======================================================================
    FAIL: test_strnormalizer_export_monday_casesensintive_lower_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (4,), (3,) mismatch)
     x: array([&#39;&#39;, &#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
     y: array([&#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnormalizer_export_monday_casesensintive_nochangecase_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (4,), (3,) mismatch)
     x: array([&#39;&#39;, &#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
     y: array([&#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnormalizer_export_monday_casesensintive_upper_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (4,), (3,) mismatch)
     x: array([&#39;&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;THURSDAY&#39;], dtype=object)
     y: array([&#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;THURSDAY&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnormalizer_export_monday_empty_output_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (2,), (1,) mismatch)
     x: array([&#39;&#39;, &#39;&#39;], dtype=object)
     y: array([&#39;&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnormalizer_export_monday_insensintive_upper_twodim_cpu (__main__.OnnxBackendNodeModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (1, 6), (1, 4) mismatch)
     x: array([[&#39;MONDAY&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;MONDAY&#39;, &#39;TUESDAY&#39;,
            &#39;WEDNESDAY&#39;]], dtype=object)
     y: array([[&#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;]], dtype=object)
    
    ======================================================================
    FAIL: test_ConvTranspose2d_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 959 / 960 (99.9%)
    Max absolute difference: 1.493
    Max relative difference: 225.788
     x: array([[[[ 5.539888e-02,  4.097741e-01,  9.570615e-02,  1.595743e-02,
              -3.908167e-01,  9.092239e-01,  9.316773e-02,  5.385656e-02,
               5.751054e-02, -3.537251e-01,  9.881802e-02, -1.250897e-01],...
     y: array([[[[-3.870082e-02,  4.058291e-01,  9.855538e-02, -3.768350e-01,
              -1.542787e-02,  6.494146e-01,  4.276419e-01, -6.573269e-01,
               3.279429e-01, -1.139378e-01,  1.777776e-01,  2.557690e-02],...
    
    ======================================================================
    FAIL: test_ConvTranspose2d_no_bias_cpu (__main__.OnnxBackendPyTorchConvertedModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 960 / 960 (100%)
    Max absolute difference: 1.274
    Max relative difference: 654.835
     x: array([[[[ 3.821167e-01, -3.122261e-01,  5.672676e-02, -8.570510e-02,
               7.182749e-02,  9.514651e-02, -2.197984e-01,  1.615958e-01,
               1.255040e-02, -2.041010e-01,  1.192159e-01, -7.977150e-03,...
     y: array([[[[ 4.195647e-01, -2.791808e-01, -4.167238e-01, -2.238854e-01,
               1.521523e-01, -2.762069e-01, -2.465742e-01,  1.158977e-01,
              -5.013817e-01, -2.075676e-01,  1.123053e-02,  3.795193e-01,...
    
    ======================================================================
    FAIL: test_operator_convtranspose_cpu (__main__.OnnxBackendPyTorchOperatorModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 191, in assert_similar_outputs
        np.testing.assert_allclose(
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 1530, in assert_allclose
        assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 844, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Not equal to tolerance rtol=0.001, atol=1e-07
    
    Mismatched elements: 924 / 1080 (85.6%)
    Max absolute difference: 0.401
    Max relative difference: 7.427
     x: array([[[[-0.236849,  0.399109, -0.070375, ..., -0.236849,  0.399109,
               0.      ],
             [-0.139387,  0.233569, -0.148454, ..., -0.139387,  0.233569,...
     y: array([[[[-0.210191,  0.348651, -0.023576, ..., -0.210191,  0.348651,
               0.      ],
             [-0.1115  , -0.104302,  0.048542, ..., -0.1115  , -0.104302,...
    
    ======================================================================
    FAIL: test_strnorm_model_monday_casesensintive_lower_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (4,), (3,) mismatch)
     x: array([&#39;&#39;, &#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
     y: array([&#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnorm_model_monday_casesensintive_nochangecase_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (4,), (3,) mismatch)
     x: array([&#39;&#39;, &#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
     y: array([&#39;tuesday&#39;, &#39;wednesday&#39;, &#39;thursday&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnorm_model_monday_casesensintive_upper_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (4,), (3,) mismatch)
     x: array([&#39;&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;THURSDAY&#39;], dtype=object)
     y: array([&#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;THURSDAY&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnorm_model_monday_empty_output_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (2,), (1,) mismatch)
     x: array([&#39;MONDAY&#39;, &#39;MONDAY&#39;], dtype=object)
     y: array([&#39;&#39;], dtype=object)
    
    ======================================================================
    FAIL: test_strnorm_model_monday_insensintive_upper_twodim_cpu (__main__.OnnxBackendSimpleModelTest)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 265, in device_test_func
        return test_func(*args, device=device, **kwargs)
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 321, in run
        self.assert_similar_outputs(ref_outputs, outputs,
      File &quot;/usr/local/lib/python3.9/site-packages/onnx/backend/test/runner/__init__.py&quot;, line 189, in assert_similar_outputs
        np.testing.assert_array_equal(outputs[i], ref_outputs[i])
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 934, in assert_array_equal
        assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
      File &quot;/usr/local/lib/python3.9/site-packages/numpy/testing/_private/utils.py&quot;, line 763, in assert_array_compare
        raise AssertionError(msg)
    AssertionError: 
    Arrays are not equal
    
    (shapes (1, 6), (1, 4) mismatch)
     x: array([[&#39;MONDAY&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;MONDAY&#39;, &#39;TUESDAY&#39;,
            &#39;WEDNESDAY&#39;]], dtype=object)
     y: array([[&#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;, &#39;TUESDAY&#39;, &#39;WEDNESDAY&#39;]], dtype=object)
    
    ----------------------------------------------------------------------
    Ran 2026 tests in 26.464s
    
    FAILED (failures=33, errors=376, skipped=1021)
</pre></div>
</div>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">ONNX Backends</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="backend_onnxruntime1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ONNX Backends for onnxruntime1</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier Dupré.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>