{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark Random Forests, Tree Ensemble, Multi-Classification\n\nThe script compares different implementations for the operator\nTreeEnsembleRegressor for a multi-regression. It replicates\nthe benchmark `l-example-tree-ensemble-reg-bench`\nfor multi-classification.\n\n## Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nfrom time import perf_counter as time\nfrom multiprocessing import cpu_count\nimport numpy\nfrom numpy.random import rand\nfrom numpy.testing import assert_almost_equal\nimport pandas\nimport matplotlib.pyplot as plt\nfrom sklearn import config_context\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils._testing import ignore_warnings\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom onnxruntime import InferenceSession\nfrom mlprodict.onnxrt import OnnxInference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nprint(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Versions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def version():\n    from datetime import datetime\n    import sklearn\n    import numpy\n    import onnx\n    import onnxruntime\n    import skl2onnx\n    import mlprodict\n    df = pandas.DataFrame([\n        {\"name\": \"date\", \"version\": str(datetime.now())},\n        {\"name\": \"numpy\", \"version\": numpy.__version__},\n        {\"name\": \"scikit-learn\", \"version\": sklearn.__version__},\n        {\"name\": \"onnx\", \"version\": onnx.__version__},\n        {\"name\": \"onnxruntime\", \"version\": onnxruntime.__version__},\n        {\"name\": \"skl2onnx\", \"version\": skl2onnx.__version__},\n        {\"name\": \"mlprodict\", \"version\": mlprodict.__version__},\n    ])\n    return df\n\n\nversion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementations to benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fcts_model(X, y, max_depth, n_estimators, n_jobs):\n    \"RandomForestClassifier.\"\n    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators,\n                                n_jobs=n_jobs)\n    rf.fit(X, y)\n\n    initial_types = [('X', FloatTensorType([None, X.shape[1]]))]\n    onx = convert_sklearn(rf, initial_types=initial_types,\n                          options={id(rf): {'zipmap': False}})\n    sess = InferenceSession(onx.SerializeToString())\n    outputs = [o.name for o in sess.get_outputs()]\n    oinf = OnnxInference(onx, runtime=\"python\")\n    oinf.sequence_[0].ops_._init(numpy.float32, 1)\n    name = outputs[1]\n    oinf2 = OnnxInference(onx, runtime=\"python\")\n    oinf2.sequence_[0].ops_._init(numpy.float32, 2)\n    oinf3 = OnnxInference(onx, runtime=\"python\")\n    oinf3.sequence_[0].ops_._init(numpy.float32, 3)\n\n    def predict_skl_predict(X, model=rf):\n        return rf.predict_proba(X)\n\n    def predict_onnxrt_predict(X, sess=sess):\n        return sess.run(outputs[:1], {'X': X})[0]\n\n    def predict_onnx_inference(X, oinf=oinf):\n        return oinf.run({'X': X})[name]\n\n    def predict_onnx_inference2(X, oinf2=oinf2):\n        return oinf2.run({'X': X})[name]\n\n    def predict_onnx_inference3(X, oinf3=oinf3):\n        return oinf3.run({'X': X})[name]\n\n    return {'predict': (\n        predict_skl_predict, predict_onnxrt_predict,\n        predict_onnx_inference, predict_onnx_inference2,\n        predict_onnx_inference3)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def allow_configuration(**kwargs):\n    return True\n\n\ndef bench(n_obs, n_features, max_depths, n_estimatorss, n_jobss,\n          methods, repeat=10, verbose=False):\n    res = []\n    for nfeat in n_features:\n\n        ntrain = 50000\n        X_train = numpy.empty((ntrain, nfeat)).astype(numpy.float32)\n        X_train[:, :] = rand(ntrain, nfeat)[:, :]\n        eps = rand(ntrain) - 0.5\n        y_train_f = X_train.sum(axis=1) + eps\n        y_train = (y_train_f > 12).astype(numpy.int64)\n        y_train[y_train_f > 15] = 2\n        y_train[y_train_f < 10] = 3\n\n        for n_jobs in n_jobss:\n            for max_depth in max_depths:\n                for n_estimators in n_estimatorss:\n                    fcts = fcts_model(X_train, y_train,\n                                      max_depth, n_estimators, n_jobs)\n\n                    for n in n_obs:\n                        for method in methods:\n\n                            fct1, fct2, fct3, fct4, fct5 = fcts[method]\n\n                            if not allow_configuration(\n                                    n=n, nfeat=nfeat, max_depth=max_depth,\n                                    n_estimator=n_estimators, n_jobs=n_jobs,\n                                    method=method):\n                                continue\n\n                            obs = dict(n_obs=n, nfeat=nfeat,\n                                       max_depth=max_depth,\n                                       n_estimators=n_estimators,\n                                       method=method,\n                                       n_jobs=n_jobs)\n\n                            # creates different inputs to avoid caching\n                            Xs = []\n                            for r in range(repeat):\n                                x = numpy.empty((n, nfeat))\n                                x[:, :] = rand(n, nfeat)[:, :]\n                                Xs.append(x.astype(numpy.float32))\n\n                            # measures the baseline\n                            with config_context(assume_finite=True):\n                                st = time()\n                                repeated = 0\n                                for X in Xs:\n                                    p1 = fct1(X)\n                                    repeated += 1\n                                    if time() - st >= 1:\n                                        break  # stops if longer than a second\n                                end = time()\n                                obs[\"time_skl\"] = (end - st) / repeated\n\n                            # measures the new implementation\n                            st = time()\n                            r2 = 0\n                            for X in Xs:\n                                p2 = fct2(X)\n                                r2 += 1\n                                if r2 >= repeated:\n                                    break\n                            end = time()\n                            obs[\"time_ort\"] = (end - st) / r2\n\n                            # measures the other new implementation\n                            st = time()\n                            r2 = 0\n                            for X in Xs:\n                                p2 = fct3(X)\n                                r2 += 1\n                                if r2 >= repeated:\n                                    break\n                            end = time()\n                            obs[\"time_mlprodict\"] = (end - st) / r2\n\n                            # measures the other new implementation 2\n                            st = time()\n                            r2 = 0\n                            for X in Xs:\n                                p2 = fct4(X)\n                                r2 += 1\n                                if r2 >= repeated:\n                                    break\n                            end = time()\n                            obs[\"time_mlprodict2\"] = (end - st) / r2\n\n                            # measures the other new implementation 3\n                            st = time()\n                            r2 = 0\n                            for X in Xs:\n                                p2 = fct5(X)\n                                r2 += 1\n                                if r2 >= repeated:\n                                    break\n                            end = time()\n                            obs[\"time_mlprodict3\"] = (end - st) / r2\n\n                            # final\n                            res.append(obs)\n                            if verbose and (len(res) % 1 == 0 or n >= 10000):\n                                print(\"bench\", len(res), \":\", obs)\n\n                            # checks that both produce the same outputs\n                            if n <= 10000:\n                                if len(p1.shape) == 1 and len(p2.shape) == 2:\n                                    p2 = p2.ravel()\n                                try:\n                                    assert_almost_equal(\n                                        p1.ravel(), p2.ravel(), decimal=5)\n                                except AssertionError as e:\n                                    warnings.warn(str(e))\n    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_rf_models(dfr):\n\n    def autolabel(ax, rects):\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('%1.1fx' % height,\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom',\n                        fontsize=8)\n\n    engines = [_.split('_')[-1] for _ in dfr.columns if _.startswith(\"time_\")]\n    engines = [_ for _ in engines if _ != 'skl']\n    for engine in engines:\n        dfr[\"speedup_%s\" % engine] = dfr[\"time_skl\"] / dfr[\"time_%s\" % engine]\n    print(dfr.tail().T)\n\n    ncols = 4\n    fig, axs = plt.subplots(len(engines), ncols, figsize=(\n        14, 4 * len(engines)), sharey=True)\n\n    row = 0\n    for row, engine in enumerate(engines):\n        pos = 0\n        name = \"RandomForestClassifier - %s\" % engine\n        for max_depth in sorted(set(dfr.max_depth)):\n            for nf in sorted(set(dfr.nfeat)):\n                for est in sorted(set(dfr.n_estimators)):\n                    for n_jobs in sorted(set(dfr.n_jobs)):\n                        sub = dfr[(dfr.max_depth == max_depth) &\n                                  (dfr.nfeat == nf) &\n                                  (dfr.n_estimators == est) &\n                                  (dfr.n_jobs == n_jobs)]\n                        ax = axs[row, pos]\n                        labels = sub.n_obs\n                        means = sub[\"speedup_%s\" % engine]\n\n                        x = numpy.arange(len(labels))\n                        width = 0.90\n\n                        rects1 = ax.bar(x, means, width, label='Speedup')\n                        if pos == 0:\n                            ax.set_yscale('log')\n                            ax.set_ylim([0.1, max(dfr[\"speedup_%s\" % engine])])\n\n                        if pos == 0:\n                            ax.set_ylabel('Speedup')\n                        ax.set_title(\n                            '%s\\ndepth %d - %d features\\n %d estimators '\n                            '%d jobs' % (name, max_depth, nf, est, n_jobs))\n                        if row == len(engines) - 1:\n                            ax.set_xlabel('batch size')\n                        ax.set_xticks(x)\n                        ax.set_xticklabels(labels)\n                        autolabel(ax, rects1)\n                        for tick in ax.xaxis.get_major_ticks():\n                            tick.label.set_fontsize(8)\n                        for tick in ax.yaxis.get_major_ticks():\n                            tick.label.set_fontsize(8)\n                        pos += 1\n\n    fig.tight_layout()\n    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run benchs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ignore_warnings(category=FutureWarning)\ndef run_bench(repeat=100, verbose=False):\n    n_obs = [1, 10, 100, 1000, 10000]\n    methods = ['predict']\n    n_features = [30]\n    max_depths = [6, 8, 10, 12]\n    n_estimatorss = [100]\n    n_jobss = [cpu_count()]\n\n    start = time()\n    results = bench(n_obs, n_features, max_depths, n_estimatorss, n_jobss,\n                    methods, repeat=repeat, verbose=verbose)\n    end = time()\n\n    results_df = pandas.DataFrame(results)\n    print(\"Total time = %0.3f sec cpu=%d\\n\" % (end - start, cpu_count()))\n\n    # plot the results\n    return results_df\n\n\nname = \"plot_random_forest_cls_multi\"\ndf = run_bench(verbose=True)\ndf.to_csv(\"%s.csv\" % name, index=False)\ndf.to_excel(\"%s.xlsx\" % name, index=False)\nfig, ax = plot_rf_models(df)\nfig.savefig(\"%s.png\" % name)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}