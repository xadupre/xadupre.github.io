{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark Linear Regression\n\nThe script compares different implementations for the operator\nLinearRegression.\n\n* *baseline*: LinearRegression from :epkg:`scikit-learn`\n* *ort*: :epkg:`onnxruntime`,\n* *mlprodict*: an implementation based on an array of structures,\n  every structure describes a node,\n\n## Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nfrom time import perf_counter as time\nfrom multiprocessing import cpu_count\nimport numpy\nfrom numpy.random import rand\nfrom numpy.testing import assert_almost_equal\nimport matplotlib.pyplot as plt\nimport pandas\nfrom onnxruntime import InferenceSession\nfrom sklearn import config_context\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.utils._testing import ignore_warnings\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom mlprodict.onnxrt import OnnxInference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nprint(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Versions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def version():\n    from datetime import datetime\n    import sklearn\n    import numpy\n    import onnx\n    import onnxruntime\n    import skl2onnx\n    import mlprodict\n    df = pandas.DataFrame([\n        {\"name\": \"date\", \"version\": str(datetime.now())},\n        {\"name\": \"numpy\", \"version\": numpy.__version__},\n        {\"name\": \"scikit-learn\", \"version\": sklearn.__version__},\n        {\"name\": \"onnx\", \"version\": onnx.__version__},\n        {\"name\": \"onnxruntime\", \"version\": onnxruntime.__version__},\n        {\"name\": \"skl2onnx\", \"version\": skl2onnx.__version__},\n        {\"name\": \"mlprodict\", \"version\": mlprodict.__version__},\n    ])\n    return df\n\n\nversion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementations to benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fcts_model(X, y, n_jobs):\n    \"LinearRegression.\"\n    model = LinearRegression(n_jobs=n_jobs)\n    model.fit(X, y)\n\n    initial_types = [('X', FloatTensorType([None, X.shape[1]]))]\n    onx = convert_sklearn(model, initial_types=initial_types)\n    sess = InferenceSession(onx.SerializeToString())\n    outputs = [o.name for o in sess.get_outputs()]\n    oinf = OnnxInference(onx, runtime=\"python\")\n\n    def predict_skl_predict(X, model=model):\n        return model.predict(X)\n\n    def predict_onnxrt_predict(X, sess=sess):\n        return sess.run(outputs[:1], {'X': X})[0]\n\n    def predict_onnx_inference(X, oinf=oinf):\n        return oinf.run({'X': X})[\"variable\"]\n\n    return {'predict': (\n        predict_skl_predict, predict_onnxrt_predict,\n        predict_onnx_inference)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def allow_configuration(**kwargs):\n    return True\n\n\ndef bench(n_obs, n_features, n_jobss,\n          methods, repeat=10, verbose=False):\n    res = []\n    for nfeat in n_features:\n\n        ntrain = 50000\n        X_train = numpy.empty((ntrain, nfeat)).astype(numpy.float32)\n        X_train[:, :] = rand(ntrain, nfeat)[:, :]\n        eps = rand(ntrain) - 0.5\n        y_train = X_train.sum(axis=1) + eps\n\n        for n_jobs in n_jobss:\n            fcts = fcts_model(X_train, y_train, n_jobs)\n\n            for n in n_obs:\n                for method in methods:\n\n                    fct1, fct2, fct3 = fcts[method]\n\n                    if not allow_configuration(n=n, nfeat=nfeat,\n                                               n_jobs=n_jobs, method=method):\n                        continue\n\n                    obs = dict(n_obs=n, nfeat=nfeat, method=method,\n                               n_jobs=n_jobs)\n\n                    # creates different inputs to avoid caching in any ways\n                    Xs = []\n                    for r in range(repeat):\n                        x = numpy.empty((n, nfeat))\n                        x[:, :] = rand(n, nfeat)[:, :]\n                        Xs.append(x.astype(numpy.float32))\n\n                    # measures the baseline\n                    with config_context(assume_finite=True):\n                        st = time()\n                        repeated = 0\n                        for X in Xs:\n                            p1 = fct1(X)\n                            repeated += 1\n                            if time() - st >= 1:\n                                break  # stops if longer than a second\n                        end = time()\n                        obs[\"time_skl\"] = (end - st) / repeated\n\n                    # measures the new implementation\n                    st = time()\n                    r2 = 0\n                    for X in Xs:\n                        p2 = fct2(X)\n                        r2 += 1\n                        if r2 >= repeated:\n                            break\n                    end = time()\n                    obs[\"time_ort\"] = (end - st) / r2\n\n                    # measures the other new implementation\n                    st = time()\n                    r2 = 0\n                    for X in Xs:\n                        p2 = fct3(X)\n                        r2 += 1\n                        if r2 >= repeated:\n                            break\n                    end = time()\n                    obs[\"time_mlprodict\"] = (end - st) / r2\n\n                    # final\n                    res.append(obs)\n                    if verbose and (len(res) % 1 == 0 or n >= 10000):\n                        print(\"bench\", len(res), \":\", obs)\n\n                    # checks that both produce the same outputs\n                    if n <= 10000:\n                        if len(p1.shape) == 1 and len(p2.shape) == 2:\n                            p2 = p2.ravel()\n                        try:\n                            assert_almost_equal(\n                                p1.ravel(), p2.ravel(), decimal=5)\n                        except AssertionError as e:\n                            warnings.warn(str(e))\n    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_rf_models(dfr):\n\n    def autolabel(ax, rects):\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('%1.1fx' % height,\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom',\n                        fontsize=8)\n\n    engines = [_.split('_')[-1] for _ in dfr.columns if _.startswith(\"time_\")]\n    engines = [_ for _ in engines if _ != 'skl']\n    for engine in engines:\n        dfr[\"speedup_%s\" % engine] = dfr[\"time_skl\"] / dfr[\"time_%s\" % engine]\n    print(dfr.tail().T)\n\n    ncols = 2\n    fig, axs = plt.subplots(len(engines), ncols, figsize=(\n        14, 4 * len(engines)), sharey=True)\n\n    row = 0\n    for row, engine in enumerate(engines):\n        pos = 0\n        name = \"LinearRegression - %s\" % engine\n        for nf in sorted(set(dfr.nfeat)):\n            for n_jobs in sorted(set(dfr.n_jobs)):\n                sub = dfr[(dfr.nfeat == nf) & (dfr.n_jobs == n_jobs)]\n                ax = axs[row, pos]\n                labels = sub.n_obs\n                means = sub[\"speedup_%s\" % engine]\n\n                x = numpy.arange(len(labels))\n                width = 0.90\n\n                rects1 = ax.bar(x, means, width, label='Speedup')\n                if pos == 0:\n                    ax.set_yscale('log')\n                    ax.set_ylim([0.1, max(dfr[\"speedup_%s\" % engine])])\n\n                if pos == 0:\n                    ax.set_ylabel('Speedup')\n                ax.set_title('%s\\n%d features\\n%d jobs' % (name, nf, n_jobs))\n                if row == len(engines) - 1:\n                    ax.set_xlabel('batch size')\n                ax.set_xticks(x)\n                ax.set_xticklabels(labels)\n                autolabel(ax, rects1)\n                for tick in ax.xaxis.get_major_ticks():\n                    tick.label.set_fontsize(8)\n                for tick in ax.yaxis.get_major_ticks():\n                    tick.label.set_fontsize(8)\n                pos += 1\n\n    fig.tight_layout()\n    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run benchs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@ignore_warnings(category=FutureWarning)\ndef run_bench(repeat=250, verbose=False):\n    n_obs = [1, 10, 100, 1000, 10000]\n    methods = ['predict']\n    n_features = [10, 50]\n    n_jobss = [cpu_count()]\n\n    start = time()\n    results = bench(n_obs, n_features, n_jobss,\n                    methods, repeat=repeat, verbose=verbose)\n    end = time()\n\n    results_df = pandas.DataFrame(results)\n    print(\"Total time = %0.3f sec cpu=%d\\n\" % (end - start, cpu_count()))\n\n    # plot the results\n    return results_df\n\n\nname = \"plot_linear_regression\"\ndf = run_bench(verbose=True)\ndf.to_csv(\"%s.csv\" % name, index=False)\ndf.to_excel(\"%s.xlsx\" % name, index=False)\nfig, ax = plot_rf_models(df)\nfig.savefig(\"%s.png\" % name)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}