{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TreeEnsembleRegressor and parallelisation\n\nThe operator `TreeEnsembleClassifier\n<https://github.com/onnx/onnx/blob/master/docs/Operators-ml.md#\nai.onnx.ml.TreeEnsembleClassifier>`_ describe any tree model\n(decision tree, random forest, gradient boosting). The runtime\nis usually implements in C/C++ and uses parallelisation.\nThe notebook studies the impact of the parallelisation.\n    :local\n\n## Graph\n\nThe following dummy graph shows the time ratio between two runtimes\ndepending on the number of observations in a batch (N) and\nthe number of trees in the forest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom mlprodict.onnxrt import OnnxInference\nfrom onnxruntime import InferenceSession\nfrom skl2onnx import to_onnx\nfrom mlprodict.onnxrt.validate.validate_benchmark import benchmark_fct\nimport sklearn\nimport numpy\nfrom tqdm import tqdm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\nfrom mlprodict.plotting.plotting import plot_benchmark_metrics\n\n\ndef plot_metric(metric, ax=None, xlabel=\"N\", ylabel=\"trees\", middle=1.,\n                transpose=False, shrink=1.0, title=None, figsize=None):\n    if figsize is not None and ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax, cbar = plot_benchmark_metrics(\n        metric, ax=ax, xlabel=xlabel, ylabel=ylabel, middle=middle,\n        transpose=transpose, cbar_kw={'shrink': shrink})\n    if title is not None:\n        ax.set_title(title)\n    return ax\n\n\ndata = {(1, 1): 0.1, (10, 1): 1, (1, 10): 2,\n        (10, 10): 100, (100, 1): 100, (100, 10): 1000}\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nplot_metric(data, ax[0], shrink=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(data, ax[1], transpose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## scikit-learn: T trees vs 1 tree\n\nLet's do first compare a *GradientBoostingClassifier* from\n*scikit-learn* with 1 tree against multiple trees.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In[4]:\n\n\nntest = 10000\nX, y = make_classification(\n    n_samples=10000 + ntest, n_features=10, n_informative=5,\n    n_classes=2, random_state=11)\nX_train, X_test, y_train, y_test = X[:-\n                                     ntest], X[-ntest:], y[:-ntest], y[-ntest:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ModelToTest = GradientBoostingClassifier\n\nN = [1, 10, 100, 1000, 10000]\nT = [1, 2, 10, 20, 50]\n\nmodels = {}\nfor nt in tqdm(T):\n    rf = ModelToTest(n_estimators=nt, max_depth=7).fit(X_train, y_train)\n    models[nt] = rf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark(X, fct1, fct2, N, repeat=10, number=20):\n\n    def ti(r, n):\n        if n <= 1:\n            return 40 * r\n        if n <= 10:\n            return 10 * r\n        if n <= 100:\n            return 4 * r\n        if n <= 1000:\n            return r\n        return r // 2\n\n    with sklearn.config_context(assume_finite=True):\n        # to warm up the engine\n        time_kwargs = {n: dict(repeat=10, number=10) for n in N}\n        benchmark_fct(fct1, X, time_kwargs=time_kwargs, skip_long_test=False)\n        benchmark_fct(fct2, X, time_kwargs=time_kwargs, skip_long_test=False)\n        # real measure\n        time_kwargs = {n: dict(repeat=ti(repeat, n), number=number) for n in N}\n        res1 = benchmark_fct(\n            fct1, X, time_kwargs=time_kwargs, skip_long_test=False)\n        res2 = benchmark_fct(\n            fct2, X, time_kwargs=time_kwargs, skip_long_test=False)\n    res = {}\n    for r in sorted(res1):\n        r1 = res1[r]\n        r2 = res2[r]\n        ratio = r2['ttime'] / r1['ttime']\n        res[r] = ratio\n    return res\n\n\ndef tree_benchmark(X, fct1, fct2, T, N, repeat=20, number=10):\n    bench = {}\n    for t in tqdm(T):\n        if callable(X):\n            x = X(t)\n        else:\n            x = X\n        r = benchmark(x, fct1(t), fct2(t), N, repeat=repeat, number=number)\n        for n, v in r.items():\n            bench[n, t] = v\n    return bench\n\n\nbench = tree_benchmark(X_test.astype(numpy.float32),\n                       lambda t: models[1].predict,\n                       lambda t: models[t].predict, T, N)\n\nlist(bench.items())[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench, title=\"scikit-learn 1 tree vs scikit-learn T trees\\n\"\n            \"< 1 means onnxruntime is faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, all ratio on first line are close to 1 since\nboth models are the same. fourth line, second column\n(T=20, N=10) means an ensemble with 20 trees is slower to\ncompute the predictions of 10 observations in a batch compare\nto an ensemble with 1 tree.\n\n## scikit-learn against onnxuntime\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X32 = X_test.astype(numpy.float32)\nmodels_onnx = {t: to_onnx(m, X32[:1]) for t, m in models.items()}\n\n\nsess_models = {t: InferenceSession(mo.SerializeToString())\n               for t, mo in models_onnx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bench_ort = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: models[t].predict_proba,\n    lambda t: (lambda x, t_=t, se=sess_models: se[t_].run(None, {'X': x})),\n    T, N)\nbench_ort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_ort, title=\"scikit-learn vs onnxruntime\\n < 1 \"\n            \"means onnxruntime is faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see onnxruntime is fast for small batches,\nstill faster but not that much for big batches.\n\n## ZipMap operator\n\nZipMap just creates a new container for the same results.\nThe copy may impact the ratio. Let's remove it from the equation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X32 = X_test.astype(numpy.float32)\nmodels_onnx = {t: to_onnx(m, X32[:1],\n                          options={ModelToTest: {'zipmap': False}})\n               for t, m in models.items()}\n\nsess_models = {t: InferenceSession(mo.SerializeToString())\n               for t, mo in models_onnx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bench_ort = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: models[t].predict_proba,\n    lambda t: (lambda x, t_=t, se=sess_models: se[t_].run(None, {'X': x})),\n    T, N)\n\nbench_ort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_ort, title=\"scikit-learn vs onnxruntime (no zipmap)\\n < 1 \"\n            \"means onnxruntime is faster\")\n\n\n# ZipMap removal significantly improves.\n#\n# Implementation details for mlprodict runtime\n# ++++++++++++++++++++++++++++++++++++++++++++\n#\n# The runtime implemented in :epkg:`mlprodict` mostly relies on\n# two files:\n# * `op_tree_ensemble_common_p_agg_.hpp <https://github.com/sdpython/\n#   mlprodict/blob/master/mlprodict/onnxrt/ops_cpu/\n#   op_tree_ensemble_common_p_agg_.hpp>`_\n# * `op_tree_ensemble_common_p_.hpp <https://github.com/sdpython/\n#   mlprodict/blob/master/mlprodict/onnxrt/ops_cpu/\n#   op_tree_ensemble_common_p_.hpp>`_\n#\n# The runtime builds a tree structure, computes the output of every\n# tree and then agregates them. The implementation distringuishes\n# when the batch size contains only 1 observations or many.\n# It parallelizes on the following conditions:\n# * if the batch size $N \\geqslant N_0$, it then parallelizes per\n#   observation, asuming every one is independant,\n# * if the batch size $N = 1$ and the number of trees\n#   $T \\geqslant T_0$, it then parallelizes per tree.\n#\n# scikit-learn against mlprodict, no parallelisation\n# ++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\noinf_models = {t: OnnxInference(mo, runtime=\"python_compiled\")\n               for t, mo in models_onnx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's modify the thresholds which trigger the parallelization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for _, oinf in oinf_models.items():\n    oinf.sequence_[0].ops_.rt_.omp_tree_ = 10000000\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 10000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bench_mlp = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: models[t].predict,\n    lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),\n    T, N)\nbench_mlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_mlp, title=\"scikit-learn vs mlprodict\\n < 1 \"\n            \"means mlprodict is faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare *onnxruntime* against *mlprodict*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bench_mlp_ort = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: (lambda x, t_=t, se=sess_models: se[t_].run(None, {'X': x})),\n    lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),\n    T, N)\nbench_mlp_ort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_mlp_ort, title=\"onnxruntime vs mlprodict\\n < 1 means \"\n            \"mlprodict is faster\\nno parallelisation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implementation is faster except for high number of trees\nor high number of observations. Let's add parallelisation for\ntrees and observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for _, oinf in oinf_models.items():\n    oinf.sequence_[0].ops_.rt_.omp_tree_ = 2\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 2\n\n\nbench_mlp_para = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: models[t].predict,\n    lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),\n    T, N)\n\nbench_mlp_para"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_mlp_para, title=\"scikit-learn vs mlprodict\\n < 1 means \"\n            \"mlprodict is faster\\nparallelisation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallelisation does improve the computation time when N is big.\nLet's compare with and without parallelisation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bench_para = {}\nfor k, v in bench_mlp.items():\n    bench_para[k] = bench_mlp_para[k] / v\n\n\nplot_metric(bench_para, title=\"mlprodict vs mlprodict parallelized\\n < 1 \"\n            \"means parallelisation is faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallelisation per trees does not seem to be efficient.\nLet's confirm with a proper benchmark as the previous merges\nresults from two benchmarks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for _, oinf in oinf_models.items():\n    oinf.sequence_[0].ops_.rt_.omp_tree_ = 1000000\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000\n\noinf_models_para = {t: OnnxInference(mo, runtime=\"python_compiled\")\n                    for t, mo in models_onnx.items()}\n\nfor _, oinf in oinf_models_para.items():\n    oinf.sequence_[0].ops_.rt_.omp_tree_ = 2\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 2\n\nbench_mlp_para = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),\n    lambda t: (lambda x, t_=t, oi=oinf_models_para: oi[t_].run({'X': x})),\n    T, N, repeat=20, number=20)\n\nbench_mlp_para"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_mlp_para, title=\"mlprodict vs mlprodict parallelized\\n < 1 \"\n            \"means parallelisation is faster\\nsame baseline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It should be run on different machines. On the current one,\nparallelisation per trees (when N=1) does not seem to help.\nParallelisation for a small number of observations does not\nseem to help either. So we need to find some threshold.\n\n## Parallelisation per trees\n\nLet's study the parallelisation per tree. We need to train new models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In[33]:\n\n\nN2 = [1, 10]\nT2 = [1, 2, 10, 50, 100, 150, 200, 300, 400, 500]\n\nmodels2 = {}\nfor nt in tqdm(T2):\n    rf = ModelToTest(n_estimators=nt, max_depth=7).fit(X_train, y_train)\n    models2[nt] = rf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversion to ONNX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X32 = X_test.astype(numpy.float32)\nmodels2_onnx = {t: to_onnx(m, X32[:1]) for t, m in models2.items()}\n\noinf_models2 = {t: OnnxInference(mo, runtime=\"python_compiled\")\n                for t, mo in models2_onnx.items()}\nfor _, oinf in oinf_models2.items():\n    oinf.sequence_[0].ops_.rt_.omp_tree_ = 1000000\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000\n\noinf_models2_para = {t: OnnxInference(\n    mo, runtime=\"python_compiled\") for t, mo in models2_onnx.items()}\nfor _, oinf in oinf_models2_para.items():\n    oinf.sequence_[0].ops_.rt_.omp_tree_ = 2\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In[36]:\n\n\nbench_mlp_tree = tree_benchmark(\n    X_test.astype(numpy.float32),\n    lambda t: (lambda x, t_=t, oi=oinf_models2: oi[t_].run({'X': x})),\n    lambda t: (lambda x, t_=t, oi=oinf_models2_para: oi[t_].run({'X': x})),\n    T2, N2, repeat=20, number=20)\nbench_mlp_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(\n    bench_mlp_tree, transpose=True, figsize=(10, 3), shrink=0.5,\n    title=\"mlprodict vs mlprodict parallelized\\n < 1 means parallelisation \"\n    \"is faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parallelisation starts to be below 1 after 400 trees.\nFor 10 observations, there is no parallelisation neither by\ntrees nor by observations. Ratios are close to 1.\nThe gain obviously depends on the tree depth. You can\ntry with a different max depth and the number of trees\nparallelisation becomes interesting depending on the tree depth.\n\n## Multi-Class DecisionTreeClassifier\n\nSame experiment when the number of tree is 1 but then we\nchange the number of classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ModelToTest = DecisionTreeClassifier\n\nC = [2, 5, 10, 15, 20, 30, 40, 50]\nN = [1, 10, 100, 1000, 10000]\ntrees = {}\nfor cl in tqdm(C):\n\n    ntest = 10000\n    X, y = make_classification(\n        n_samples=10000 + ntest, n_features=12, n_informative=8,\n        n_classes=cl, random_state=11)\n    X_train, X_test, y_train, y_test = (\n        X[:-ntest], X[-ntest:], y[:-ntest], y[-ntest:])\n\n    dt = ModelToTest(max_depth=7).fit(X_train, y_train)\n\n    X32 = X_test.astype(numpy.float32)\n    monnx = to_onnx(dt, X32[:1])\n    oinf = OnnxInference(monnx)\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000\n    trees[cl] = dict(model=dt, X_test=X_test, X32=X32, monnx=monnx, oinf=oinf)\n\n\nbench_dt = tree_benchmark(lambda cl: trees[cl]['X32'],\n                          lambda cl: trees[cl]['model'].predict_proba,\n                          lambda cl: (\n                              lambda x, c=cl: trees[c]['oinf'].run({'X': x})),\n                          C, N)\n\nbench_dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_dt, ylabel=\"classes\", transpose=True, shrink=0.75,\n            title=\"scikit-learn vs mlprodict (DecisionTreeClassifier) \\n\"\n            \"< 1 means mlprodict is faster\\n no parallelisation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-class LogisticRegression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ModelToTest = LogisticRegression\n\nC = [2, 5, 10, 15, 20]\nN = [1, 10, 100, 1000, 10000]\n\nmodels = {}\nfor cl in tqdm(C):\n\n    ntest = 10000\n    X, y = make_classification(\n        n_samples=10000 + ntest, n_features=10, n_informative=6,\n        n_classes=cl, random_state=11)\n    X_train, X_test, y_train, y_test = (\n        X[:-ntest], X[-ntest:], y[:-ntest], y[-ntest:])\n\n    model = ModelToTest().fit(X_train, y_train)\n\n    X32 = X_test.astype(numpy.float32)\n    monnx = to_onnx(model, X32[:1])\n    oinf = OnnxInference(monnx)\n    models[cl] = dict(model=model, X_test=X_test,\n                      X32=X32, monnx=monnx, oinf=oinf)\n\n\nbench_lr = tree_benchmark(lambda cl: models[cl]['X32'],\n                          lambda cl: models[cl]['model'].predict_proba,\n                          lambda cl: (\n                              lambda x, c=cl: trees[c]['oinf'].run({'X': x})),\n                          C, N)\nbench_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_lr, ylabel=\"classes\",\n            title=\"scikit-learn vs mlprodict (LogisticRegression) \\n\"\n            \"< 1 means mlprodict is faster\\n no parallelisation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Tree and number of features\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ModelToTest = DecisionTreeClassifier\n\nNF = [2, 10, 20, 40, 50, 70, 100, 200, 500, 1000]\nN = [1, 10, 100, 1000, 10000, 50000]\ntrees_nf = {}\n\nfor nf in tqdm(NF):\n    ntest = 10000\n    X, y = make_classification(\n        n_samples=10000 + ntest, n_features=nf, n_informative=nf // 2 + 1,\n        n_redundant=0, n_repeated=0,\n        n_classes=2, random_state=11)\n    X_train, X_test, y_train, y_test = (\n        X[:-ntest], X[-ntest:], y[:-ntest], y[-ntest:])\n\n    dt = ModelToTest(max_depth=7).fit(X_train, y_train)\n\n    X32 = X_test.astype(numpy.float32)\n    monnx = to_onnx(dt, X32[:1])\n    oinf = OnnxInference(monnx)\n    oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000\n    trees_nf[nf] = dict(model=dt, X_test=X_test,\n                        X32=X32, monnx=monnx, oinf=oinf)\n\n\nbench_dt_nf = tree_benchmark(\n    lambda nf: trees_nf[nf]['X32'],\n    lambda nf: trees_nf[nf]['model'].predict_proba,\n    lambda nf: (lambda x, c=nf: trees_nf[c]['oinf'].run({'X': x})), NF, N)\nbench_dt_nf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(\n    bench_dt_nf, ylabel=\"number of features\", transpose=True, figsize=(10, 4),\n    title=\"scikit-learn vs mlprodict (DecisionTreeClassifier) \\n\"\n    \"< 1 means mlprodict is faster\\n no parallelisation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}