{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmark Random Forests, Tree Ensemble\n\nThe following script benchmarks different libraries\nimplementing random forests and boosting trees.\nThis benchmark can be replicated by installing the\nfollowing packages:\n\n::\n\n    python -m virtualenv env\n    cd env\n    pip install -i https://test.pypi.org/simple/ ort-nightly\n    pip install git+https://github.com/microsoft/onnxconverter-common.git@jenkins\n    pip install git+https://https://github.com/xadupre/sklearn-onnx.git@jenkins\n    pip install mlprodict matplotlib scikit-learn pandas threadpoolctl\n    pip install mlprodict lightgbm xgboost jinja2\n\n## Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport pickle\nfrom pprint import pprint\nimport numpy\nimport pandas\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom onnxruntime import InferenceSession\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom skl2onnx import to_onnx\nfrom mlprodict.onnx_conv import register_converters\nfrom mlprodict.onnxrt.validate.validate_helper import measure_time\nfrom mlprodict.onnxrt import OnnxInference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Registers new converters for :epkg:`sklearn-onnx`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "register_converters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_depth = 7\nn_classes = 20\nn_estimators = 500\nn_features = 100\nREPEAT = 3\nNUMBER = 1\ntrain, test = 1000, 10000\n\nprint('dataset')\nX_, y_ = make_classification(n_samples=train + test, n_features=n_features,\n                             n_classes=n_classes, n_informative=n_features - 3)\nX_ = X_.astype(numpy.float32)\ny_ = y_.astype(numpy.int64)\nX_train, X_test = X_[:train], X_[train:]\ny_train, y_test = y_[:train], y_[train:]\n\ncompilation = []\n\n\ndef train_cache(model, X_train, y_train, max_depth, n_estimators, n_classes):\n    name = \"cache-{}-N{}-f{}-d{}-e{}-cl{}.pkl\".format(\n        model.__class__.__name__, X_train.shape[0], X_train.shape[1],\n        max_depth, n_estimators, n_classes)\n    if os.path.exists(name):\n        with open(name, 'rb') as f:\n            return pickle.load(f)\n    else:\n        model.fit(X_train, y_train)\n        with open(name, 'wb') as f:\n            pickle.dump(model, f)\n        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RandomForestClassifier\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\nprint('train')\nrf = train_cache(rf, X_train, y_train, max_depth, n_estimators, n_classes)\n\nres = measure_time(rf.predict_proba, X_test[:10],\n                   repeat=REPEAT, number=NUMBER,\n                   div_by_number=True, first_run=True)\nres['model'], res['runtime'] = rf.__class__.__name__, 'INNER'\npprint(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ONNX\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def measure_onnx_runtime(model, xt, repeat=REPEAT, number=NUMBER,\n                         verbose=True):\n    if verbose:\n        print(model.__class__.__name__)\n\n    res = measure_time(model.predict_proba, xt,\n                       repeat=repeat, number=number,\n                       div_by_number=True, first_run=True)\n    res['model'], res['runtime'] = model.__class__.__name__, 'INNER'\n    res['N'] = X_test.shape[0]\n    res[\"max_depth\"] = max_depth\n    res[\"n_estimators\"] = n_estimators\n    res[\"n_features\"] = n_features\n    if verbose:\n        pprint(res)\n    yield res\n\n    onx = to_onnx(model, X_train[:1], options={id(model): {'zipmap': False}})\n\n    oinf = OnnxInference(onx)\n    res = measure_time(lambda x: oinf.run({'X': x}), xt,\n                       repeat=repeat, number=number,\n                       div_by_number=True, first_run=True)\n    res['model'], res['runtime'] = model.__class__.__name__, 'NPY/C++'\n    res['N'] = X_test.shape[0]\n    res['size'] = len(onx.SerializeToString())\n    res[\"max_depth\"] = max_depth\n    res[\"n_estimators\"] = n_estimators\n    res[\"n_features\"] = n_features\n    if verbose:\n        pprint(res)\n    yield res\n\n    sess = InferenceSession(onx.SerializeToString())\n    res = measure_time(lambda x: sess.run(None, {'X': x}), xt,\n                       repeat=repeat, number=number,\n                       div_by_number=True, first_run=True)\n    res['model'], res['runtime'] = model.__class__.__name__, 'ORT'\n    res['N'] = X_test.shape[0]\n    res['size'] = len(onx.SerializeToString())\n    res[\"max_depth\"] = max_depth\n    res[\"n_estimators\"] = n_estimators\n    res[\"n_features\"] = n_features\n    if verbose:\n        pprint(res)\n    yield res\n\n\ncompilation.extend(list(measure_onnx_runtime(rf, X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HistGradientBoostingClassifier\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hist = HistGradientBoostingClassifier(\n    max_iter=n_estimators, max_depth=max_depth)\nprint('train')\nhist = train_cache(hist, X_train, y_train, max_depth, n_estimators, n_classes)\n\ncompilation.extend(list(measure_onnx_runtime(hist, X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LightGBM\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lgb = LGBMClassifier(n_estimators=n_estimators,\n                     max_depth=max_depth, pred_early_stop=False)\nprint('train')\nlgb = train_cache(lgb, X_train, y_train, max_depth, n_estimators, n_classes)\n\ncompilation.extend(list(measure_onnx_runtime(lgb, X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xgb = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth)\nprint('train')\nxgb = train_cache(xgb, X_train, y_train, max_depth, n_estimators, n_classes)\n\ncompilation.extend(list(measure_onnx_runtime(xgb, X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nAll data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = 'plot_time_tree_ensemble'\ndf = pandas.DataFrame(compilation)\ndf.to_csv('%s.csv' % name, index=False)\ndf.to_excel('%s.xlsx' % name, index=False)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time per model and runtime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piv = df.pivot(\"model\", \"runtime\", \"average\")\npiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = piv.T.plot(kind=\"bar\")\nax.set_title(\"Computation time ratio for %d observations and %d features\\n\"\n             \"lower is better for onnx runtimes\" % X_test.shape)\nplt.savefig('%s.png' % name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nprint(code_optimisation())\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}