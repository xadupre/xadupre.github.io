{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TopK benchmark\n\nThis example compares :epkg:`onnxruntime` and :epkg:`mlprodict`\nfor an implementation of operator `TopK\n<https://github.com/onnx/onnx/blob/master/docs/Operators.md#TopK>`_.\nWe measure two runtimes by computing a ratio between their\ntime execution through the following kind of graphs.\n\n## Graph to compare performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from numpy.random import randn\nimport numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom onnxruntime import InferenceSession, __version__ as ort_version\nfrom tqdm import tqdm\nfrom cpyquickhelper.numbers import measure_time\nfrom pyquickhelper.pycode.profiling import profile\nfrom skl2onnx.algebra.onnx_ops import OnnxTopK_11\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom skl2onnx.algebra.onnx_ops import OnnxTopK\nfrom mlprodict.onnxrt.validate.validate_benchmark import benchmark_fct\nfrom mlprodict.onnxrt import OnnxInference\nfrom mlprodict.onnxrt.ops_cpu.op_topk import (\n    topk_sorted_implementation, topk_sorted_implementation_cpp)\nfrom mlprodict import __version__ as mlp_version\nfrom mlprodict.plotting.plotting import plot_benchmark_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available optimisation on this machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nprint(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_metric(metric, ax=None, xlabel=\"N\", ylabel=\"k\", middle=1.,\n                transpose=False, shrink=1.0, title=None):\n    ax, cbar = plot_benchmark_metrics(\n        metric, ax=ax, xlabel=xlabel, ylabel=ylabel, middle=middle,\n        transpose=transpose, cbar_kw={'shrink': shrink})\n    if title is not None:\n        ax.set_title(title)\n    return ax\n\n\ndata = {(1, 1): 0.1, (10, 1): 1, (1, 10): 2,\n        (10, 10): 100, (100, 1): 100, (100, 10): 1000}\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nplot_metric(data, ax[0], shrink=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(data, ax[1], transpose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TopK in ONNX\n\nThe following lines creates an ONNX graph using\none TopK ONNX node. The outcome is the ONNX graph\nconverted into json.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X32 = randn(100000, 100).astype(numpy.float32)\n\nnode = OnnxTopK_11('X', numpy.array([5], dtype=numpy.int64),\n                   output_names=['dist', 'ind'])\n\nmodel_onnx = node.to_onnx(\n    [('X', X32)], target_opset=12,\n    # shape inference does not seem to work in onnxruntime\n    # so we speccify the output shape\n    outputs=[('dist', X32[:1, :5]),\n             ('ind', X32[:1, :5].astype(numpy.int64))])\nmodel_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That gives...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oinf = OnnxInference(model_onnx, runtime=\"python\")\nres = oinf.run({'X': X32})\ndist, ind = res['dist'], res['ind']\ndist[:2], ind[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With onnxruntime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(model_onnx.SerializeToString())\ndist, ind = sess.run(None, {'X': X32})\ndist[:2], ind[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare two implementations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def benchmark(X, fct1, fct2, N, repeat=10, number=10):\n\n    def ti(n):\n        if n <= 1:\n            return 50\n        if n <= 1000:\n            return 2\n        if n <= 10000:\n            return 0.51\n        return 0.11\n\n    # to warm up the engine\n    time_kwargs = {n: dict(repeat=10, number=10) for n in N[:2]}\n    benchmark_fct(fct1, X, time_kwargs=time_kwargs, skip_long_test=False)\n    benchmark_fct(fct2, X, time_kwargs=time_kwargs, skip_long_test=False)\n    # real measure\n    time_kwargs = {n: dict(repeat=int(repeat * ti(n)),\n                           number=int(number * ti(n))) for n in N}\n    res1 = benchmark_fct(fct1, X, time_kwargs=time_kwargs,\n                         skip_long_test=False)\n    res2 = benchmark_fct(fct2, X, time_kwargs=time_kwargs,\n                         skip_long_test=False)\n\n    res = {}\n    for r in sorted(res1):\n        r1 = res1[r]\n        r2 = res2[r]\n        ratio = r2['ttime'] / r1['ttime']\n        res[r] = ratio\n    return res\n\n\nN = [1, 10, 100, 1000, 10000, 100000]\nres = benchmark(X32, lambda x: sess.run(None, {'X': x}),\n                lambda x: oinf.run({'X': x}), N=N)\nres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The implementation in `mlprodict\n<https://github.com/sdpython/mlprodict/blob/master/\nmlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpp#L246>`_\nis faster when the number of rows grows. It is faster\nfor 1 rows, for many rows, the implementation\nuses openmp to parallelize.\n\n## C++ implementation vs numpy\n\n:epkg:`scikit-learn` uses :epkg:`numpy` to compute the top *k* elements.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res = benchmark(X32, lambda x: topk_sorted_implementation(x, 5, 1, 0),\n                lambda x: topk_sorted_implementation_cpp(x, 5, 1, 0), N=N)\nres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems to be faster too. Let's profile.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xr = randn(1000000, 100)\ntext = profile(lambda: topk_sorted_implementation(xr, 5, 1, 0),\n               pyinst_format='text')[1]\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallelisation\n\nWe need to disable the parallelisation to\nreally compare both implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In[11]:\n\n\ndef benchmark_test(X, fct1, fct2, N, K, repeat=10, number=10):\n    res = {}\n    for k in tqdm(K):\n        def f1(x, k=k): return fct1(x, k=k)\n        def f2(x, k=k): return fct2(x, k=k)\n        r = benchmark(X32, f1, f2, N=N, repeat=repeat, number=number)\n        for n, v in r.items():\n            res[n, k] = v\n    return res\n\n\nK = [1, 2, 5, 10, 15]\nN = [1, 2, 3, 10, 100, 1000, 10000]\n\nbench_para = benchmark_test(\n    X32, (lambda x, k: topk_sorted_implementation_cpp(\n        x, k=k, axis=1, largest=0, th_para=100000000)),\n    (lambda x, k: topk_sorted_implementation_cpp(\n        x, k=k, axis=1, largest=0, th_para=1)),\n    N=N, K=K)\n\nbench_para"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_para, transpose=False, title=\"TopK and parallelisation\\n\"\n            \"< 1 means parallelisation is faster\", shrink=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is somehow expected. First column is closed to\n1 as it is the same code being compared. Next columns\nare red, meaning the parallelisation does not help,\nthe parallelisation helps for bigger N, as least more than 100.\n\n## Parallellisation with ONNX\n\nWe replicate the same experiment with an ONNX graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "k_ = numpy.array([3], dtype=numpy.int64)\nnode = OnnxTopK_11('X', 'k',\n                   output_names=['dist', 'ind'])\n\nmodel_onnx = node.to_onnx(\n    [('X', X32), ('k', k_)], target_opset=12,\n    # shape inference does not seem to work in onnxruntime\n    # so we speccify the output shape\n    outputs=[('dist', X32[:1, :5]),\n             ('ind', X32[:1, :5].astype(numpy.int64))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oinf_no_para = OnnxInference(model_onnx, runtime=\"python\")\nres = oinf_no_para.run({'X': X32, 'k': k_})\ndist, ind = res['dist'], res['ind']\ndist[:2], ind[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's play with the thresholds triggering the parallelisation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oinf_para = OnnxInference(model_onnx, runtime=\"python\")\noinf_no_para.sequence_[0].ops_.th_para = 100000000\noinf_para.sequence_[0].ops_.th_para = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bench_onnx_para = benchmark_test(\n    X32, (lambda x, k: oinf_no_para.run(\n        {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),\n    (lambda x, k: oinf_para.run(\n        {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),\n    N=N, K=K)\nbench_onnx_para"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_onnx_para, transpose=False,\n            title=\"TopK and parallelisation with ONNX\\n< 1 means \"\n            \"parallelisation is faster\", shrink=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pretty much the same results.\n\n## onnxruntime vs mlprodict (no parallelisation)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sess = InferenceSession(model_onnx.SerializeToString())\n\n\nbench_ort = benchmark_test(\n    X32, (lambda x, k: sess.run(\n        None, {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),\n    (lambda x, k: oinf_no_para.run(\n        {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),\n    N=N, K=K)\nbench_ort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_ort, transpose=False,\n            title=\"TopK, onnxruntime vs mlprodict\\n< 1 means mlprodict \"\n            \"is faster\\nno parallelisation\", shrink=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems the implementation of operator TopK in\nonnxruntime 1.1.1 can be improved.\n\nVersions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ort_version, mlp_version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And with parallelisation above 50 rows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oinf_para.sequence_[0].ops_.th_para = 50\nbench_ort_para = benchmark_test(\n    X32, (lambda x, k: sess.run(\n        None, {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),\n    (lambda x, k: oinf_para.run(\n        {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),\n    N=N, K=K)\nbench_ort_para"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_metric(bench_ort_para, transpose=False,\n            title=\"TopK, onnxruntime vs mlprodict\\n< 1 means mlprodict \"\n            \"is faster\\nparallelisation above 50 rows\", shrink=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "onnxruntime and mlprodict implement the same algorithm.\n The only difference comes from the threshold which\n trigger the parallelisation. It should depend on the machine.\n That explains the difference in time for 100 observations.\n\n#############################\n Interesting...\n\n Comparison with onnxruntime\n +++++++++++++++++++++++++++\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = numpy.array([\n    [0, 1, 2, 3],\n    [4, 5, 6, 7],\n    [8, 9, 10, 11],\n], dtype=numpy.float32)\n\nK = numpy.array([3], dtype=numpy.int64)\n\n\nnode = OnnxTopK('X', K, output_names=['values', 'indices'],\n                op_version=12)\nonx = node.to_onnx([('X', FloatTensorType())])\n\npy_topk = OnnxInference(onx, runtime=\"python_compiled\")\nort_topk = InferenceSession(onx.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r1 = py_topk.run({'X': X})\nr1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2 = ort_topk.run(None, {'X': X})\nr2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some figures.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bs = []\nbs.append(measure_time(lambda: py_topk.run({'X': X}),\n                       context=globals(), div_by_number=True))\nbs[-1]['c'] = 'py'\nbs[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bs.append(measure_time(lambda: ort_topk.run(None, {'X': X}),\n                       context=globals(), div_by_number=True))\nbs[-1]['c'] = 'or'\nbs[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = numpy.random.randn(10000, 100).astype(numpy.float32)\n\n\nbs.append(measure_time(lambda: py_topk.run({'X': X}),\n                       context=globals(), div_by_number=True))\nbs[-1]['c'] = 'py-100'\nbs[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bs.append(measure_time(lambda: ort_topk.run(None, {'X': X}),\n                       context=globals(), div_by_number=True))\nbs[-1]['c'] = 'ort-100'\nbs[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataFrame(bs)\ndf"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}