{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compares implementations of Tranpose\n\nThis example compares the :epkg:`numpy:transpose` from numpy,\nto :epkg:`onnxruntime` implementation.\nIf available, :epkg:`tensorflow` and :epkg:`pytorch` are included as well.\n\n## Available optimisation\n\nThe code shows which parallelisation optimisation could be used,\n*AVX* or *SSE* and the number of available processors.\nBoth :epkg:`numpy` and :epkg:`torch` have lazy implementations,\nthe function switches dimensions and strides but does not move\nany data. That's why function *contiguous* was called in both cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nimport pandas\nimport matplotlib.pyplot as plt\nfrom onnxruntime import InferenceSession\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom skl2onnx.algebra.onnx_ops import OnnxTranspose\nfrom cpyquickhelper.numbers import measure_time\nfrom tqdm import tqdm\nfrom mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation\nprint(code_optimisation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transpose implementations\n\nFunction einsum is used from tensorflow and pytorch\ninstead of transpose. The equation reflects the required\ntransposition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    from tensorflow import transpose as tf_transpose, convert_to_tensor\nexcept ImportError:\n    tf_transpose = None\ntry:\n    from torch import einsum as torch_einsum, from_numpy\nexcept ImportError:\n    torch_einsum = None\n\n\ndef build_ort_transpose(perm, op_version=12):\n    node = OnnxTranspose('x', perm=perm, op_version=op_version,\n                         output_names=['z'])\n    onx = node.to_onnx(inputs=[('x', FloatTensorType())],\n                       target_opset=op_version)\n    sess = InferenceSession(onx.SerializeToString())\n    return lambda x, y: sess.run(None, {'x': x})\n\n\ndef loop_fct(fct, xs, ys):\n    for x, y in zip(xs, ys):\n        fct(x, y)\n\n\ndef perm2eq(perm):\n    first = \"\".join(chr(97 + i) for i in range(len(perm)))\n    second = \"\".join(first[p] for p in perm)\n    return \"%s->%s\" % (first, second)\n\n\ndef benchmark_op(perm, repeat=5, number=5, name=\"Transpose\", shape_fct=None):\n    if shape_fct is None:\n        def shape_fct(dim): return (3, dim, 1, 512)\n    ort_fct = build_ort_transpose(perm)\n    res = []\n    for dim in tqdm([8, 16, 32, 64, 100, 128, 200,\n                     256, 400, 512, 1024]):\n        shape = shape_fct(dim)\n        n_arrays = 10 if dim < 512 else 4\n        xs = [numpy.random.rand(*shape).astype(numpy.float32)\n              for _ in range(n_arrays)]\n        ys = [perm for _ in range(n_arrays)]\n        equation = perm2eq(perm)\n        info = dict(perm=perm, shape=shape)\n\n        # numpy\n        ctx = dict(\n            xs=xs, ys=ys,\n            fct=lambda x, y: numpy.ascontiguousarray(numpy.transpose(x, y)),\n            loop_fct=loop_fct)\n        obs = measure_time(\n            \"loop_fct(fct, xs, ys)\",\n            div_by_number=True, context=ctx, repeat=repeat, number=number)\n        obs['dim'] = dim\n        obs['fct'] = 'numpy'\n        obs.update(info)\n        res.append(obs)\n\n        # onnxruntime\n        ctx['fct'] = ort_fct\n        obs = measure_time(\n            \"loop_fct(fct, xs, ys)\",\n            div_by_number=True, context=ctx, repeat=repeat, number=number)\n        obs['dim'] = dim\n        obs['fct'] = 'ort'\n        obs.update(info)\n        res.append(obs)\n\n        if tf_transpose is not None:\n            # tensorflow\n            ctx['fct'] = tf_transpose\n            ctx['xs'] = [convert_to_tensor(x) for x in xs]\n            ctx['ys'] = [convert_to_tensor(y) for y in ys]\n            obs = measure_time(\n                \"loop_fct(fct, xs, ys)\",\n                div_by_number=True, context=ctx, repeat=repeat, number=number)\n            obs['dim'] = dim\n            obs['fct'] = 'tf'\n            obs.update(info)\n            res.append(obs)\n\n            # tensorflow with copy\n            ctx['fct'] = lambda x, y: tf_transpose(\n                convert_to_tensor(x)).numpy()\n            ctx['xs'] = xs\n            ctx['ys'] = ys\n            obs = measure_time(\n                \"loop_fct(fct, xs, ys)\",\n                div_by_number=True, context=ctx, repeat=repeat, number=number)\n            obs['dim'] = dim\n            obs['fct'] = 'tf_copy'\n            obs.update(info)\n            res.append(obs)\n\n        if torch_einsum is not None:\n            # torch\n            ctx['fct'] = lambda x, y: torch_einsum(equation, x).contiguous()\n            ctx['xs'] = [from_numpy(x) for x in xs]\n            ctx['ys'] = ys  # [from_numpy(y) for y in ys]\n            obs = measure_time(\n                \"loop_fct(fct, xs, ys)\",\n                div_by_number=True, context=ctx, repeat=repeat, number=number)\n            obs['dim'] = dim\n            obs['fct'] = 'torch'\n            obs.update(info)\n            res.append(obs)\n\n    # Dataframes\n    shape_name = str(shape).replace(str(dim), \"N\")\n    df = pandas.DataFrame(res)\n    df.columns = [_.replace('dim', 'N') for _ in df.columns]\n    piv = df.pivot('N', 'fct', 'average')\n\n    rs = piv.copy()\n    for c in ['ort', 'torch', 'tf', 'tf_copy']:\n        if c in rs.columns:\n            rs[c] = rs['numpy'] / rs[c]\n    rs['numpy'] = 1.\n\n    # Graphs.\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    piv.plot(logx=True, logy=True, ax=ax[0],\n             title=\"%s benchmark\\n%r - %r - %s\"\n                   \" lower better\" % (name, shape_name, perm, equation))\n    ax[0].legend(prop={\"size\": 9})\n    rs.plot(logx=True, logy=True, ax=ax[1],\n            title=\"%s Speedup, baseline=numpy\\n%r - %r - %s\"\n                  \" higher better\" % (name, shape_name, perm, equation))\n    ax[1].plot([min(rs.index), max(rs.index)], [0.5, 0.5], 'g--')\n    ax[1].plot([min(rs.index), max(rs.index)], [2., 2.], 'g--')\n    ax[1].legend(prop={\"size\": 9})\n    return df, rs, ax\n\n\ndfs = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First permutation: (1, 0, 2, 3)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = (1, 0, 2, 3)\ndf, piv, ax = benchmark_op(perm)\ndfs.append(df)\ndf.pivot(\"fct\", \"N\", \"average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second permutation: (0, 1, 3, 2)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = (1, 0, 3, 2)\ndf, piv, ax = benchmark_op(perm)\ndfs.append(df)\ndf.pivot(\"fct\", \"N\", \"average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Third permutation: (0, 2, 1, 3)\n\nThis transposition is equivalent to a reshape\nbecause it only moves the empty axis.\nThe comparison is entirely fair as the cost\nfor onnxruntime includes a copy from numpy to\nonnxruntime, a reshape = another copy, than a copy\nback to numpy. Tensorflow and pytorch seems\nto have a lazy implementation in this case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = (0, 2, 1, 3)\ndf, piv, ax = benchmark_op(perm)\ndfs.append(df)\ndf.pivot(\"fct\", \"N\", \"average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fourth permutation: (3, 1, 2, 0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = (3, 1, 2, 0)\ndf, piv, ax = benchmark_op(perm)\ndfs.append(df)\ndf.pivot(\"fct\", \"N\", \"average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fifth permutation: (1, 2, 3, 0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = (1, 2, 3, 0)\ndf, piv, ax = benchmark_op(perm)\ndfs.append(df)\ndf.pivot(\"fct\", \"N\", \"average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Six th permutation: (1, 2, 4, 3, 0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = (1, 2, 4, 3, 0)\ndf, piv, ax = benchmark_op(perm, shape_fct=lambda dim: (3, dim, 1, 8, 512))\ndfs.append(df)\ndf.pivot(\"fct\", \"N\", \"average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nAll libraries have similar implementations.\n:epkg:`onnxruntime` measures includes 2 mores copies,\none to copy from numpy container to onnxruntime container,\nanother one to copy back from onnxruntime container to numpy.\nParallelisation should be investigated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "merged = pandas.concat(dfs)\nname = \"transpose\"\nmerged.to_csv(\"plot_%s.csv\" % name, index=False)\nmerged.to_excel(\"plot_%s.xlsx\" % name, index=False)\nplt.savefig(\"plot_%s.png\" % name)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}