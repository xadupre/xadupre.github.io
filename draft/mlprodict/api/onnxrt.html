
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Python Runtime for ONNX &#8212; Python Runtime for ONNX</title>
    
    <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style_notebook_snippet.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinxtrib-images/LightBox2/lightbox2/css/lightbox.css" />
    <link rel="stylesheet" type="text/css" href="../_static/my-styles.css" />
    
    <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script src="../_static/require.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/jquery-1.11.0.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2/js/lightbox.min.js"></script>
    <script src="../_static/sphinxtrib-images/LightBox2/lightbox2_customize/jquery-noconflict.js"></script>
    <link rel="shortcut icon" href="../_static/project_ico.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Python Runtime for ONNX operators" href="onnxrt_ops.html" />
    <link rel="prev" title="AST" href="ast.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/project_ico.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../installation.html">
  Installation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../tutorial/index.html">
  Tutorial
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  API
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../onnx.html">
  ONNX, Runtime, Backends
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../onnx_bench.html">
  scikit-learn Converters and Benchmarks
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_cmd.html">
  Command lines
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_ex.html">
  Examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../i_index.html">
  FAQ, code, â€¦
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../gyexamples/index.html">
  Gallery of examples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../all_notebooks.html">
  Notebook Gallery
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../HISTORY.html">
  History
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="onnx_conv.html">
   Additional ONNX Converters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sklapi.html">
   scikit-learn API and ONNX graph in pipelines
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="npy.html">
   Complete Numpy API for ONNX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xop.html">
   Xop API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ast.html">
   AST
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Python Runtime for ONNX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="onnxrt_ops.html">
   Python Runtime for ONNX operators
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="testing.html">
   Experimental implementations
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="asv.html">
   Benchmarking with asv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="validation.html">
   Validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tools.html">
   Tools
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="cc_grammar.html">
   Former Experiments
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backend-validation">
   Backend validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#python-to-onnx">
   Python to ONNX
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#onnx-export">
   ONNX Export
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#onnx-structure">
   ONNX Structure
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#onnxruntime">
   onnxruntime
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#validation-of-scikit-learn-models">
   Validation of scikit-learn models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-classes">
   C++ classes
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="python-runtime-for-onnx">
<h1>Python Runtime for ONNX<a class="headerlink" href="#python-runtime-for-onnx" title="Permalink to this headline">Â¶</a></h1>
<p>This runtime does not take any dependency on <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a>,
only on <a class="reference external" href="https://www.numpy.org/">numpy</a>, <a class="reference external" href="https://www.scipy.org/">scipy</a>, and has custom implementations
in C++ (<a class="reference external" href="https://cython.org/">cython</a>, <a class="reference external" href="https://github.com/pybind/pybind11">pybind11</a>).</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#inference" id="id9">Inference</a></p></li>
<li><p><a class="reference internal" href="#backend-validation" id="id10">Backend validation</a></p></li>
<li><p><a class="reference internal" href="#python-to-onnx" id="id11">Python to ONNX</a></p></li>
<li><p><a class="reference internal" href="#onnx-export" id="id12">ONNX Export</a></p></li>
<li><p><a class="reference internal" href="#onnx-structure" id="id13">ONNX Structure</a></p></li>
<li><p><a class="reference internal" href="#onnxruntime" id="id14">onnxruntime</a></p></li>
<li><p><a class="reference internal" href="#validation-of-scikit-learn-models" id="id15">Validation of scikit-learn models</a></p></li>
<li><p><a class="reference internal" href="#c-classes" id="id16">C++ classes</a></p></li>
</ul>
</div>
<section id="inference">
<h2><a class="toc-backref" href="#id9">Inference</a><a class="headerlink" href="#inference" title="Permalink to this headline">Â¶</a></h2>
<p>The main class reads an <a class="reference external" href="https://onnx.ai/">ONNX</a> file
and may computes predictions based on a runtime
implementated in <a class="reference external" href="https://www.python.org/">Python</a>. The <a class="reference external" href="https://onnx.ai/">ONNX</a> model relies
on the following operators <a class="reference internal" href="onnxrt_ops.html#l-onnx-runtime-operators"><span class="std std-ref">Python Runtime for ONNX operators</span></a>.</p>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference" title="mlprodict.onnxrt.onnx_inference.OnnxInference"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.OnnxInference</span></code></a> (<em>self</em>, <em>onnx_or_bytes_or_stream</em>, <em>runtime</em> = <cite>None</cite>, <em>skip_run</em> = <cite>False</cite>, <em>inplace</em> = <cite>True</cite>, <em>input_inplace</em> = <cite>False</cite>, <em>ir_version</em> = <cite>None</cite>, <em>target_opset</em> = <cite>None</cite>, <em>runtime_options</em> = <cite>None</cite>, <em>session_options</em> = <cite>None</cite>, <em>inside_loop</em> = <cite>False</cite>, <em>static_inputs</em> = <cite>None</cite>, <em>new_outputs</em> = <cite>None</cite>, <em>new_opset</em> = <cite>None</cite>, <em>device</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Loads an <a class="reference external" href="https://onnx.ai/">ONNX</a> file or object or stream.
Computes the output of the <a class="reference external" href="https://onnx.ai/">ONNX</a> graph.
Several runtimes are available.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'python'</span></code>: the runtime implements every onnx operator
needed to run a <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> model by using <a class="reference external" href="https://www.numpy.org/">numpy</a>
or C++ code.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'python_compiled'</span></code>: it is the same runtime than the previous
one except every operator is called from a compiled function
(<a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference._build_compile_run" title="mlprodict.onnxrt.onnx_inference.OnnxInference._build_compile_run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_build_compile_run</span></code></a>) instead for a method going through
the list of operator</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'onnxruntime1'</span></code>: uses <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'onnxruntime2'</span></code>: this mode is mostly used to debug as
python handles calling every operator but <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>
is called for every of them, this process may fail due to
wrong inference type specially of the graph includes
custom nodes, in that case, it is better to compute the output
of intermediates nodes. It is much slower as fo every output, every
node is computed but more robust.</p></li>
</ul>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference.check_model" title="mlprodict.onnxrt.onnx_inference.OnnxInference.check_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">check_model</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Checks the model follow <a class="reference external" href="https://onnx.ai/">ONNX</a> conventions.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference.get_profiling" title="mlprodict.onnxrt.onnx_inference.OnnxInference.get_profiling"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_profiling</span></code></a> (<em>self</em>, <em>as_df</em> = <cite>False</cite>)</p>
<blockquote>
<div><p>Returns the profiling after a couple of execution.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference.run" title="mlprodict.onnxrt.onnx_inference.OnnxInference.run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">run</span></code></a> (<em>self</em>, <em>inputs</em>, <em>clean_right_away</em> = <cite>False</cite>, <em>intermediate</em> = <cite>False</cite>, <em>verbose</em> = <cite>0</cite>, <em>node_time</em> = <cite>False</cite>, <em>overwrite_types</em> = <cite>None</cite>, <em>yield_ops</em> = <cite>None</cite>, <em>fLOG</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Computes the predictions for this <a class="reference external" href="https://github.com/onnx/onnx">onnx</a> graph.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference.run2onnx" title="mlprodict.onnxrt.onnx_inference.OnnxInference.run2onnx"><code class="xref py py-meth docutils literal notranslate"><span class="pre">run2onnx</span></code></a> (<em>self</em>, <em>inputs</em>, <em>verbose</em> = <cite>0</cite>, <em>fLOG</em> = <cite>None</cite>, <em>as_parameter</em> = <cite>True</cite>, <em>suffix</em> = <cite>â€˜_DBGâ€™</cite>, <em>param_name</em> = <cite>None</cite>, <em>node_type</em> = <cite>â€˜DEBUGâ€™</cite>, <em>domain</em> = <cite>â€˜DEBUGâ€™</cite>, <em>domain_opset</em> = <cite>1</cite>)</p>
<blockquote>
<div><p>Executes the graphs with the given inputs, then adds the intermediate
results into ONNX nodes in the original graph. Once saved, it can be
looked with a tool such as <a class="reference external" href="https://github.com/lutzroeder/netron">netron</a>.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference.shape_inference" title="mlprodict.onnxrt.onnx_inference.OnnxInference.shape_inference"><code class="xref py py-meth docutils literal notranslate"><span class="pre">shape_inference</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Infers the shape of the outputs
with <a class="reference external" href="https://github.com/onnx/onnx">onnx</a> package.</p>
</div></blockquote>
</div></blockquote>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.onnx_micro_inference.OnnxMicroRuntime</span></code></p>
<p>The following is technically implemented as a runtime but it does
shape inference.</p>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_shape_inference.html#mlprodict.onnxrt.onnx_shape_inference.OnnxShapeInference" title="mlprodict.onnxrt.onnx_shape_inference.OnnxShapeInference"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.OnnxShapeInference</span></code></a> (<em>self</em>, <em>model_onnx</em>)</p>
<blockquote>
<div><p>Implements a micro runtime for ONNX graphs.
It does not implements all the operator types.</p>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_shape_inference.html#mlprodict.onnxrt.onnx_shape_inference.OnnxShapeInference.run" title="mlprodict.onnxrt.onnx_shape_inference.OnnxShapeInference.run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">run</span></code></a> (<em>self</em>, <em>inputs</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Runs shape inference and type given known inputs.</p>
</div></blockquote>
</div></blockquote>
<p>The execution produces a result of type:</p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_container.html#mlprodict.onnxrt.ops_shape.shape_container.ShapeContainer" title="mlprodict.onnxrt.ops_shape.shape_container.ShapeContainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_shape.shape_container.ShapeContainer</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Stores all infered shapes as <a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShapeResult</span></code></a>.</p>
<p>Attributes:</p>
<ul class="simple">
<li><p><cite>shapes</cite>: dictionary <cite>{ result name: ShapeResult }</cite></p></li>
<li><dl class="simple">
<dt><cite>names</cite>: some dimensions are unknown and represented as</dt><dd><p>variables, this dictionary keeps track of them</p>
</dd>
</dl>
</li>
<li><p><cite>names_rev</cite>: reverse dictionary of <cite>names</cite></p></li>
</ul>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_container.html#mlprodict.onnxrt.ops_shape.shape_container.ShapeContainer.get" title="mlprodict.onnxrt.ops_shape.shape_container.ShapeContainer.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Returns the value of attribute <cite>resolved_</cite>
(method <cite>resolve()</cite> must have been called first).</p>
</div></blockquote>
</div></blockquote>
<p>Methods <cite>get</cite> returns a dictionary mapping result name and the following type:</p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_shape.shape_result.ShapeResult</span></code></a> (<em>self</em>, <em>name</em>, <em>shape</em> = <cite>None</cite>, <em>dtype</em> = <cite>None</cite>, <em>sparse</em> = <cite>False</cite>, <em>mtype</em> = <cite>OnnxKind.Tensor</cite>, <em>constraints</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Contains information about shape and type of a result
in an onnx graph.</p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.broadcast" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.broadcast"><code class="xref py py-meth docutils literal notranslate"><span class="pre">broadcast</span></code></a> (<em>sh1</em>, <em>sh2</em>, <em>name</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Broadcasts dimensions for an element wise operator.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.copy" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy</span></code></a> (<em>self</em>, <em>deep</em> = <cite>False</cite>)</p>
<blockquote>
<div><p>Returns a copy for the result.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.merge" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.merge"><code class="xref py py-meth docutils literal notranslate"><span class="pre">merge</span></code></a> (<em>self</em>, <em>other_result</em>)</p>
<blockquote>
<div><p>Merges constraints from <em>other_results</em> into <em>self</em>.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.n_dims" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.n_dims"><code class="xref py py-meth docutils literal notranslate"><span class="pre">n_dims</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Returns the number of dimensions if it is a tensor.
Raises an exception otherwise.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_shape/shape_result.html#mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.resolve" title="mlprodict.onnxrt.ops_shape.shape_result.ShapeResult.resolve"><code class="xref py py-meth docutils literal notranslate"><span class="pre">resolve</span></code></a> (<em>self</em>, <em>variables</em>)</p>
<blockquote>
<div><p>Results variables in a shape using values stored
in <em>variables</em>. It does not copy any constraints.</p>
</div></blockquote>
</div></blockquote>
</section>
<section id="backend-validation">
<h2><a class="toc-backref" href="#id10">Backend validation</a><a class="headerlink" href="#backend-validation" title="Permalink to this headline">Â¶</a></h2>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.tools.onnx_backend.enumerate_onnx_tests</span></code></p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.tools.onnx_backend.OnnxBackendTest</span></code></p>
</section>
<section id="python-to-onnx">
<h2><a class="toc-backref" href="#id11">Python to ONNX</a><a class="headerlink" href="#python-to-onnx" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="../mlprodict/onnx_tools/onnx_grammar/onnx_translation.html#mlprodict.onnx_tools.onnx_grammar.onnx_translation.translate_fct2onnx" title="mlprodict.onnx_tools.onnx_grammar.onnx_translation.translate_fct2onnx"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnx_tools.onnx_grammar.translate_fct2onnx</span></code></a> (<em>fct</em>, <em>context</em> = <cite>None</cite>, <em>cpl</em> = <cite>False</cite>, <em>context_cpl</em> = <cite>None</cite>, <em>output_names</em> = <cite>None</cite>, <em>dtype</em> = <cite>&lt;class â€˜numpy.float32â€™&gt;</cite>, <em>verbose</em> = <cite>0</cite>, <em>fLOG</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Translates a function into <a class="reference external" href="https://onnx.ai/">ONNX</a>. The code it produces
is using classes <em>OnnxAbs</em>, <em>OnnxAdd</em>, â€¦</p>
</div></blockquote>
</section>
<section id="onnx-export">
<h2><a class="toc-backref" href="#id12">ONNX Export</a><a class="headerlink" href="#onnx-export" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="../mlprodict/onnxrt/onnx_inference_exports.html#mlprodict.onnxrt.onnx_inference_exports.OnnxInferenceExport" title="mlprodict.onnxrt.onnx_inference_exports.OnnxInferenceExport"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.onnx_inference_exports.OnnxInferenceExport</span></code></a> (<em>self</em>, <em>oinf</em>)</p>
<blockquote>
<div><p>Implements methods to export a instance of
<a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference" title="mlprodict.onnxrt.onnx_inference.OnnxInference"><code class="xref py py-class docutils literal notranslate"><span class="pre">OnnxInference</span></code></a> into <a class="reference external" href="https://docs.python.org/3/library/json.html">json</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/DOT_(graph_description_language)">dot</a>,
<em>text</em>, <em>python</em>.</p>
</div></blockquote>
</section>
<section id="onnx-structure">
<h2><a class="toc-backref" href="#id13">ONNX Structure</a><a class="headerlink" href="#onnx-structure" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="../mlprodict/onnx_tools/onnx_manipulations.html#mlprodict.onnx_tools.onnx_manipulations.enumerate_model_node_outputs" title="mlprodict.onnx_tools.onnx_manipulations.enumerate_model_node_outputs"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnx_tools.onnx_manipulations.enumerate_model_node_outputs</span></code></a> (<em>model</em>, <em>add_node</em> = <cite>False</cite>, <em>order</em> = <cite>False</cite>)</p>
<blockquote>
<div><p>Enumerates all the nodes of a model.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnx_tools/onnx_manipulations.html#mlprodict.onnx_tools.onnx_manipulations.select_model_inputs_outputs" title="mlprodict.onnx_tools.onnx_manipulations.select_model_inputs_outputs"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnx_tools.onnx_manipulations.select_model_inputs_outputs</span></code></a> (<em>model</em>, <em>outputs</em> = <cite>None</cite>, <em>inputs</em> = <cite>None</cite>, <em>infer_shapes</em> = <cite>False</cite>, <em>overwrite</em> = <cite>None</cite>, <em>remove_unused</em> = <cite>True</cite>, <em>verbose</em> = <cite>0</cite>, <em>fLOG</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Takes a model and changes its outputs.</p>
</div></blockquote>
</section>
<section id="onnxruntime">
<h2><a class="toc-backref" href="#id14">onnxruntime</a><a class="headerlink" href="#onnxruntime" title="Permalink to this headline">Â¶</a></h2>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.onnx_inference_ort.device_to_providers</span></code></p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.onnx_inference_ort.get_ort_device</span></code></p>
</section>
<section id="validation-of-scikit-learn-models">
<h2><a class="toc-backref" href="#id15">Validation of scikit-learn models</a><a class="headerlink" href="#validation-of-scikit-learn-models" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="../mlprodict/onnxrt/validate/validate.html#mlprodict.onnxrt.validate.validate.enumerate_validated_operator_opsets" title="mlprodict.onnxrt.validate.validate.enumerate_validated_operator_opsets"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.validate.enumerate_validated_operator_opsets</span></code></a> (<em>verbose</em> = <cite>0</cite>, <em>opset_min</em> = <cite>-1</cite>, <em>opset_max</em> = <cite>-1</cite>, <em>check_runtime</em> = <cite>True</cite>, <em>debug</em> = <cite>False</cite>, <em>runtime</em> = <cite>â€˜pythonâ€™</cite>, <em>models</em> = <cite>None</cite>, <em>dump_folder</em> = <cite>None</cite>, <em>store_models</em> = <cite>False</cite>, <em>benchmark</em> = <cite>False</cite>, <em>skip_models</em> = <cite>None</cite>, <em>assume_finite</em> = <cite>True</cite>, <em>node_time</em> = <cite>False</cite>, <em>fLOG</em> = <cite>&lt;built-in function print&gt;</cite>, <em>filter_exp</em> = <cite>None</cite>, <em>versions</em> = <cite>False</cite>, <em>extended_list</em> = <cite>False</cite>, <em>time_kwargs</em> = <cite>None</cite>, <em>dump_all</em> = <cite>False</cite>, <em>n_features</em> = <cite>None</cite>, <em>skip_long_test</em> = <cite>True</cite>, <em>fail_bad_results</em> = <cite>False</cite>, <em>filter_scenario</em> = <cite>None</cite>, <em>time_kwargs_fact</em> = <cite>None</cite>, <em>time_limit</em> = <cite>4</cite>, <em>n_jobs</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Tests all possible configurations for all possible
operators and returns the results.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/validate/side_by_side.html#mlprodict.onnxrt.validate.side_by_side.side_by_side_by_values" title="mlprodict.onnxrt.validate.side_by_side.side_by_side_by_values"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.validate.side_by_side.side_by_side_by_values</span></code></a> (<em>sessions</em>, <em>args</em>, <em>inputs</em> = <cite>None</cite>, <em>return_results</em> = <cite>False</cite>, <em>kwargs</em>)</p>
<blockquote>
<div><p>Compares the execution of two sessions.
It calls method <a class="reference internal" href="../mlprodict/onnxrt/onnx_inference.html#mlprodict.onnxrt.onnx_inference.OnnxInference.run" title="mlprodict.onnxrt.onnx_inference.OnnxInference.run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">OnnxInference.run</span></code></a>
with value <code class="docutils literal notranslate"><span class="pre">intermediate=True</span></code> and compares the results.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/validate/validate_summary.html#mlprodict.onnxrt.validate.validate_summary.summary_report" title="mlprodict.onnxrt.validate.validate_summary.summary_report"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.validate.summary_report</span></code></a> (<em>df</em>, <em>add_cols</em> = <cite>None</cite>, <em>add_index</em> = <cite>None</cite>)</p>
<blockquote>
<div><p>Finalizes the results computed by function
<a class="reference internal" href="../mlprodict/onnxrt/validate/validate.html#mlprodict.onnxrt.validate.validate.enumerate_validated_operator_opsets" title="mlprodict.onnxrt.validate.validate.enumerate_validated_operator_opsets"><code class="xref py py-func docutils literal notranslate"><span class="pre">enumerate_validated_operator_opsets</span></code></a>.</p>
</div></blockquote>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.validate.validate_graph.plot_validate_benchmark</span></code></p>
</section>
<section id="c-classes">
<h2><a class="toc-backref" href="#id16">C++ classes</a><a class="headerlink" href="#c-classes" title="Permalink to this headline">Â¶</a></h2>
<p><strong>Gather</strong></p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_gather_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_gather_.GatherDouble" title="mlprodict.onnxrt.ops_cpu.op_gather_.GatherDouble"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_gather_.GatherDouble</span></code></a> (<em>self</em>, <em>arg0</em>)</p>
<blockquote>
<div><p>Implements runtime for operator Gather. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/tensor/gather.cc">tfidfvectorizer.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_gather_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_gather_.GatherFloat" title="mlprodict.onnxrt.ops_cpu.op_gather_.GatherFloat"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_gather_.GatherFloat</span></code></a> (<em>self</em>, <em>arg0</em>)</p>
<blockquote>
<div><p>Implements runtime for operator Gather. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/tensor/gather.cc">tfidfvectorizer.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_gather_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_gather_.GatherInt64" title="mlprodict.onnxrt.ops_cpu.op_gather_.GatherInt64"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_gather_.GatherInt64</span></code></a> (<em>self</em>, <em>arg0</em>)</p>
<blockquote>
<div><p>Implements runtime for operator Gather. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/tensor/gather.cc">tfidfvectorizer.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>.</p>
</div></blockquote>
<p><strong>ArrayFeatureExtractor</strong></p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_double" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_double"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_double</span></code></a> (<em>arg0</em>, <em>arg1</em>)</p>
<blockquote>
<div><p>array_feature_extractor_double(arg0: numpy.ndarray[numpy.float64], arg1: numpy.ndarray[numpy.int64]) -&gt; numpy.ndarray[numpy.float64]</p>
<p>C++ implementation of operator ArrayFeatureExtractor for float64.
The function only works with contiguous arrays.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_float" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_float"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_float</span></code></a> (<em>arg0</em>, <em>arg1</em>)</p>
<blockquote>
<div><p>array_feature_extractor_float(arg0: numpy.ndarray[numpy.float32], arg1: numpy.ndarray[numpy.int64]) -&gt; numpy.ndarray[numpy.float32]</p>
<p>C++ implementation of operator ArrayFeatureExtractor for float32.
The function only works with contiguous arrays.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_int64" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_int64"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.array_feature_extractor_int64</span></code></a> (<em>arg0</em>, <em>arg1</em>)</p>
<blockquote>
<div><p>array_feature_extractor_int64(arg0: numpy.ndarray[numpy.int64], arg1: numpy.ndarray[numpy.int64]) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator ArrayFeatureExtractor for int64.
The function only works with contiguous arrays.</p>
</div></blockquote>
<p><strong>SVM</strong></p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_svm_classifier_.RuntimeSVMClassifier</span></code></p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_svm_regressor_.RuntimeSVMRegressor</span></code></p>
<p><strong>Tree Ensemble</strong></p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_classifier_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_.RuntimeTreeEnsembleClassifierDouble" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_.RuntimeTreeEnsembleClassifierDouble"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_.RuntimeTreeEnsembleClassifierDouble</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Implements runtime for operator TreeEnsembleClassifier. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_classifier.cc">tree_ensemble_classifier.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports double only.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_classifier_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_.RuntimeTreeEnsembleClassifierFloat" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_.RuntimeTreeEnsembleClassifierFloat"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_.RuntimeTreeEnsembleClassifierFloat</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Implements runtime for operator TreeEnsembleClassifier. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_classifier.cc">tree_ensemble_classifier.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports float only.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_regressor_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_.RuntimeTreeEnsembleRegressorDouble" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_.RuntimeTreeEnsembleRegressorDouble"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_.RuntimeTreeEnsembleRegressorDouble</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Implements double runtime for operator TreeEnsembleRegressor. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_Regressor.cc">tree_ensemble_regressor.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports double only.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_regressor_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_.RuntimeTreeEnsembleRegressorFloat" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_.RuntimeTreeEnsembleRegressorFloat"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_.RuntimeTreeEnsembleRegressorFloat</span></code></a> (<em>self</em>)</p>
<blockquote>
<div><p>Implements float runtime for operator TreeEnsembleRegressor. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_Regressor.cc">tree_ensemble_regressor.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports float only.</p>
</div></blockquote>
<p><strong>Still tree ensembles but refactored.</strong></p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_classifier_p_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_p_.RuntimeTreeEnsembleClassifierPDouble" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_p_.RuntimeTreeEnsembleClassifierPDouble"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_p_.RuntimeTreeEnsembleClassifierPDouble</span></code></a> (<em>self</em>, <em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>Implements double runtime for operator TreeEnsembleClassifier. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_Classifier.cc">tree_ensemble_Classifier.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports double only.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_classifier_p_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_p_.RuntimeTreeEnsembleClassifierPFloat" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_p_.RuntimeTreeEnsembleClassifierPFloat"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_classifier_p_.RuntimeTreeEnsembleClassifierPFloat</span></code></a> (<em>self</em>, <em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>Implements float runtime for operator TreeEnsembleClassifier. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_Classifier.cc">tree_ensemble_Classifier.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports float only.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_regressor_p_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPDouble" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPDouble"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPDouble</span></code></a> (<em>self</em>, <em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>Implements double runtime for operator TreeEnsembleRegressor. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_Regressor.cc">tree_ensemble_regressor.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports double only.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/op_tree_ensemble_regressor_p_.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPFloat" title="mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPFloat"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPFloat</span></code></a> (<em>self</em>, <em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>Implements float runtime for operator TreeEnsembleRegressor. The code is inspired from
<a class="reference external" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/core/providers/cpu/ml/tree_ensemble_Regressor.cc">tree_ensemble_regressor.cc</a>
in <a class="reference external" href="https://github.com/microsoft/onnxruntime">onnxruntime</a>. Supports float only.</p>
</div></blockquote>
<p><strong>Topk</strong></p>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_double" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_double"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_double</span></code></a> (<em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>topk_element_max_double(arg0: numpy.ndarray[numpy.float64], arg1: int, arg2: bool, arg3: int) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator TopK for float32.
The function only works with contiguous arrays.
The function is parallelized for more than <em>th_para</em> rows.
It only does it on the last axis.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_float" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_float"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_float</span></code></a> (<em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>topk_element_max_float(arg0: numpy.ndarray[numpy.float32], arg1: int, arg2: bool, arg3: int) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator TopK for float32.
The function only works with contiguous arrays.
The function is parallelized for more than <em>th_para</em> rows.
It only does it on the last axis.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_int64" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_int64"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_max_int64</span></code></a> (<em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>topk_element_max_int64(arg0: numpy.ndarray[numpy.int64], arg1: int, arg2: bool, arg3: int) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator TopK for float32.
The function only works with contiguous arrays.
The function is parallelized for more than <em>th_para</em> rows.
It only does it on the last axis.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_double" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_double"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_double</span></code></a> (<em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>topk_element_min_double(arg0: numpy.ndarray[numpy.float64], arg1: int, arg2: bool, arg3: int) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator TopK for float32.
The function only works with contiguous arrays.
The function is parallelized for more than <em>th_para</em> rows.
It only does it on the last axis.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_float" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_float"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_float</span></code></a> (<em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>topk_element_min_float(arg0: numpy.ndarray[numpy.float32], arg1: int, arg2: bool, arg3: int) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator TopK for float32.
The function only works with contiguous arrays.
The function is parallelized for more than <em>th_para</em> rows.
It only does it on the last axis.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_int64" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_int64"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_min_int64</span></code></a> (<em>arg0</em>, <em>arg1</em>, <em>arg2</em>, <em>arg3</em>)</p>
<blockquote>
<div><p>topk_element_min_int64(arg0: numpy.ndarray[numpy.int64], arg1: int, arg2: bool, arg3: int) -&gt; numpy.ndarray[numpy.int64]</p>
<p>C++ implementation of operator TopK for float32.
The function only works with contiguous arrays.
The function is parallelized for more than <em>th_para</em> rows.
It only does it on the last axis.</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_double" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_double"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_double</span></code></a> (<em>arg0</em>, <em>arg1</em>)</p>
<blockquote>
<div><p>topk_element_fetch_double(arg0: numpy.ndarray[numpy.float64], arg1: numpy.ndarray[numpy.int64]) -&gt; numpy.ndarray[numpy.float64]</p>
<p>Fetches the top k element knowing their indices
on each row (= last dimension for a multi dimension array).</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_float" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_float"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_float</span></code></a> (<em>arg0</em>, <em>arg1</em>)</p>
<blockquote>
<div><p>topk_element_fetch_float(arg0: numpy.ndarray[numpy.float32], arg1: numpy.ndarray[numpy.int64]) -&gt; numpy.ndarray[numpy.float32]</p>
<p>Fetches the top k element knowing their indices
on each row (= last dimension for a multi dimension array).</p>
</div></blockquote>
<p><a class="reference internal" href="../mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpython-39-x86_64-linux-gnu.html#mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_int64" title="mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_int64"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlprodict.onnxrt.ops_cpu._op_onnx_numpy.topk_element_fetch_int64</span></code></a> (<em>arg0</em>, <em>arg1</em>)</p>
<blockquote>
<div><p>topk_element_fetch_int64(arg0: numpy.ndarray[numpy.int64], arg1: numpy.ndarray[numpy.int64]) -&gt; numpy.ndarray[numpy.int64]</p>
<p>Fetches the top k element knowing their indices
on each row (= last dimension for a multi dimension array).</p>
</div></blockquote>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ast.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">AST</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="onnxrt_ops.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Python Runtime for ONNX operators</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Xavier DuprÃ©.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>