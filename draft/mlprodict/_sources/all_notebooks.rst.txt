
:orphan:


.. _l-notebooks:

Notebook Gallery
================





:ref:`l-notebooks-coverage`


.. contents::
    :depth: 1
    :local:

.. toctree::
    :maxdepth: 1

    notebooks/sklearn_grammar_lr
    notebooks/onnx_discrepencies
    notebooks/einsum_decomposition
    notebooks/topk_cpp
    notebooks/onnx_operator_cost
    notebooks/numpy_api_onnx_ccl
    notebooks/numpy_api_onnx_ftr
    notebooks/lightgbm_double
    notebooks/loss_functions
    notebooks/onnx_profile
    notebooks/onnx_fft
    notebooks/onnx_float32_and_64
    notebooks/onnx_sbs
    notebooks/onnx_visualization
    notebooks/onnx_pdist
    notebooks/onnx_shaker
    notebooks/onnx_profile_ort
    notebooks/onnx_node_time
    notebooks/transfer_learning
    notebooks/onnx_float_double_skl_decision_trees

.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The logistic regression is trained in python and executed in C.">

.. only:: html

    .. figure:: /notebooks/sklearn_grammar_lr.thumb.png

        :ref:`sklearngrammarlrrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The notebook shows one example where the conversion leads with discrepencies if default options are used. It converts a pipeline with two steps, a scaler followed by a tree.">

.. only:: html

    .. figure:: /notebooks/onnx_discrepencies.thumb.png

        :ref:`onnxdiscrepenciesrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="This notebook shows a way to decompose einsum into a subset of operations (expand_dims, squeeze, transpose, extended matrix multiplication).">

.. only:: html

    .. figure:: /notebooks/einsum_decomposition.thumb.png

        :ref:`einsumdecompositionrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="Looking for the top k elements is something needed to implement a simple k nearest neighbors. The implementation *scikit-learn* is using relies on *numpy*: _kneighbors_reduce_func. *mlprodict* also contains a C++ implementation of the same function. Let's compare them.">

.. only:: html

    .. figure:: /notebooks/topk_cpp.thumb.png

        :ref:`topkcpprst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="This notebooks explores a way to predict the cost of operator Transpose based on some features.">

.. only:: html

    .. figure:: /notebooks/onnx_operator_cost.thumb.png

        :ref:`onnxoperatorcostrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="This notebook shows how to write python classifier using similar functions as numpy offers and get a class which can be inserted into a pipeline and still be converted into ONNX.">

.. only:: html

    .. figure:: /notebooks/numpy_api_onnx_ccl.thumb.png

        :ref:`numpyapionnxcclrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="This notebook shows how to write python functions similar functions as numpy offers and get a function which can be converted into ONNX.">

.. only:: html

    .. figure:: /notebooks/numpy_api_onnx_ftr.thumb.png

        :ref:`numpyapionnxftrrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="Discrepencies usually happens with lightgbm because its code is used double to represent the threshold of trees as ONNX is using float only. There is no way to fix this discrepencies unless the ONNX implementation of trees is using double.">

.. only:: html

    .. figure:: /notebooks/lightgbm_double.thumb.png

        :ref:`lightgbmdoublerst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The following notebook show how to translate common loss function into ONNX.">

.. only:: html

    .. figure:: /notebooks/loss_functions.thumb.png

        :ref:`lossfunctionsrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The first benchmark based on scikti-learn's benchmark shows high peaks of memory usage for the python runtime on linear models. Let's see how to measure that.">

.. only:: html

    .. figure:: /notebooks/onnx_profile.thumb.png

        :ref:`onnxprofilerst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="ONNX does not fully support complex yet. It does not have any FFT operators either. What if we need them anyway?">

.. only:: html

    .. figure:: /notebooks/onnx_fft.thumb.png

        :ref:`onnxfftrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The notebook shows discrepencies obtained by using double floats instead of single float in two cases. The second one involves GaussianProcessRegressor.">

.. only:: html

    .. figure:: /notebooks/onnx_float32_and_64.thumb.png

        :ref:`onnxfloat32and64rst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The notebook compares two runtimes for the same ONNX and looks into differences at each step of the graph.">

.. only:: html

    .. figure:: /notebooks/onnx_sbs.thumb.png

        :ref:`onnxsbsrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="ONNX is a serialization format for machine learned model. It is a list of mathematical functions used to describe every prediction function for standard and deep machine learning. Module onnx offers some tools to display ONNX graph. Netron is another approach. The following notebooks explore a ligher visualization.">

.. only:: html

    .. figure:: /notebooks/onnx_visualization.thumb.png

        :ref:`onnxvisualizationrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="Function pdist computes pairwise distances between observations in n-dimensional space. It is not that difficult to convert that into *ONNX* when the dimension of the input is always the same. What if not?">

.. only:: html

    .. figure:: /notebooks/onnx_pdist.thumb.png

        :ref:`onnxpdistrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The notebook studies the loss of precision while converting a non-continuous model into float32. It studies the conversion of GradientBoostingClassifier and then a DecisionTreeRegressor for which a runtime supported float64 was implemented.">

.. only:: html

    .. figure:: /notebooks/onnx_shaker.thumb.png

        :ref:`onnxshakerrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The notebook profiles the execution of an ONNX graph built from a *KMeans* model and executed with *onnxruntime*. It then study the decomposition of one einsum equation into more simple operators.">

.. only:: html

    .. figure:: /notebooks/onnx_profile_ort.thumb.png

        :ref:`onnxprofileortrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The following notebook show how long the runtime spends in each node of an ONNX graph.">

.. only:: html

    .. figure:: /notebooks/onnx_node_time.thumb.png

        :ref:`onnxnodetimerst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="The notebooks retrieve the already converted onnx model for SqueezeNet and uses it in a pipeline created with scikit-learn.">

.. only:: html

    .. figure:: /notebooks/transfer_learning.thumb.png

        :ref:`transferlearningrst`

.. raw:: html

    </div>


.. raw:: html

    <div class="sphx-pyq-thumb" tooltip="*scikit-learn* use a specific comparison when computing the preduction of a decision tree, it does ``(float)x <= threshold`` (see tree.pyx / method apply_dense. *ONNX* does not specify such things and compares *x* to _threshold_, both having the same type. What to do then when writing the converter.">

.. only:: html

    .. figure:: /notebooks/onnx_float_double_skl_decision_trees.thumb.png

        :ref:`onnxfloatdoubleskldecisiontreesrst`

.. raw:: html

    </div>




.. toctree::
    :hidden: 

    all_notebooks_coverage
