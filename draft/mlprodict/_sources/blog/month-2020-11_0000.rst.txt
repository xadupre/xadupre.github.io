
:orphan:

|rss_image|  **2020-11 - 1/1** :ref:`Blog <ap-main-0>` :ref:`onnx (8) <ap-cat-onnx-0>` :ref:`runtime (3) <ap-cat-runtime-0>`

.. |rss_image| image:: feed-icon-16x16.png
    :target: ../_downloads/rss.xml
    :alt: RSS

----


.. index:: 2020-11


.. _ap-month-2020-11-0:

2020-11 - 1/1
+++++++++++++

.. blogpostagg::
    :title: Parallelization of Random Forest predictions
    :date: 2020-11-27
    :keywords: scikit-learn,parallelization,Random Forest
    :categories: runtime
    :rawfile: 2020/2020-11-27_parallelisation.rst

    I've been struggling to understand why the first implementation
    of TreeEnsemble could not get as fast as *scikit-learn* implementation
    for a RandomForest when the number of observations was 100.000 or above,
    100 trees and a depth >= 10. The only difference was that the computation
    was parallelized by trees and not by observations. These
    observations are benchmarked in
    :ref:`l-example-tree-ensemble-reg-bench`
    (:ref:`l-example-tree-ensemble-cls-bench-multi` for the
    multiclass version).
    

    ...





----

|rss_image|  **2020-11 - 1/1** :ref:`2020-11 (1) <ap-month-2020-11-0>` :ref:`2021-05 (2) <ap-month-2021-05-0>` :ref:`2021-07 (2) <ap-month-2021-07-0>` :ref:`2021-08 (3) <ap-month-2021-08-0>` :ref:`2022-02 (1) <ap-month-2022-02-0>`