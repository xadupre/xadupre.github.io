
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gyexamples/plot_parallelism.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gyexamples_plot_parallelism.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gyexamples_plot_parallelism.py:


.. _l-example-parallelism:

When to parallelize?
====================

That is the question. Parallize computation
takes some time to set up, it is not the right
solution in every case. The following example studies
the parallelism introduced into the runtime of
*TreeEnsembleRegressor* to see when it is best
to do it.

.. contents::
    :local:

.. GENERATED FROM PYTHON SOURCE LINES 18-33

.. code-block:: default

    from pprint import pprint
    import numpy
    from pandas import DataFrame
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    from sklearn import config_context
    from sklearn.datasets import make_regression
    from sklearn.ensemble import HistGradientBoostingRegressor
    from sklearn.model_selection import train_test_split
    from cpyquickhelper.numbers import measure_time
    from pyquickhelper.pycode.profiling import profile
    from mlprodict.onnx_conv import to_onnx, register_rewritten_operators
    from mlprodict.onnxrt import OnnxInference
    from mlprodict.tools.model_info import analyze_model








.. GENERATED FROM PYTHON SOURCE LINES 34-35

Available optimisations on this machine.

.. GENERATED FROM PYTHON SOURCE LINES 35-40

.. code-block:: default


    from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation
    print(code_optimisation())






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    AVX-omp=8




.. GENERATED FROM PYTHON SOURCE LINES 41-43

Training and converting a model
+++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 43-53

.. code-block:: default



    data = make_regression(50000, 20)
    X, y = data
    X_train, X_test, y_train, y_test = train_test_split(X, y)

    hgb = HistGradientBoostingRegressor(max_iter=100, max_depth=6)
    hgb.fit(X_train, y_train)
    print(hgb)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    HistGradientBoostingRegressor(max_depth=6)




.. GENERATED FROM PYTHON SOURCE LINES 54-55

Let's get more statistics about the model itself.

.. GENERATED FROM PYTHON SOURCE LINES 55-57

.. code-block:: default

    pprint(analyze_model(hgb))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {'_predictors.max|tree_.max_depth': 6,
     '_predictors.size': 100,
     '_predictors.sum|tree_.leave_count': 3100,
     '_predictors.sum|tree_.node_count': 6100,
     'train_score_.shape': 101,
     'validation_score_.shape': 101}




.. GENERATED FROM PYTHON SOURCE LINES 58-59

And let's convert it.

.. GENERATED FROM PYTHON SOURCE LINES 59-66

.. code-block:: default


    register_rewritten_operators()
    onx = to_onnx(hgb, X_train[:1].astype(numpy.float32))
    oinf = OnnxInference(onx, runtime='python_compiled')
    print(oinf)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    OnnxInference(...)
        def compiled_run(dict_inputs, yield_ops=None):
            if yield_ops is not None:
                raise NotImplementedError('yields_ops should be None.')
            # inputs
            X = dict_inputs['X']
            (variable, ) = n0_treeensembleregressor(X)
            return {
                'variable': variable,
            }




.. GENERATED FROM PYTHON SOURCE LINES 67-68

The runtime of the forest is in the following object.

.. GENERATED FROM PYTHON SOURCE LINES 68-72

.. code-block:: default


    print(oinf.sequence_[0].ops_)
    print(oinf.sequence_[0].ops_.rt_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    TreeEnsembleRegressor(
        op_type=TreeEnsembleRegressor
        aggregate_function=b'SUM',
        base_values=[0.7807642],
        domain=ai.onnx.ml,
        inplaces={},
        ir_version=8,
        n_targets=1,
        nodes_falsenodeids=[30 17 10 ...  0  0  0],
        nodes_featureids=[ 2 14 16 ...  0  0  0],
        nodes_hitrates=[1. 1. 1. ... 1. 1. 1.],
        nodes_missing_value_tracks_true=[0 1 0 ... 0 0 0],
        nodes_modes=[b'BRANCH_LEQ' b'BRANCH_LEQ' b'BRANCH_LEQ' ... b'LEAF' b'LEAF' b'LEAF'],
        nodes_nodeids=[ 0  1  2 ... 58 59 60],
        nodes_treeids=[ 0  0  0 ... 99 99 99],
        nodes_truenodeids=[1 2 3 ... 0 0 0],
        nodes_values=[-0.00458882  0.08421846 -0.07731909 ...  0.          0.
      0.        ],
        post_transform=b'NONE',
        target_ids=[0 0 0 ... 0 0 0],
        target_nodeids=[ 5  6  8 ... 58 59 60],
        target_opset=1,
        target_treeids=[ 0  0  0 ... 99 99 99],
        target_weights=[-30.826473   -20.658823   -21.4012     ...   0.87109977   2.5309787
       1.9359126 ],
    )
    <mlprodict.onnxrt.ops_cpu.op_tree_ensemble_regressor_p_.RuntimeTreeEnsembleRegressorPFloat object at 0x7fccb82472f0>




.. GENERATED FROM PYTHON SOURCE LINES 73-75

And the threshold used to start parallelizing
based on the number of observations.

.. GENERATED FROM PYTHON SOURCE LINES 75-79

.. code-block:: default


    print(oinf.sequence_[0].ops_.rt_.omp_N_)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    20




.. GENERATED FROM PYTHON SOURCE LINES 80-87

Profiling
+++++++++

This step involves :epkg:`pyinstrument` to measure
where the time is spent. Both :epkg:`scikit-learn`
and :epkg:`mlprodict` runtime are called so that
the prediction times can be compared.

.. GENERATED FROM PYTHON SOURCE LINES 87-102

.. code-block:: default


    X32 = X_test.astype(numpy.float32)


    def runlocal():
        with config_context(assume_finite=True):
            for i in range(0, 100):
                oinf.run({'X': X32[:1000]})
                hgb.predict(X_test[:1000])


    print("profiling...")
    txt = profile(runlocal, pyinst_format='text')
    print(txt[1])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    profiling...

      _     ._   __/__   _ _  _  _ _/_   Recorded: 09:14:06 AM Samples:  1506
     /_//_/// /_\ / //_// / //_'/ //     Duration: 2.312     CPU time: 18.319
    /   _/                      v4.1.1

    Program: /var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/examples/plot_parallelism.py

    2.312 profile  ../pycode/profiling.py:457
    `- 2.312 runlocal  plot_parallelism.py:91
          [33 frames hidden]  plot_parallelism, sklearn, <built-in>...
             0.829 _predict_from_raw_data  <built-in>:0
             0.650 _run  mlprodict/onnxrt/ops_cpu/op_tree_ensemble_regressor.py:80






.. GENERATED FROM PYTHON SOURCE LINES 103-108

Now let's measure the performance the average
computation time per observations for 2 to 100
observations. The runtime implemented in
:epkg:`mlprodict` parallizes the computation
after a given number of observations.

.. GENERATED FROM PYTHON SOURCE LINES 108-134

.. code-block:: default


    obs = []
    for N in tqdm(list(range(2, 21))):
        m = measure_time("oinf.run({'X': x})",
                         {'oinf': oinf, 'x': X32[:N]},
                         div_by_number=True,
                         number=20)
        m['N'] = N
        m['RT'] = 'ONNX'
        obs.append(m)

        with config_context(assume_finite=True):
            m = measure_time("hgb.predict(x)",
                             {'hgb': hgb, 'x': X32[:N]},
                             div_by_number=True,
                             number=15)
        m['N'] = N
        m['RT'] = 'SKL'
        obs.append(m)

    df = DataFrame(obs)
    num = ['min_exec', 'average', 'max_exec']
    for c in num:
        df[c] /= df['N']
    df.head()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/19 [00:00<?, ?it/s]      5%|5         | 1/19 [00:01<00:23,  1.28s/it]     11%|#         | 2/19 [00:02<00:21,  1.28s/it]     16%|#5        | 3/19 [00:03<00:20,  1.29s/it]     21%|##1       | 4/19 [00:05<00:19,  1.29s/it]     26%|##6       | 5/19 [00:06<00:18,  1.29s/it]     32%|###1      | 6/19 [00:07<00:16,  1.29s/it]     37%|###6      | 7/19 [00:09<00:15,  1.29s/it]     42%|####2     | 8/19 [00:10<00:14,  1.29s/it]     47%|####7     | 9/19 [00:11<00:13,  1.30s/it]     53%|#####2    | 10/19 [00:12<00:11,  1.32s/it]     58%|#####7    | 11/19 [00:14<00:10,  1.33s/it]     63%|######3   | 12/19 [00:15<00:09,  1.33s/it]     68%|######8   | 13/19 [00:16<00:07,  1.33s/it]     74%|#######3  | 14/19 [00:18<00:06,  1.32s/it]     79%|#######8  | 15/19 [00:19<00:05,  1.32s/it]     84%|########4 | 16/19 [00:20<00:03,  1.32s/it]     89%|########9 | 17/19 [00:22<00:02,  1.32s/it]     95%|#########4| 18/19 [00:23<00:01,  1.32s/it]    100%|##########| 19/19 [00:24<00:00,  1.33s/it]    100%|##########| 19/19 [00:24<00:00,  1.31s/it]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>N</th>
          <th>RT</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.000028</td>
          <td>0.000002</td>
          <td>0.000027</td>
          <td>0.000030</td>
          <td>10</td>
          <td>20</td>
          <td>0.000557</td>
          <td>232</td>
          <td>2</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.004233</td>
          <td>0.000054</td>
          <td>0.004210</td>
          <td>0.004288</td>
          <td>10</td>
          <td>15</td>
          <td>0.084666</td>
          <td>232</td>
          <td>2</td>
          <td>SKL</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.000023</td>
          <td>0.000002</td>
          <td>0.000022</td>
          <td>0.000024</td>
          <td>10</td>
          <td>20</td>
          <td>0.000678</td>
          <td>232</td>
          <td>3</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.002818</td>
          <td>0.000015</td>
          <td>0.002812</td>
          <td>0.002826</td>
          <td>10</td>
          <td>15</td>
          <td>0.084528</td>
          <td>232</td>
          <td>3</td>
          <td>SKL</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.000020</td>
          <td>0.000002</td>
          <td>0.000020</td>
          <td>0.000021</td>
          <td>10</td>
          <td>20</td>
          <td>0.000798</td>
          <td>232</td>
          <td>4</td>
          <td>ONNX</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 135-136

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 136-145

.. code-block:: default


    fig, ax = plt.subplots(1, 2, figsize=(10, 4))
    df[df.RT == 'ONNX'].set_index('N')[num].plot(ax=ax[0])
    ax[0].set_title("Average ONNX prediction time per observation in a batch.")
    df[df.RT == 'SKL'].set_index('N')[num].plot(ax=ax[1])
    ax[1].set_title(
        "Average scikit-learn prediction time\nper observation in a batch.")





.. image-sg:: /gyexamples/images/sphx_glr_plot_parallelism_001.png
   :alt: Average ONNX prediction time per observation in a batch., Average scikit-learn prediction time per observation in a batch.
   :srcset: /gyexamples/images/sphx_glr_plot_parallelism_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(0.5, 1.0, 'Average scikit-learn prediction time\nper observation in a batch.')



.. GENERATED FROM PYTHON SOURCE LINES 146-154

Gain from parallelization
+++++++++++++++++++++++++

There is a clear gap between after and before 10 observations
when it is parallelized. Does this threshold depends on the number
of trees in the model?
For that we compute for each model the average prediction time
up to 10 and from 10 to 20.

.. GENERATED FROM PYTHON SOURCE LINES 154-166

.. code-block:: default


    def parallized_gain(df):
        df = df[df.RT == 'ONNX']
        df10 = df[df.N <= 10]
        t10 = sum(df10['average']) / df10.shape[0]
        df10p = df[df.N > 10]
        t10p = sum(df10p['average']) / df10p.shape[0]
        return t10 / t10p


    print('gain', parallized_gain(df))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    gain 1.303851504830905




.. GENERATED FROM PYTHON SOURCE LINES 167-175

Measures based on the number of trees
+++++++++++++++++++++++++++++++++++++

We trained many models with different number
of trees to see how the parallelization gain
is moving. One models is trained for every
distinct number of trees and then the prediction
time is measured for different number of observations.

.. GENERATED FROM PYTHON SOURCE LINES 175-179

.. code-block:: default


    tries_set = [2, 5, 8] + list(range(10, 50, 5)) + list(range(50, 101, 10))
    tries = [(nb, N) for N in range(2, 21, 2) for nb in tries_set]








.. GENERATED FROM PYTHON SOURCE LINES 180-181

training

.. GENERATED FROM PYTHON SOURCE LINES 181-191

.. code-block:: default


    models = {100: (hgb, oinf)}
    for nb in tqdm(set(_[0] for _ in tries)):
        if nb not in models:
            hgb = HistGradientBoostingRegressor(max_iter=nb, max_depth=6)
            hgb.fit(X_train, y_train)
            onx = to_onnx(hgb, X_train[:1].astype(numpy.float32))
            oinf = OnnxInference(onx, runtime='python_compiled')
            models[nb] = (hgb, oinf)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/17 [00:00<?, ?it/s]      6%|5         | 1/17 [00:00<00:05,  3.09it/s]     12%|#1        | 2/17 [00:01<00:13,  1.09it/s]     24%|##3       | 4/17 [00:02<00:06,  2.11it/s]     29%|##9       | 5/17 [00:04<00:12,  1.06s/it]     35%|###5      | 6/17 [00:04<00:09,  1.12it/s]     41%|####1     | 7/17 [00:06<00:10,  1.07s/it]     47%|####7     | 8/17 [00:07<00:08,  1.09it/s]     53%|#####2    | 9/17 [00:08<00:09,  1.14s/it]     59%|#####8    | 10/17 [00:09<00:07,  1.01s/it]     65%|######4   | 11/17 [00:12<00:09,  1.51s/it]     71%|#######   | 12/17 [00:13<00:07,  1.59s/it]     76%|#######6  | 13/17 [00:14<00:05,  1.38s/it]     82%|########2 | 14/17 [00:15<00:03,  1.27s/it]     88%|########8 | 15/17 [00:18<00:03,  1.77s/it]     94%|#########4| 16/17 [00:20<00:01,  1.86s/it]    100%|##########| 17/17 [00:21<00:00,  1.65s/it]    100%|##########| 17/17 [00:21<00:00,  1.29s/it]




.. GENERATED FROM PYTHON SOURCE LINES 192-193

prediction time

.. GENERATED FROM PYTHON SOURCE LINES 193-213

.. code-block:: default


    obs = []

    for nb, N in tqdm(tries):
        hgb, oinf = models[nb]
        m = measure_time("oinf.run({'X': x})",
                         {'oinf': oinf, 'x': X32[:N]},
                         div_by_number=True,
                         number=50)
        m['N'] = N
        m['nb'] = nb
        m['RT'] = 'ONNX'
        obs.append(m)

    df = DataFrame(obs)
    num = ['min_exec', 'average', 'max_exec']
    for c in num:
        df[c] /= df['N']
    df.head()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/170 [00:00<?, ?it/s]      4%|3         | 6/170 [00:00<00:02, 57.17it/s]      7%|7         | 12/170 [00:00<00:03, 50.25it/s]     11%|#         | 18/170 [00:00<00:03, 44.50it/s]     14%|#4        | 24/170 [00:00<00:03, 46.88it/s]     17%|#7        | 29/170 [00:00<00:03, 43.42it/s]     20%|##        | 34/170 [00:00<00:03, 36.46it/s]     24%|##3       | 40/170 [00:00<00:03, 40.46it/s]     26%|##6       | 45/170 [00:01<00:03, 38.37it/s]     29%|##8       | 49/170 [00:01<00:03, 33.45it/s]     31%|###1      | 53/170 [00:01<00:03, 32.00it/s]     34%|###4      | 58/170 [00:01<00:03, 34.87it/s]     36%|###6      | 62/170 [00:01<00:03, 32.99it/s]     39%|###8      | 66/170 [00:01<00:03, 28.15it/s]     41%|####      | 69/170 [00:01<00:03, 25.97it/s]     44%|####3     | 74/170 [00:02<00:03, 30.42it/s]     46%|####5     | 78/170 [00:02<00:03, 29.57it/s]     48%|####8     | 82/170 [00:02<00:03, 25.60it/s]     50%|#####     | 85/170 [00:02<00:04, 21.18it/s]     53%|#####2    | 90/170 [00:02<00:03, 26.44it/s]     55%|#####5    | 94/170 [00:02<00:02, 26.88it/s]     57%|#####7    | 97/170 [00:03<00:02, 24.87it/s]     59%|#####8    | 100/170 [00:03<00:03, 21.03it/s]     61%|######    | 103/170 [00:03<00:03, 19.18it/s]     64%|######3   | 108/170 [00:03<00:02, 24.08it/s]     65%|######5   | 111/170 [00:03<00:02, 24.15it/s]     67%|######7   | 114/170 [00:03<00:02, 22.21it/s]     69%|######8   | 117/170 [00:04<00:02, 18.56it/s]     71%|#######   | 120/170 [00:04<00:02, 16.90it/s]     74%|#######3  | 125/170 [00:04<00:02, 21.81it/s]     75%|#######5  | 128/170 [00:04<00:01, 22.00it/s]     77%|#######7  | 131/170 [00:04<00:01, 20.19it/s]     79%|#######8  | 134/170 [00:05<00:02, 16.79it/s]     80%|########  | 136/170 [00:05<00:02, 14.17it/s]     83%|########2 | 141/170 [00:05<00:01, 19.83it/s]     85%|########4 | 144/170 [00:05<00:01, 20.79it/s]     86%|########6 | 147/170 [00:05<00:01, 19.46it/s]     88%|########8 | 150/170 [00:05<00:01, 16.58it/s]     89%|########9 | 152/170 [00:06<00:01, 13.92it/s]     91%|######### | 154/170 [00:06<00:01, 13.89it/s]     93%|#########2| 158/170 [00:06<00:00, 18.68it/s]     95%|#########4| 161/170 [00:06<00:00, 19.68it/s]     96%|#########6| 164/170 [00:06<00:00, 18.18it/s]     98%|#########8| 167/170 [00:07<00:00, 15.24it/s]     99%|#########9| 169/170 [00:07<00:00, 12.68it/s]    100%|##########| 170/170 [00:07<00:00, 22.96it/s]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>N</th>
          <th>nb</th>
          <th>RT</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.000015</td>
          <td>4.640621e-07</td>
          <td>0.000015</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000309</td>
          <td>232</td>
          <td>2</td>
          <td>2</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.000016</td>
          <td>2.732580e-07</td>
          <td>0.000016</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000316</td>
          <td>232</td>
          <td>2</td>
          <td>5</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.000016</td>
          <td>2.690261e-07</td>
          <td>0.000016</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000318</td>
          <td>232</td>
          <td>2</td>
          <td>8</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.000016</td>
          <td>4.722324e-07</td>
          <td>0.000016</td>
          <td>0.000017</td>
          <td>10</td>
          <td>50</td>
          <td>0.000329</td>
          <td>232</td>
          <td>2</td>
          <td>10</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.000017</td>
          <td>3.640444e-07</td>
          <td>0.000017</td>
          <td>0.000018</td>
          <td>10</td>
          <td>50</td>
          <td>0.000345</td>
          <td>232</td>
          <td>2</td>
          <td>15</td>
          <td>ONNX</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 214-215

Let's compute the gains.

.. GENERATED FROM PYTHON SOURCE LINES 215-225

.. code-block:: default


    gains = []
    for nb in set(df['nb']):
        gain = parallized_gain(df[df.nb == nb])
        gains.append(dict(nb=nb, gain=gain))

    dfg = DataFrame(gains)
    dfg = dfg.sort_values('nb').reset_index(drop=True).copy()
    dfg






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>nb</th>
          <th>gain</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>2</td>
          <td>3.342564</td>
        </tr>
        <tr>
          <th>1</th>
          <td>5</td>
          <td>3.070982</td>
        </tr>
        <tr>
          <th>2</th>
          <td>8</td>
          <td>2.849541</td>
        </tr>
        <tr>
          <th>3</th>
          <td>10</td>
          <td>2.719479</td>
        </tr>
        <tr>
          <th>4</th>
          <td>15</td>
          <td>2.462390</td>
        </tr>
        <tr>
          <th>5</th>
          <td>20</td>
          <td>2.250934</td>
        </tr>
        <tr>
          <th>6</th>
          <td>25</td>
          <td>2.078773</td>
        </tr>
        <tr>
          <th>7</th>
          <td>30</td>
          <td>1.968299</td>
        </tr>
        <tr>
          <th>8</th>
          <td>35</td>
          <td>1.849501</td>
        </tr>
        <tr>
          <th>9</th>
          <td>40</td>
          <td>1.770980</td>
        </tr>
        <tr>
          <th>10</th>
          <td>45</td>
          <td>1.700082</td>
        </tr>
        <tr>
          <th>11</th>
          <td>50</td>
          <td>1.654464</td>
        </tr>
        <tr>
          <th>12</th>
          <td>60</td>
          <td>1.577692</td>
        </tr>
        <tr>
          <th>13</th>
          <td>70</td>
          <td>1.513925</td>
        </tr>
        <tr>
          <th>14</th>
          <td>80</td>
          <td>1.443747</td>
        </tr>
        <tr>
          <th>15</th>
          <td>90</td>
          <td>1.378006</td>
        </tr>
        <tr>
          <th>16</th>
          <td>100</td>
          <td>1.353666</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 226-227

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 227-232

.. code-block:: default


    ax = dfg.set_index('nb').plot()
    ax.set_title(
        "Parallelization gain depending\non the number of trees\n(max_depth=6).")




.. image-sg:: /gyexamples/images/sphx_glr_plot_parallelism_002.png
   :alt: Parallelization gain depending on the number of trees (max_depth=6).
   :srcset: /gyexamples/images/sphx_glr_plot_parallelism_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(0.5, 1.0, 'Parallelization gain depending\non the number of trees\n(max_depth=6).')



.. GENERATED FROM PYTHON SOURCE LINES 233-241

That does not answer the question we are looking for
as we would like to know the best threshold *th*
which defines the number of observations for which
we should parallelized. This number depends on the number
of trees. A gain > 1 means the parallization should happen
Here, even two observations is ok.
Let's check with lighter trees (``max_depth=2``),
maybe in that case, the conclusion is different.

.. GENERATED FROM PYTHON SOURCE LINES 241-269

.. code-block:: default


    models = {100: (hgb, oinf)}
    for nb in tqdm(set(_[0] for _ in tries)):
        if nb not in models:
            hgb = HistGradientBoostingRegressor(max_iter=nb, max_depth=2)
            hgb.fit(X_train, y_train)
            onx = to_onnx(hgb, X_train[:1].astype(numpy.float32))
            oinf = OnnxInference(onx, runtime='python_compiled')
            models[nb] = (hgb, oinf)

    obs = []
    for nb, N in tqdm(tries):
        hgb, oinf = models[nb]
        m = measure_time("oinf.run({'X': x})",
                         {'oinf': oinf, 'x': X32[:N]},
                         div_by_number=True,
                         number=50)
        m['N'] = N
        m['nb'] = nb
        m['RT'] = 'ONNX'
        obs.append(m)

    df = DataFrame(obs)
    num = ['min_exec', 'average', 'max_exec']
    for c in num:
        df[c] /= df['N']
    df.head()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/17 [00:00<?, ?it/s]      6%|5         | 1/17 [00:00<00:04,  3.70it/s]     12%|#1        | 2/17 [00:00<00:06,  2.45it/s]     24%|##3       | 4/17 [00:01<00:03,  4.10it/s]     29%|##9       | 5/17 [00:01<00:04,  2.49it/s]     35%|###5      | 6/17 [00:02<00:04,  2.67it/s]     41%|####1     | 7/17 [00:02<00:04,  2.35it/s]     47%|####7     | 8/17 [00:03<00:03,  2.53it/s]     53%|#####2    | 9/17 [00:03<00:03,  2.22it/s]     59%|#####8    | 10/17 [00:03<00:02,  2.36it/s]     65%|######4   | 11/17 [00:04<00:03,  1.84it/s]     71%|#######   | 12/17 [00:05<00:02,  1.77it/s]     76%|#######6  | 13/17 [00:05<00:02,  1.94it/s]     82%|########2 | 14/17 [00:06<00:01,  2.04it/s]     88%|########8 | 15/17 [00:07<00:01,  1.62it/s]     94%|#########4| 16/17 [00:07<00:00,  1.57it/s]    100%|##########| 17/17 [00:08<00:00,  1.71it/s]    100%|##########| 17/17 [00:08<00:00,  2.06it/s]
      0%|          | 0/170 [00:00<?, ?it/s]      4%|4         | 7/170 [00:00<00:02, 59.72it/s]      8%|7         | 13/170 [00:00<00:02, 56.18it/s]     11%|#1        | 19/170 [00:00<00:02, 52.57it/s]     15%|#4        | 25/170 [00:00<00:02, 53.73it/s]     18%|#8        | 31/170 [00:00<00:02, 51.14it/s]     22%|##1       | 37/170 [00:00<00:02, 47.75it/s]     25%|##5       | 43/170 [00:00<00:02, 48.88it/s]     28%|##8       | 48/170 [00:00<00:02, 46.62it/s]     31%|###1      | 53/170 [00:01<00:02, 42.31it/s]     35%|###4      | 59/170 [00:01<00:02, 44.89it/s]     38%|###7      | 64/170 [00:01<00:02, 43.49it/s]     41%|####      | 69/170 [00:01<00:02, 37.51it/s]     44%|####4     | 75/170 [00:01<00:02, 41.22it/s]     47%|####7     | 80/170 [00:01<00:02, 40.70it/s]     50%|#####     | 85/170 [00:01<00:02, 33.31it/s]     54%|#####3    | 91/170 [00:02<00:02, 37.95it/s]     56%|#####6    | 96/170 [00:02<00:01, 38.21it/s]     59%|#####9    | 101/170 [00:02<00:02, 34.16it/s]     62%|######1   | 105/170 [00:02<00:02, 32.38it/s]     65%|######4   | 110/170 [00:02<00:01, 35.34it/s]     67%|######7   | 114/170 [00:02<00:01, 34.91it/s]     69%|######9   | 118/170 [00:02<00:01, 30.92it/s]     72%|#######1  | 122/170 [00:03<00:01, 29.27it/s]     75%|#######4  | 127/170 [00:03<00:01, 32.78it/s]     77%|#######7  | 131/170 [00:03<00:01, 32.53it/s]     79%|#######9  | 135/170 [00:03<00:01, 28.57it/s]     82%|########1 | 139/170 [00:03<00:01, 27.01it/s]     85%|########4 | 144/170 [00:03<00:00, 30.67it/s]     87%|########7 | 148/170 [00:03<00:00, 30.52it/s]     89%|########9 | 152/170 [00:04<00:00, 26.64it/s]     91%|#########1| 155/170 [00:04<00:00, 24.16it/s]     94%|#########4| 160/170 [00:04<00:00, 28.76it/s]     96%|#########6| 164/170 [00:04<00:00, 29.29it/s]     99%|#########8| 168/170 [00:04<00:00, 26.31it/s]    100%|##########| 170/170 [00:04<00:00, 34.27it/s]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>N</th>
          <th>nb</th>
          <th>RT</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.000015</td>
          <td>1.688440e-06</td>
          <td>0.000015</td>
          <td>0.000018</td>
          <td>10</td>
          <td>50</td>
          <td>0.000309</td>
          <td>232</td>
          <td>2</td>
          <td>2</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.000015</td>
          <td>2.889513e-07</td>
          <td>0.000015</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000306</td>
          <td>232</td>
          <td>2</td>
          <td>5</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.000015</td>
          <td>2.158380e-07</td>
          <td>0.000015</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000308</td>
          <td>232</td>
          <td>2</td>
          <td>8</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.000016</td>
          <td>4.048876e-07</td>
          <td>0.000015</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000312</td>
          <td>232</td>
          <td>2</td>
          <td>10</td>
          <td>ONNX</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.000016</td>
          <td>3.301538e-07</td>
          <td>0.000016</td>
          <td>0.000016</td>
          <td>10</td>
          <td>50</td>
          <td>0.000318</td>
          <td>232</td>
          <td>2</td>
          <td>15</td>
          <td>ONNX</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 270-271

Measures.

.. GENERATED FROM PYTHON SOURCE LINES 271-281

.. code-block:: default


    gains = []
    for nb in set(df['nb']):
        gain = parallized_gain(df[df.nb == nb])
        gains.append(dict(nb=nb, gain=gain))

    dfg = DataFrame(gains)
    dfg = dfg.sort_values('nb').reset_index(drop=True).copy()
    dfg






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>nb</th>
          <th>gain</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>2</td>
          <td>3.455291</td>
        </tr>
        <tr>
          <th>1</th>
          <td>5</td>
          <td>3.282338</td>
        </tr>
        <tr>
          <th>2</th>
          <td>8</td>
          <td>3.160855</td>
        </tr>
        <tr>
          <th>3</th>
          <td>10</td>
          <td>3.085528</td>
        </tr>
        <tr>
          <th>4</th>
          <td>15</td>
          <td>2.940223</td>
        </tr>
        <tr>
          <th>5</th>
          <td>20</td>
          <td>2.781014</td>
        </tr>
        <tr>
          <th>6</th>
          <td>25</td>
          <td>2.662970</td>
        </tr>
        <tr>
          <th>7</th>
          <td>30</td>
          <td>2.573553</td>
        </tr>
        <tr>
          <th>8</th>
          <td>35</td>
          <td>2.474134</td>
        </tr>
        <tr>
          <th>9</th>
          <td>40</td>
          <td>2.393840</td>
        </tr>
        <tr>
          <th>10</th>
          <td>45</td>
          <td>2.288964</td>
        </tr>
        <tr>
          <th>11</th>
          <td>50</td>
          <td>2.224130</td>
        </tr>
        <tr>
          <th>12</th>
          <td>60</td>
          <td>2.072905</td>
        </tr>
        <tr>
          <th>13</th>
          <td>70</td>
          <td>1.950264</td>
        </tr>
        <tr>
          <th>14</th>
          <td>80</td>
          <td>1.852916</td>
        </tr>
        <tr>
          <th>15</th>
          <td>90</td>
          <td>1.776336</td>
        </tr>
        <tr>
          <th>16</th>
          <td>100</td>
          <td>1.354441</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 282-283

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 283-288

.. code-block:: default


    ax = dfg.set_index('nb').plot()
    ax.set_title(
        "Parallelization gain depending\non the number of trees\n(max_depth=3).")




.. image-sg:: /gyexamples/images/sphx_glr_plot_parallelism_003.png
   :alt: Parallelization gain depending on the number of trees (max_depth=3).
   :srcset: /gyexamples/images/sphx_glr_plot_parallelism_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(0.5, 1.0, 'Parallelization gain depending\non the number of trees\n(max_depth=3).')



.. GENERATED FROM PYTHON SOURCE LINES 289-300

The conclusion is somewhat the same but
it shows that the bigger the number of trees is
the bigger the gain is and under the number of
cores of the processor.

Moving the theshold
+++++++++++++++++++

The last experiment consists in comparing the prediction
time with or without parallelization for different
number of observation.

.. GENERATED FROM PYTHON SOURCE LINES 300-335

.. code-block:: default


    hgb = HistGradientBoostingRegressor(max_iter=40, max_depth=6)
    hgb.fit(X_train, y_train)
    onx = to_onnx(hgb, X_train[:1].astype(numpy.float32))
    oinf = OnnxInference(onx, runtime='python_compiled')


    obs = []
    for N in tqdm(list(range(2, 51))):
        oinf.sequence_[0].ops_.rt_.omp_N_ = 100
        m = measure_time("oinf.run({'X': x})",
                         {'oinf': oinf, 'x': X32[:N]},
                         div_by_number=True,
                         number=20)
        m['N'] = N
        m['RT'] = 'ONNX'
        m['PARALLEL'] = False
        obs.append(m)

        oinf.sequence_[0].ops_.rt_.omp_N_ = 1
        m = measure_time("oinf.run({'X': x})",
                         {'oinf': oinf, 'x': X32[:N]},
                         div_by_number=True,
                         number=50)
        m['N'] = N
        m['RT'] = 'ONNX'
        m['PARALLEL'] = True
        obs.append(m)

    df = DataFrame(obs)
    num = ['min_exec', 'average', 'max_exec']
    for c in num:
        df[c] /= df['N']
    df.head()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/49 [00:00<?, ?it/s]      6%|6         | 3/49 [00:00<00:01, 25.83it/s]     12%|#2        | 6/49 [00:00<00:01, 25.14it/s]     18%|#8        | 9/49 [00:00<00:01, 23.21it/s]     24%|##4       | 12/49 [00:00<00:01, 21.29it/s]     31%|###       | 15/49 [00:00<00:01, 19.57it/s]     35%|###4      | 17/49 [00:00<00:01, 18.06it/s]     39%|###8      | 19/49 [00:00<00:01, 16.99it/s]     43%|####2     | 21/49 [00:01<00:01, 15.99it/s]     47%|####6     | 23/49 [00:01<00:01, 15.08it/s]     51%|#####1    | 25/49 [00:01<00:01, 14.15it/s]     55%|#####5    | 27/49 [00:01<00:01, 13.36it/s]     59%|#####9    | 29/49 [00:01<00:01, 12.68it/s]     63%|######3   | 31/49 [00:01<00:01, 12.06it/s]     67%|######7   | 33/49 [00:02<00:01, 11.44it/s]     71%|#######1  | 35/49 [00:02<00:01, 10.91it/s]     76%|#######5  | 37/49 [00:02<00:01, 10.45it/s]     80%|#######9  | 39/49 [00:02<00:00, 10.02it/s]     84%|########3 | 41/49 [00:03<00:00,  9.34it/s]     86%|########5 | 42/49 [00:03<00:00,  9.18it/s]     88%|########7 | 43/49 [00:03<00:00,  8.76it/s]     90%|########9 | 44/49 [00:03<00:00,  8.63it/s]     92%|#########1| 45/49 [00:03<00:00,  8.45it/s]     94%|#########3| 46/49 [00:03<00:00,  8.33it/s]     96%|#########5| 47/49 [00:03<00:00,  8.20it/s]     98%|#########7| 48/49 [00:03<00:00,  8.05it/s]    100%|##########| 49/49 [00:04<00:00,  7.91it/s]    100%|##########| 49/49 [00:04<00:00, 12.04it/s]


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>N</th>
          <th>RT</th>
          <th>PARALLEL</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.000021</td>
          <td>0.000001</td>
          <td>0.000021</td>
          <td>0.000023</td>
          <td>10</td>
          <td>20</td>
          <td>0.000418</td>
          <td>232</td>
          <td>2</td>
          <td>ONNX</td>
          <td>False</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.000035</td>
          <td>0.000066</td>
          <td>0.000022</td>
          <td>0.000133</td>
          <td>10</td>
          <td>50</td>
          <td>0.000695</td>
          <td>232</td>
          <td>2</td>
          <td>ONNX</td>
          <td>True</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.000016</td>
          <td>0.000001</td>
          <td>0.000015</td>
          <td>0.000016</td>
          <td>10</td>
          <td>20</td>
          <td>0.000465</td>
          <td>232</td>
          <td>3</td>
          <td>ONNX</td>
          <td>False</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.000016</td>
          <td>0.000001</td>
          <td>0.000015</td>
          <td>0.000017</td>
          <td>10</td>
          <td>50</td>
          <td>0.000466</td>
          <td>232</td>
          <td>3</td>
          <td>ONNX</td>
          <td>True</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.000013</td>
          <td>0.000001</td>
          <td>0.000013</td>
          <td>0.000014</td>
          <td>10</td>
          <td>20</td>
          <td>0.000512</td>
          <td>232</td>
          <td>4</td>
          <td>ONNX</td>
          <td>False</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 336-337

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 337-342

.. code-block:: default


    piv = df[['N', 'PARALLEL', 'average']].pivot('N', 'PARALLEL', 'average')
    ax = piv.plot(logy=True)
    ax.set_title("Prediction time with and without parallelization.")




.. image-sg:: /gyexamples/images/sphx_glr_plot_parallelism_004.png
   :alt: Prediction time with and without parallelization.
   :srcset: /gyexamples/images/sphx_glr_plot_parallelism_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(0.5, 1.0, 'Prediction time with and without parallelization.')



.. GENERATED FROM PYTHON SOURCE LINES 343-344

Parallelization is working.

.. GENERATED FROM PYTHON SOURCE LINES 344-347

.. code-block:: default



    plt.show()








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  22.746 seconds)


.. _sphx_glr_download_gyexamples_plot_parallelism.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_parallelism.py <plot_parallelism.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_parallelism.ipynb <plot_parallelism.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
