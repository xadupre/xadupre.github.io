
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gyexamples/plot_onnx_tree_ensemble_parallel.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gyexamples_plot_onnx_tree_ensemble_parallel.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gyexamples_plot_onnx_tree_ensemble_parallel.py:


.. _onnxtreeensembleparallelrst:

TreeEnsembleRegressor and parallelisation
=========================================

The operator `TreeEnsembleClassifier
<https://github.com/onnx/onnx/blob/master/docs/Operators-ml.md#
ai.onnx.ml.TreeEnsembleClassifier>`_ describe any tree model
(decision tree, random forest, gradient boosting). The runtime
is usually implements in C/C++ and uses parallelisation.
The notebook studies the impact of the parallelisation.

.. contents::
    :local

Graph
+++++

The following dummy graph shows the time ratio between two runtimes
depending on the number of observations in a batch (N) and
the number of trees in the forest.

.. GENERATED FROM PYTHON SOURCE LINES 25-60

.. code-block:: default


    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from mlprodict.onnxrt import OnnxInference
    from onnxruntime import InferenceSession
    from skl2onnx import to_onnx
    from mlprodict.onnxrt.validate.validate_benchmark import benchmark_fct
    import sklearn
    import numpy
    from tqdm import tqdm
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.datasets import make_classification
    import matplotlib.pyplot as plt
    from mlprodict.plotting.plotting import plot_benchmark_metrics


    def plot_metric(metric, ax=None, xlabel="N", ylabel="trees", middle=1.,
                    transpose=False, shrink=1.0, title=None, figsize=None):
        if figsize is not None and ax is None:
            _, ax = plt.subplots(1, 1, figsize=figsize)

        ax, cbar = plot_benchmark_metrics(
            metric, ax=ax, xlabel=xlabel, ylabel=ylabel, middle=middle,
            transpose=transpose, cbar_kw={'shrink': shrink})
        if title is not None:
            ax.set_title(title)
        return ax


    data = {(1, 1): 0.1, (10, 1): 1, (1, 10): 2,
            (10, 10): 100, (100, 1): 100, (100, 10): 1000}

    fig, ax = plt.subplots(1, 2, figsize=(10, 4))
    plot_metric(data, ax[0], shrink=0.6)




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_001.png
   :alt: plot onnx tree ensemble parallel
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 62-66

.. code-block:: default


    plot_metric(data, ax[1], transpose=True)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:xlabel='trees', ylabel='N'>



.. GENERATED FROM PYTHON SOURCE LINES 67-72

scikit-learn: T trees vs 1 tree
+++++++++++++++++++++++++++++++

Let's do first compare a *GradientBoostingClassifier* from
*scikit-learn* with 1 tree against multiple trees.

.. GENERATED FROM PYTHON SOURCE LINES 72-84

.. code-block:: default


    # In[4]:


    ntest = 10000
    X, y = make_classification(
        n_samples=10000 + ntest, n_features=10, n_informative=5,
        n_classes=2, random_state=11)
    X_train, X_test, y_train, y_test = X[:-
                                         ntest], X[-ntest:], y[:-ntest], y[-ntest:]









.. GENERATED FROM PYTHON SOURCE LINES 86-99

.. code-block:: default



    ModelToTest = GradientBoostingClassifier

    N = [1, 10, 100, 1000, 10000]
    T = [1, 2, 10, 20, 50]

    models = {}
    for nt in tqdm(T):
        rf = ModelToTest(n_estimators=nt, max_depth=7).fit(X_train, y_train)
        models[nt] = rf






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:00<00:00,  4.11it/s]     40%|####      | 2/5 [00:00<00:00,  3.02it/s]     60%|######    | 3/5 [00:02<00:02,  1.07s/it]     80%|########  | 4/5 [00:06<00:02,  2.19s/it]    100%|##########| 5/5 [00:16<00:00,  4.91s/it]    100%|##########| 5/5 [00:16<00:00,  3.24s/it]




.. GENERATED FROM PYTHON SOURCE LINES 100-101

Benchmark.

.. GENERATED FROM PYTHON SOURCE LINES 101-156

.. code-block:: default



    def benchmark(X, fct1, fct2, N, repeat=10, number=20):

        def ti(r, n):
            if n <= 1:
                return 40 * r
            if n <= 10:
                return 10 * r
            if n <= 100:
                return 4 * r
            if n <= 1000:
                return r
            return r // 2

        with sklearn.config_context(assume_finite=True):
            # to warm up the engine
            time_kwargs = {n: dict(repeat=10, number=10) for n in N}
            benchmark_fct(fct1, X, time_kwargs=time_kwargs, skip_long_test=False)
            benchmark_fct(fct2, X, time_kwargs=time_kwargs, skip_long_test=False)
            # real measure
            time_kwargs = {n: dict(repeat=ti(repeat, n), number=number) for n in N}
            res1 = benchmark_fct(
                fct1, X, time_kwargs=time_kwargs, skip_long_test=False)
            res2 = benchmark_fct(
                fct2, X, time_kwargs=time_kwargs, skip_long_test=False)
        res = {}
        for r in sorted(res1):
            r1 = res1[r]
            r2 = res2[r]
            ratio = r2['ttime'] / r1['ttime']
            res[r] = ratio
        return res


    def tree_benchmark(X, fct1, fct2, T, N, repeat=20, number=10):
        bench = {}
        for t in tqdm(T):
            if callable(X):
                x = X(t)
            else:
                x = X
            r = benchmark(x, fct1(t), fct2(t), N, repeat=repeat, number=number)
            for n, v in r.items():
                bench[n, t] = v
        return bench


    bench = tree_benchmark(X_test.astype(numpy.float32),
                           lambda t: models[1].predict,
                           lambda t: models[t].predict, T, N)

    list(bench.items())[:3]






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:19<01:17, 19.33s/it]     40%|####      | 2/5 [00:38<00:58, 19.45s/it]     60%|######    | 3/5 [01:00<00:40, 20.24s/it]     80%|########  | 4/5 [01:23<00:21, 21.44s/it]    100%|##########| 5/5 [01:52<00:00, 24.27s/it]    100%|##########| 5/5 [01:52<00:00, 22.52s/it]

    [((1, 1), 1.0013139904270998), ((10, 1), 1.0010604038493331), ((100, 1), 1.0035114077891263)]



.. GENERATED FROM PYTHON SOURCE LINES 157-158

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 158-164

.. code-block:: default



    plot_metric(bench, title="scikit-learn 1 tree vs scikit-learn T trees\n"
                "< 1 means onnxruntime is faster")





.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_002.png
   :alt: scikit-learn 1 tree vs scikit-learn T trees < 1 means onnxruntime is faster
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn 1 tree vs scikit-learn T trees\n< 1 means onnxruntime is faster'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 165-173

As expected, all ratio on first line are close to 1 since
both models are the same. fourth line, second column
(T=20, N=10) means an ensemble with 20 trees is slower to
compute the predictions of 10 observations in a batch compare
to an ensemble with 1 tree.

scikit-learn against onnxuntime
+++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 173-183

.. code-block:: default



    X32 = X_test.astype(numpy.float32)
    models_onnx = {t: to_onnx(m, X32[:1]) for t, m in models.items()}


    sess_models = {t: InferenceSession(mo.SerializeToString())
                   for t, mo in models_onnx.items()}






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)




.. GENERATED FROM PYTHON SOURCE LINES 184-185

Benchmark.

.. GENERATED FROM PYTHON SOURCE LINES 185-194

.. code-block:: default



    bench_ort = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: models[t].predict_proba,
        lambda t: (lambda x, t_=t, se=sess_models: se[t_].run(None, {'X': x})),
        T, N)
    bench_ort





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:13<00:54, 13.53s/it]     40%|####      | 2/5 [00:27<00:41, 13.68s/it]     60%|######    | 3/5 [00:44<00:30, 15.32s/it]     80%|########  | 4/5 [01:04<00:17, 17.07s/it]    100%|##########| 5/5 [01:31<00:00, 20.86s/it]    100%|##########| 5/5 [01:31<00:00, 18.38s/it]

    {(1, 1): 0.10424098947333152, (10, 1): 0.11678877928411963, (100, 1): 0.25231563217446046, (1000, 1): 1.3024337321818737, (10000, 1): 3.4593536289194544, (1, 2): 0.10272153933050635, (10, 2): 0.11772712447442003, (100, 2): 0.2556447100796513, (1000, 2): 1.2122698480844467, (10000, 2): 2.9812042036055413, (1, 10): 0.10301340113624993, (10, 10): 0.12833744240825215, (100, 10): 0.31454597196881834, (1000, 10): 1.246646803470623, (10000, 10): 2.015540408614125, (1, 20): 0.10612854041592411, (10, 20): 0.1455467885493075, (100, 20): 0.2953096260061555, (1000, 20): 0.9461570860134277, (10000, 20): 1.302789057993744, (1, 50): 0.10685798491586797, (10, 50): 0.1816632328014663, (100, 50): 0.25140926148074827, (1000, 50): 0.6322834057818965, (10000, 50): 0.7627067124199851}



.. GENERATED FROM PYTHON SOURCE LINES 195-196

Graphs

.. GENERATED FROM PYTHON SOURCE LINES 196-202

.. code-block:: default



    plot_metric(bench_ort, title="scikit-learn vs onnxruntime\n < 1 "
                "means onnxruntime is faster")





.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_003.png
   :alt: scikit-learn vs onnxruntime  < 1 means onnxruntime is faster
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn vs onnxruntime\n < 1 means onnxruntime is faster'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 203-211

We see onnxruntime is fast for small batches,
still faster but not that much for big batches.

ZipMap operator
+++++++++++++++

ZipMap just creates a new container for the same results.
The copy may impact the ratio. Let's remove it from the equation.

.. GENERATED FROM PYTHON SOURCE LINES 211-221

.. code-block:: default


    X32 = X_test.astype(numpy.float32)
    models_onnx = {t: to_onnx(m, X32[:1],
                              options={ModelToTest: {'zipmap': False}})
                   for t, m in models.items()}

    sess_models = {t: InferenceSession(mo.SerializeToString())
                   for t, mo in models_onnx.items()}






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)




.. GENERATED FROM PYTHON SOURCE LINES 222-223

Benchmarks.

.. GENERATED FROM PYTHON SOURCE LINES 223-233

.. code-block:: default



    bench_ort = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: models[t].predict_proba,
        lambda t: (lambda x, t_=t, se=sess_models: se[t_].run(None, {'X': x})),
        T, N)

    bench_ort





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:10<00:40, 10.23s/it]     40%|####      | 2/5 [00:20<00:31, 10.39s/it]     60%|######    | 3/5 [00:35<00:25, 12.50s/it]     80%|########  | 4/5 [00:53<00:14, 14.61s/it]    100%|##########| 5/5 [01:20<00:00, 19.02s/it]    100%|##########| 5/5 [01:20<00:00, 16.08s/it]

    {(1, 1): 0.09160526187052971, (10, 1): 0.09296710877481121, (100, 1): 0.1015025620196855, (1000, 1): 0.12065028334154042, (10000, 1): 0.16773957684570978, (1, 2): 0.09130045194830455, (10, 2): 0.09397244732312922, (100, 2): 0.10312349910439614, (1000, 2): 0.13042786917456167, (10000, 2): 0.18505298189581296, (1, 10): 0.09155368229072167, (10, 10): 0.10268164556380488, (100, 10): 0.20180069174766888, (1000, 10): 0.6213442745797167, (10000, 10): 1.2076668691650052, (1, 20): 0.09317032368910169, (10, 20): 0.11914873325753274, (100, 20): 0.1936233074472649, (1000, 20): 0.5144874093269644, (10000, 20): 0.8976505367187934, (1, 50): 0.09616321623828449, (10, 50): 0.16896410350864716, (100, 50): 0.1848226671969928, (1000, 50): 0.4371413840226076, (10000, 50): 0.7370800222287045}



.. GENERATED FROM PYTHON SOURCE LINES 234-235

Graphs.

.. GENERATED FROM PYTHON SOURCE LINES 235-270

.. code-block:: default


    plot_metric(bench_ort, title="scikit-learn vs onnxruntime (no zipmap)\n < 1 "
                "means onnxruntime is faster")


    # ZipMap removal significantly improves.
    #
    # Implementation details for mlprodict runtime
    # ++++++++++++++++++++++++++++++++++++++++++++
    #
    # The runtime implemented in :epkg:`mlprodict` mostly relies on
    # two files:
    # * `op_tree_ensemble_common_p_agg_.hpp <https://github.com/sdpython/
    #   mlprodict/blob/master/mlprodict/onnxrt/ops_cpu/
    #   op_tree_ensemble_common_p_agg_.hpp>`_
    # * `op_tree_ensemble_common_p_.hpp <https://github.com/sdpython/
    #   mlprodict/blob/master/mlprodict/onnxrt/ops_cpu/
    #   op_tree_ensemble_common_p_.hpp>`_
    #
    # The runtime builds a tree structure, computes the output of every
    # tree and then agregates them. The implementation distringuishes
    # when the batch size contains only 1 observations or many.
    # It parallelizes on the following conditions:
    # * if the batch size $N \geqslant N_0$, it then parallelizes per
    #   observation, asuming every one is independant,
    # * if the batch size $N = 1$ and the number of trees
    #   $T \geqslant T_0$, it then parallelizes per tree.
    #
    # scikit-learn against mlprodict, no parallelisation
    # ++++++++++++++++++++++++++++++++++++++++++++++++++


    oinf_models = {t: OnnxInference(mo, runtime="python_compiled")
                   for t, mo in models_onnx.items()}




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_004.png
   :alt: scikit-learn vs onnxruntime (no zipmap)  < 1 means onnxruntime is faster
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 271-272

Let's modify the thresholds which trigger the parallelization.

.. GENERATED FROM PYTHON SOURCE LINES 272-278

.. code-block:: default


    for _, oinf in oinf_models.items():
        oinf.sequence_[0].ops_.rt_.omp_tree_ = 10000000
        oinf.sequence_[0].ops_.rt_.omp_N_ = 10000000









.. GENERATED FROM PYTHON SOURCE LINES 279-280

Benchmarks.

.. GENERATED FROM PYTHON SOURCE LINES 280-289

.. code-block:: default



    bench_mlp = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: models[t].predict,
        lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),
        T, N)
    bench_mlp





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:10<00:43, 10.93s/it]     40%|####      | 2/5 [00:22<00:33, 11.24s/it]     60%|######    | 3/5 [00:38<00:27, 13.56s/it]     80%|########  | 4/5 [01:02<00:17, 17.75s/it]    100%|##########| 5/5 [01:48<00:00, 27.71s/it]    100%|##########| 5/5 [01:48<00:00, 21.65s/it]

    {(1, 1): 0.05356442598317358, (10, 1): 0.05662386236600935, (100, 1): 0.08313073508816103, (1000, 1): 0.26075600463786647, (10000, 1): 0.605413012942091, (1, 2): 0.05378774968610339, (10, 2): 0.05826168857114018, (100, 2): 0.09688015493147759, (1000, 2): 0.3377353509096784, (10000, 2): 0.7598735740085413, (1, 10): 0.05576816363606588, (10, 10): 0.07457427919256454, (100, 10): 0.230186210189309, (1000, 10): 0.9113223319043814, (10000, 10): 1.5495759563420775, (1, 20): 0.057816890304058004, (10, 20): 0.0996111314287846, (100, 20): 0.420102313595589, (1000, 20): 1.5082676874680272, (10000, 20): 2.2372318141019116, (1, 50): 0.06320928546611229, (10, 50): 0.14710144604571573, (100, 50): 0.7217686608041467, (1000, 50): 2.0636704220891655, (10000, 50): 2.610534074510723}



.. GENERATED FROM PYTHON SOURCE LINES 290-291

Graphs.

.. GENERATED FROM PYTHON SOURCE LINES 291-295

.. code-block:: default


    plot_metric(bench_mlp, title="scikit-learn vs mlprodict\n < 1 "
                "means mlprodict is faster")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_005.png
   :alt: scikit-learn vs mlprodict  < 1 means mlprodict is faster
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn vs mlprodict\n < 1 means mlprodict is faster'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 296-297

Let's compare *onnxruntime* against *mlprodict*.

.. GENERATED FROM PYTHON SOURCE LINES 297-306

.. code-block:: default



    bench_mlp_ort = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: (lambda x, t_=t, se=sess_models: se[t_].run(None, {'X': x})),
        lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),
        T, N)
    bench_mlp_ort





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:02<00:08,  2.08s/it]     40%|####      | 2/5 [00:04<00:06,  2.30s/it]     60%|######    | 3/5 [00:12<00:09,  4.90s/it]     80%|########  | 4/5 [00:26<00:08,  8.65s/it]    100%|##########| 5/5 [00:58<00:00, 17.07s/it]    100%|##########| 5/5 [00:58<00:00, 11.79s/it]

    {(1, 1): 0.6181220427027221, (10, 1): 0.6386502367086391, (100, 1): 0.8741933354387792, (1000, 1): 2.274159029786125, (10000, 1): 3.884654132265959, (1, 2): 0.6195137654648646, (10, 2): 0.6540695059224749, (100, 2): 0.9324101459676842, (1000, 2): 2.7452679251267442, (10000, 2): 4.3853907604775335, (1, 10): 0.6379033012409552, (10, 10): 0.7513338162478611, (100, 10): 1.1357594810690215, (1000, 10): 1.4311997690551637, (10000, 10): 1.705233123950048, (1, 20): 0.6482833658279787, (10, 20): 0.8621562354658804, (100, 20): 2.195703118540976, (1000, 20): 2.885502595574936, (10000, 20): 3.2773663085782707, (1, 50): 0.6870979758865935, (10, 50): 0.9140824208550775, (100, 50): 3.9241094829676544, (1000, 50): 4.677945898305827, (10000, 50): 4.78176056757303}



.. GENERATED FROM PYTHON SOURCE LINES 307-308

Graphs.

.. GENERATED FROM PYTHON SOURCE LINES 308-313

.. code-block:: default



    plot_metric(bench_mlp_ort, title="onnxruntime vs mlprodict\n < 1 means "
                "mlprodict is faster\nno parallelisation")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_006.png
   :alt: onnxruntime vs mlprodict  < 1 means mlprodict is faster no parallelisation
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_006.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'onnxruntime vs mlprodict\n < 1 means mlprodict is faster\nno parallelisation'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 314-317

This implementation is faster except for high number of trees
or high number of observations. Let's add parallelisation for
trees and observations.

.. GENERATED FROM PYTHON SOURCE LINES 317-332

.. code-block:: default



    for _, oinf in oinf_models.items():
        oinf.sequence_[0].ops_.rt_.omp_tree_ = 2
        oinf.sequence_[0].ops_.rt_.omp_N_ = 2


    bench_mlp_para = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: models[t].predict,
        lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),
        T, N)

    bench_mlp_para





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:10<00:43, 10.78s/it]     40%|####      | 2/5 [00:21<00:32, 10.96s/it]     60%|######    | 3/5 [00:34<00:23, 11.82s/it]     80%|########  | 4/5 [00:49<00:13, 13.16s/it]    100%|##########| 5/5 [01:12<00:00, 16.43s/it]    100%|##########| 5/5 [01:12<00:00, 14.43s/it]

    {(1, 1): 0.05453706562097426, (10, 1): 0.06342786451923263, (100, 1): 0.08319796403741121, (1000, 1): 0.21201494400781845, (10000, 1): 0.45980915967145775, (1, 2): 0.05450807888947425, (10, 2): 0.06387851448364172, (100, 2): 0.08600896081036033, (1000, 2): 0.23862868593946532, (10000, 2): 0.49699850353527714, (1, 10): 0.06759102417333916, (10, 10): 0.07297627836376824, (100, 10): 0.0897689816093807, (1000, 10): 0.1682068309826805, (10000, 10): 0.25397561296432564, (1, 20): 0.06702751740881528, (10, 20): 0.07138708219995443, (100, 20): 0.09074010400094405, (1000, 20): 0.16229510121461688, (10000, 20): 0.22042517300202016, (1, 50): 0.06600002698875726, (10, 50): 0.06876315408567246, (100, 50): 0.09474214305221844, (1000, 50): 0.17166484497359427, (10000, 50): 0.20980702452014977}



.. GENERATED FROM PYTHON SOURCE LINES 333-334

Graphs.

.. GENERATED FROM PYTHON SOURCE LINES 334-339

.. code-block:: default



    plot_metric(bench_mlp_para, title="scikit-learn vs mlprodict\n < 1 means "
                "mlprodict is faster\nparallelisation")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_007.png
   :alt: scikit-learn vs mlprodict  < 1 means mlprodict is faster parallelisation
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_007.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn vs mlprodict\n < 1 means mlprodict is faster\nparallelisation'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 340-342

Parallelisation does improve the computation time when N is big.
Let's compare with and without parallelisation.

.. GENERATED FROM PYTHON SOURCE LINES 342-352

.. code-block:: default



    bench_para = {}
    for k, v in bench_mlp.items():
        bench_para[k] = bench_mlp_para[k] / v


    plot_metric(bench_para, title="mlprodict vs mlprodict parallelized\n < 1 "
                "means parallelisation is faster")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_008.png
   :alt: mlprodict vs mlprodict parallelized  < 1 means parallelisation is faster
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_008.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'mlprodict vs mlprodict parallelized\n < 1 means parallelisation is faster'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 353-356

Parallelisation per trees does not seem to be efficient.
Let's confirm with a proper benchmark as the previous merges
results from two benchmarks.

.. GENERATED FROM PYTHON SOURCE LINES 356-377

.. code-block:: default



    for _, oinf in oinf_models.items():
        oinf.sequence_[0].ops_.rt_.omp_tree_ = 1000000
        oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000

    oinf_models_para = {t: OnnxInference(mo, runtime="python_compiled")
                        for t, mo in models_onnx.items()}

    for _, oinf in oinf_models_para.items():
        oinf.sequence_[0].ops_.rt_.omp_tree_ = 2
        oinf.sequence_[0].ops_.rt_.omp_N_ = 2

    bench_mlp_para = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: (lambda x, t_=t, oi=oinf_models: oi[t_].run({'X': x})),
        lambda t: (lambda x, t_=t, oi=oinf_models_para: oi[t_].run({'X': x})),
        T, N, repeat=20, number=20)

    bench_mlp_para





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:03<00:15,  3.99s/it]     40%|####      | 2/5 [00:08<00:13,  4.48s/it]     60%|######    | 3/5 [00:18<00:13,  6.79s/it]     80%|########  | 4/5 [00:37<00:11, 11.62s/it]    100%|##########| 5/5 [01:21<00:00, 23.24s/it]    100%|##########| 5/5 [01:21<00:00, 16.25s/it]

    {(1, 1): 0.9886636457450123, (10, 1): 1.1096448660119518, (100, 1): 1.1080829154539347, (1000, 1): 1.1344177371014905, (10000, 1): 1.1255913281686416, (1, 2): 0.9858734936236828, (10, 2): 1.0961114615231888, (100, 2): 1.0507032321035432, (1000, 2): 1.0239198393684823, (10000, 2): 1.0069208019350238, (1, 10): 1.2022700258717773, (10, 10): 0.95836266571393, (100, 10): 0.3811194576272681, (1000, 10): 0.18050631084625796, (10000, 10): 0.1603224040911625, (1, 20): 1.1281791151689267, (10, 20): 0.69897001159127, (100, 20): 0.20707471424511897, (1000, 20): 0.105077090782184, (10000, 20): 0.09869046176393491, (1, 50): 1.0259053131218472, (10, 50): 0.45283581637141895, (100, 50): 0.12778776192478603, (1000, 50): 0.08253996466677072, (10000, 50): 0.07961939329901453}



.. GENERATED FROM PYTHON SOURCE LINES 378-379

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 379-384

.. code-block:: default



    plot_metric(bench_mlp_para, title="mlprodict vs mlprodict parallelized\n < 1 "
                "means parallelisation is faster\nsame baseline")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_009.png
   :alt: mlprodict vs mlprodict parallelized  < 1 means parallelisation is faster same baseline
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_009.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'mlprodict vs mlprodict parallelized\n < 1 means parallelisation is faster\nsame baseline'}, xlabel='N', ylabel='trees'>



.. GENERATED FROM PYTHON SOURCE LINES 385-394

It should be run on different machines. On the current one,
parallelisation per trees (when N=1) does not seem to help.
Parallelisation for a small number of observations does not
seem to help either. So we need to find some threshold.

Parallelisation per trees
+++++++++++++++++++++++++

Let's study the parallelisation per tree. We need to train new models.

.. GENERATED FROM PYTHON SOURCE LINES 394-406

.. code-block:: default


    # In[33]:


    N2 = [1, 10]
    T2 = [1, 2, 10, 50, 100, 150, 200, 300, 400, 500]

    models2 = {}
    for nt in tqdm(T2):
        rf = ModelToTest(n_estimators=nt, max_depth=7).fit(X_train, y_train)
        models2[nt] = rf





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/10 [00:00<?, ?it/s]     10%|#         | 1/10 [00:00<00:01,  5.10it/s]     20%|##        | 2/10 [00:00<00:02,  3.23it/s]     30%|###       | 3/10 [00:02<00:07,  1.05s/it]     40%|####      | 4/10 [00:12<00:26,  4.45s/it]     50%|#####     | 5/10 [00:31<00:48,  9.79s/it]     60%|######    | 6/10 [01:00<01:05, 16.28s/it]     70%|#######   | 7/10 [01:38<01:10, 23.53s/it]     80%|########  | 8/10 [02:36<01:08, 34.38s/it]     90%|######### | 9/10 [03:53<00:47, 47.61s/it]    100%|##########| 10/10 [05:29<00:00, 62.56s/it]    100%|##########| 10/10 [05:29<00:00, 32.91s/it]




.. GENERATED FROM PYTHON SOURCE LINES 407-408

Conversion to ONNX.

.. GENERATED FROM PYTHON SOURCE LINES 408-424

.. code-block:: default


    X32 = X_test.astype(numpy.float32)
    models2_onnx = {t: to_onnx(m, X32[:1]) for t, m in models2.items()}

    oinf_models2 = {t: OnnxInference(mo, runtime="python_compiled")
                    for t, mo in models2_onnx.items()}
    for _, oinf in oinf_models2.items():
        oinf.sequence_[0].ops_.rt_.omp_tree_ = 1000000
        oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000

    oinf_models2_para = {t: OnnxInference(
        mo, runtime="python_compiled") for t, mo in models2_onnx.items()}
    for _, oinf in oinf_models2_para.items():
        oinf.sequence_[0].ops_.rt_.omp_tree_ = 2
        oinf.sequence_[0].ops_.rt_.omp_N_ = 100





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)
    /usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.
      warnings.warn(msg, category=FutureWarning)




.. GENERATED FROM PYTHON SOURCE LINES 425-426

And benchmark.

.. GENERATED FROM PYTHON SOURCE LINES 426-437

.. code-block:: default


    # In[36]:


    bench_mlp_tree = tree_benchmark(
        X_test.astype(numpy.float32),
        lambda t: (lambda x, t_=t, oi=oinf_models2: oi[t_].run({'X': x})),
        lambda t: (lambda x, t_=t, oi=oinf_models2_para: oi[t_].run({'X': x})),
        T2, N2, repeat=20, number=20)
    bench_mlp_tree





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/10 [00:00<?, ?it/s]     10%|#         | 1/10 [00:02<00:22,  2.49s/it]     20%|##        | 2/10 [00:05<00:20,  2.50s/it]     30%|###       | 3/10 [00:07<00:18,  2.65s/it]     40%|####      | 4/10 [00:11<00:17,  2.92s/it]     50%|#####     | 5/10 [00:15<00:16,  3.38s/it]     60%|######    | 6/10 [00:20<00:16,  4.01s/it]     70%|#######   | 7/10 [00:26<00:14,  4.76s/it]     80%|########  | 8/10 [00:38<00:13,  6.87s/it]     90%|######### | 9/10 [00:57<00:10, 10.65s/it]    100%|##########| 10/10 [01:23<00:00, 15.61s/it]    100%|##########| 10/10 [01:23<00:00,  8.40s/it]

    {(1, 1): 0.998938550827195, (10, 1): 0.9948149156512845, (1, 2): 1.0006619455466634, (10, 2): 0.9994121174297411, (1, 10): 1.1485504885333047, (10, 10): 0.9886337288255559, (1, 50): 1.0284039387762969, (10, 50): 0.5329759339331916, (1, 100): 0.9380063973810872, (10, 100): 0.3251066514081821, (1, 150): 0.8668314193978172, (10, 150): 0.22210973759612498, (1, 200): 0.8299537214846059, (10, 200): 0.17138233368561928, (1, 300): 0.4494230848976636, (10, 300): 0.09855151383524731, (1, 400): 0.2627816525073751, (10, 400): 0.06993722466695872, (1, 500): 0.19699645740341667, (10, 500): 0.06285146577195286}



.. GENERATED FROM PYTHON SOURCE LINES 438-439

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 439-445

.. code-block:: default


    plot_metric(
        bench_mlp_tree, transpose=True, figsize=(10, 3), shrink=0.5,
        title="mlprodict vs mlprodict parallelized\n < 1 means parallelisation "
        "is faster")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_010.png
   :alt: mlprodict vs mlprodict parallelized  < 1 means parallelisation is faster
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_010.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'mlprodict vs mlprodict parallelized\n < 1 means parallelisation is faster'}, xlabel='trees', ylabel='N'>



.. GENERATED FROM PYTHON SOURCE LINES 446-458

The parallelisation starts to be below 1 after 400 trees.
For 10 observations, there is no parallelisation neither by
trees nor by observations. Ratios are close to 1.
The gain obviously depends on the tree depth. You can
try with a different max depth and the number of trees
parallelisation becomes interesting depending on the tree depth.

Multi-Class DecisionTreeClassifier
++++++++++++++++++++++++++++++++++

Same experiment when the number of tree is 1 but then we
change the number of classes.

.. GENERATED FROM PYTHON SOURCE LINES 458-492

.. code-block:: default



    ModelToTest = DecisionTreeClassifier

    C = [2, 5, 10, 15, 20, 30, 40, 50]
    N = [1, 10, 100, 1000, 10000]
    trees = {}
    for cl in tqdm(C):

        ntest = 10000
        X, y = make_classification(
            n_samples=10000 + ntest, n_features=12, n_informative=8,
            n_classes=cl, random_state=11)
        X_train, X_test, y_train, y_test = (
            X[:-ntest], X[-ntest:], y[:-ntest], y[-ntest:])

        dt = ModelToTest(max_depth=7).fit(X_train, y_train)

        X32 = X_test.astype(numpy.float32)
        monnx = to_onnx(dt, X32[:1])
        oinf = OnnxInference(monnx)
        oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000
        trees[cl] = dict(model=dt, X_test=X_test, X32=X32, monnx=monnx, oinf=oinf)


    bench_dt = tree_benchmark(lambda cl: trees[cl]['X32'],
                              lambda cl: trees[cl]['model'].predict_proba,
                              lambda cl: (
                                  lambda x, c=cl: trees[c]['oinf'].run({'X': x})),
                              C, N)

    bench_dt






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/8 [00:00<?, ?it/s]     12%|#2        | 1/8 [00:00<00:02,  2.53it/s]     25%|##5       | 2/8 [00:00<00:02,  2.45it/s]     38%|###7      | 3/8 [00:01<00:02,  2.33it/s]     50%|#####     | 4/8 [00:01<00:01,  2.20it/s]     62%|######2   | 5/8 [00:02<00:01,  2.06it/s]     75%|#######5  | 6/8 [00:02<00:01,  1.85it/s]     88%|########7 | 7/8 [00:03<00:00,  1.65it/s]    100%|##########| 8/8 [00:04<00:00,  1.49it/s]    100%|##########| 8/8 [00:04<00:00,  1.78it/s]
      0%|          | 0/8 [00:00<?, ?it/s]     12%|#2        | 1/8 [00:05<00:38,  5.48s/it]     25%|##5       | 2/8 [00:11<00:34,  5.70s/it]     38%|###7      | 3/8 [00:17<00:29,  5.97s/it]     50%|#####     | 4/8 [00:24<00:25,  6.28s/it]     62%|######2   | 5/8 [00:31<00:19,  6.58s/it]     75%|#######5  | 6/8 [00:39<00:14,  7.01s/it]     88%|########7 | 7/8 [00:47<00:07,  7.52s/it]    100%|##########| 8/8 [00:57<00:00,  8.09s/it]    100%|##########| 8/8 [00:57<00:00,  7.16s/it]

    {(1, 2): 0.4629677099759947, (10, 2): 0.45930770762329487, (100, 2): 0.48996870805773557, (1000, 2): 0.662240470434861, (10000, 2): 0.879852891539155, (1, 5): 0.4641832898143389, (10, 5): 0.4614435882386817, (100, 5): 0.5129970471372728, (1000, 5): 0.692930393281724, (10000, 5): 0.9006308155320546, (1, 10): 0.4685666864325466, (10, 10): 0.46825440229403226, (100, 10): 0.5174533938211818, (1000, 10): 0.6698227512463435, (10000, 10): 0.8289910363406787, (1, 15): 0.4676642731763692, (10, 15): 0.45681163665174107, (100, 15): 0.5123165944837202, (1000, 15): 0.6667628885257949, (10000, 15): 0.8091675756063686, (1, 20): 0.4650270299183181, (10, 20): 0.45970365184368556, (100, 20): 0.5057628455518898, (1000, 20): 0.6511569793418237, (10000, 20): 0.7661758327691311, (1, 30): 0.4609559686741916, (10, 30): 0.46876930855876825, (100, 30): 0.5194527600395841, (1000, 30): 0.6315142657738696, (10000, 30): 0.7384215269093166, (1, 40): 0.4626154859541388, (10, 40): 0.47000943519199584, (100, 40): 0.529953696192831, (1000, 40): 0.6390411915079991, (10000, 40): 0.7517517128419064, (1, 50): 0.466000946122624, (10, 50): 0.4754648560243059, (100, 50): 0.5415967322325641, (1000, 50): 0.6257865238530069, (10000, 50): 0.7520660507263313}



.. GENERATED FROM PYTHON SOURCE LINES 493-494

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 494-500

.. code-block:: default



    plot_metric(bench_dt, ylabel="classes", transpose=True, shrink=0.75,
                title="scikit-learn vs mlprodict (DecisionTreeClassifier) \n"
                "< 1 means mlprodict is faster\n no parallelisation")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_011.png
   :alt: scikit-learn vs mlprodict (DecisionTreeClassifier)  < 1 means mlprodict is faster  no parallelisation
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_011.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn vs mlprodict (DecisionTreeClassifier) \n< 1 means mlprodict is faster\n no parallelisation'}, xlabel='classes', ylabel='N'>



.. GENERATED FROM PYTHON SOURCE LINES 501-503

Multi-class LogisticRegression
++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 503-536

.. code-block:: default



    ModelToTest = LogisticRegression

    C = [2, 5, 10, 15, 20]
    N = [1, 10, 100, 1000, 10000]

    models = {}
    for cl in tqdm(C):

        ntest = 10000
        X, y = make_classification(
            n_samples=10000 + ntest, n_features=10, n_informative=6,
            n_classes=cl, random_state=11)
        X_train, X_test, y_train, y_test = (
            X[:-ntest], X[-ntest:], y[:-ntest], y[-ntest:])

        model = ModelToTest().fit(X_train, y_train)

        X32 = X_test.astype(numpy.float32)
        monnx = to_onnx(model, X32[:1])
        oinf = OnnxInference(monnx)
        models[cl] = dict(model=model, X_test=X_test,
                          X32=X32, monnx=monnx, oinf=oinf)


    bench_lr = tree_benchmark(lambda cl: models[cl]['X32'],
                              lambda cl: models[cl]['model'].predict_proba,
                              lambda cl: (
                                  lambda x, c=cl: trees[c]['oinf'].run({'X': x})),
                              C, N)
    bench_lr





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:00<00:01,  3.00it/s]     40%|####      | 2/5 [00:01<00:01,  1.75it/s]     60%|######    | 3/5 [00:01<00:01,  1.49it/s]     80%|########  | 4/5 [00:03<00:00,  1.05it/s]    100%|##########| 5/5 [00:04<00:00,  1.18s/it]    100%|##########| 5/5 [00:04<00:00,  1.03it/s]
      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:05<00:23,  5.92s/it]     40%|####      | 2/5 [00:14<00:22,  7.42s/it]     60%|######    | 3/5 [00:24<00:17,  8.63s/it]     80%|########  | 4/5 [00:36<00:09,  9.78s/it]    100%|##########| 5/5 [00:49<00:00, 10.95s/it]    100%|##########| 5/5 [00:49<00:00,  9.81s/it]

    {(1, 2): 0.38045560796482486, (10, 2): 0.39061525707006556, (100, 2): 0.42234162733373476, (1000, 2): 0.6639939309529477, (10000, 2): 1.1419368661944793, (1, 5): 0.3239886062029652, (10, 5): 0.3131068864493691, (100, 5): 0.31159029061571586, (1000, 5): 0.3087493429250024, (10000, 5): 0.3215289243294188, (1, 10): 0.32553520850059964, (10, 10): 0.3116757417624611, (100, 10): 0.2918936189191376, (1000, 10): 0.2464890187566459, (10000, 10): 0.2421692931257926, (1, 15): 0.32459021683981537, (10, 15): 0.28548320020113005, (100, 15): 0.2747596063227062, (1000, 15): 0.21948511503283388, (10000, 15): 0.2222126752548292, (1, 20): 0.32006610819292336, (10, 20): 0.29937587317120185, (100, 20): 0.25934533051007114, (1000, 20): 0.19590690645589057, (10000, 20): 0.19555549366539973}



.. GENERATED FROM PYTHON SOURCE LINES 537-538

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 538-544

.. code-block:: default



    plot_metric(bench_lr, ylabel="classes",
                title="scikit-learn vs mlprodict (LogisticRegression) \n"
                "< 1 means mlprodict is faster\n no parallelisation")




.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_012.png
   :alt: scikit-learn vs mlprodict (LogisticRegression)  < 1 means mlprodict is faster  no parallelisation
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_012.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn vs mlprodict (LogisticRegression) \n< 1 means mlprodict is faster\n no parallelisation'}, xlabel='N', ylabel='classes'>



.. GENERATED FROM PYTHON SOURCE LINES 545-547

Decision Tree and number of features
++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 547-579

.. code-block:: default


    ModelToTest = DecisionTreeClassifier

    NF = [2, 10, 20, 40, 50, 70, 100, 200, 500, 1000]
    N = [1, 10, 100, 1000, 10000, 50000]
    trees_nf = {}

    for nf in tqdm(NF):
        ntest = 10000
        X, y = make_classification(
            n_samples=10000 + ntest, n_features=nf, n_informative=nf // 2 + 1,
            n_redundant=0, n_repeated=0,
            n_classes=2, random_state=11)
        X_train, X_test, y_train, y_test = (
            X[:-ntest], X[-ntest:], y[:-ntest], y[-ntest:])

        dt = ModelToTest(max_depth=7).fit(X_train, y_train)

        X32 = X_test.astype(numpy.float32)
        monnx = to_onnx(dt, X32[:1])
        oinf = OnnxInference(monnx)
        oinf.sequence_[0].ops_.rt_.omp_N_ = 1000000
        trees_nf[nf] = dict(model=dt, X_test=X_test,
                            X32=X32, monnx=monnx, oinf=oinf)


    bench_dt_nf = tree_benchmark(
        lambda nf: trees_nf[nf]['X32'],
        lambda nf: trees_nf[nf]['model'].predict_proba,
        lambda nf: (lambda x, c=nf: trees_nf[c]['oinf'].run({'X': x})), NF, N)
    bench_dt_nf





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/10 [00:00<?, ?it/s]     10%|#         | 1/10 [00:00<00:01,  7.86it/s]     20%|##        | 2/10 [00:00<00:01,  4.11it/s]     30%|###       | 3/10 [00:01<00:02,  2.54it/s]     40%|####      | 4/10 [00:02<00:03,  1.51it/s]     50%|#####     | 5/10 [00:03<00:04,  1.10it/s]     60%|######    | 6/10 [00:05<00:04,  1.22s/it]     70%|#######   | 7/10 [00:07<00:05,  1.68s/it]     80%|########  | 8/10 [00:13<00:05,  2.80s/it]     90%|######### | 9/10 [00:26<00:06,  6.07s/it]    100%|##########| 10/10 [00:56<00:00, 13.49s/it]    100%|##########| 10/10 [00:56<00:00,  5.65s/it]
      0%|          | 0/10 [00:00<?, ?it/s]     10%|#         | 1/10 [00:09<01:25,  9.50s/it]     20%|##        | 2/10 [00:19<01:16,  9.54s/it]     30%|###       | 3/10 [00:28<01:07,  9.66s/it]     40%|####      | 4/10 [00:38<00:58,  9.80s/it]     50%|#####     | 5/10 [00:48<00:49,  9.91s/it]     60%|######    | 6/10 [00:59<00:40, 10.03s/it]     70%|#######   | 7/10 [01:10<00:30, 10.27s/it]     80%|########  | 8/10 [01:22<00:22, 11.12s/it]     90%|######### | 9/10 [01:40<00:13, 13.21s/it]    100%|##########| 10/10 [02:02<00:00, 15.93s/it]    100%|##########| 10/10 [02:02<00:00, 12.28s/it]

    {(1, 2): 0.4640013643383116, (10, 2): 0.4521400198962513, (100, 2): 0.4920788861741094, (1000, 2): 0.6707522002106752, (10000, 2): 0.9114997454749157, (50000, 2): 0.9932033018909163, (1, 10): 0.46435427972598126, (10, 10): 0.4608513108077591, (100, 10): 0.4819988666590757, (1000, 10): 0.6644733056561918, (10000, 10): 0.8901142039873676, (50000, 10): 0.9728052918045471, (1, 20): 0.4632685683872123, (10, 20): 0.45968377908588315, (100, 20): 0.4881672411600742, (1000, 20): 0.6567230238323213, (10000, 20): 0.9018073377659173, (50000, 20): 0.9677591181152027, (1, 40): 0.46460210627348253, (10, 40): 0.4602617711061367, (100, 40): 0.48061304600154686, (1000, 40): 0.6409293554433383, (10000, 40): 0.9060193668601648, (50000, 40): 0.9728630503409673, (1, 50): 0.4627084851523794, (10, 50): 0.4593554868597305, (100, 50): 0.4789526138212457, (1000, 50): 0.6253934051373589, (10000, 50): 0.9077017994150829, (50000, 50): 0.9720325415554889, (1, 70): 0.4631847525778227, (10, 70): 0.4617276686858359, (100, 70): 0.48006349338488746, (1000, 70): 0.6152489404287362, (10000, 70): 0.9181728971268079, (50000, 70): 0.9850177782174152, (1, 100): 0.45494865359509723, (10, 100): 0.4568720380862202, (100, 100): 0.4873542509886396, (1000, 100): 0.5796491887578703, (10000, 100): 0.9194053763616127, (50000, 100): 0.9788076727518386, (1, 200): 0.4609348743186567, (10, 200): 0.4599930349859481, (100, 200): 0.4819602676933489, (1000, 200): 0.60336268997293, (10000, 200): 0.9349504423999264, (50000, 200): 0.9784432613540692, (1, 500): 0.46103835674863736, (10, 500): 0.4539126579761783, (100, 500): 0.4706712991865358, (1000, 500): 0.7750785740913202, (10000, 500): 1.0596651054412543, (50000, 500): 1.0989084938274973, (1, 1000): 0.4591427424849715, (10, 1000): 0.45609531806299747, (100, 1000): 0.46452001332139964, (1000, 1000): 0.820308676867606, (10000, 1000): 1.099673597244456, (50000, 1000): 1.14617776734304}



.. GENERATED FROM PYTHON SOURCE LINES 580-581

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 581-587

.. code-block:: default



    plot_metric(
        bench_dt_nf, ylabel="number of features", transpose=True, figsize=(10, 4),
        title="scikit-learn vs mlprodict (DecisionTreeClassifier) \n"
        "< 1 means mlprodict is faster\n no parallelisation")



.. image-sg:: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_013.png
   :alt: scikit-learn vs mlprodict (DecisionTreeClassifier)  < 1 means mlprodict is faster  no parallelisation
   :srcset: /gyexamples/images/sphx_glr_plot_onnx_tree_ensemble_parallel_013.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'scikit-learn vs mlprodict (DecisionTreeClassifier) \n< 1 means mlprodict is faster\n no parallelisation'}, xlabel='number of features', ylabel='N'>




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 23 minutes  17.617 seconds)


.. _sphx_glr_download_gyexamples_plot_onnx_tree_ensemble_parallel.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_onnx_tree_ensemble_parallel.py <plot_onnx_tree_ensemble_parallel.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_onnx_tree_ensemble_parallel.ipynb <plot_onnx_tree_ensemble_parallel.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
