
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gyexamples/plot_op_onnx_topk.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gyexamples_plot_op_onnx_topk.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gyexamples_plot_op_onnx_topk.py:


.. _onnxtopkrst:

TopK benchmark
==============

This example compares :epkg:`onnxruntime` and :epkg:`mlprodict`
for an implementation of operator `TopK
<https://github.com/onnx/onnx/blob/master/docs/Operators.md#TopK>`_.
We measure two runtimes by computing a ratio between their
time execution through the following kind of graphs.

.. contents::
    :local:

Graph to compare performance
++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 19-38

.. code-block:: default


    from numpy.random import randn
    import numpy
    import matplotlib.pyplot as plt
    from pandas import DataFrame
    from onnxruntime import InferenceSession, __version__ as ort_version
    from tqdm import tqdm
    from cpyquickhelper.numbers import measure_time
    from pyquickhelper.pycode.profiling import profile
    from skl2onnx.algebra.onnx_ops import OnnxTopK_11
    from skl2onnx.common.data_types import FloatTensorType
    from skl2onnx.algebra.onnx_ops import OnnxTopK
    from mlprodict.onnxrt.validate.validate_benchmark import benchmark_fct
    from mlprodict.onnxrt import OnnxInference
    from mlprodict.onnxrt.ops_cpu.op_topk import (
        topk_sorted_implementation, topk_sorted_implementation_cpp)
    from mlprodict import __version__ as mlp_version
    from mlprodict.plotting.plotting import plot_benchmark_metrics








.. GENERATED FROM PYTHON SOURCE LINES 39-40

Available optimisation on this machine.

.. GENERATED FROM PYTHON SOURCE LINES 40-44

.. code-block:: default


    from mlprodict.testing.experimental_c_impl.experimental_c import code_optimisation
    print(code_optimisation())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    AVX-omp=8




.. GENERATED FROM PYTHON SOURCE LINES 45-46

Graph.

.. GENERATED FROM PYTHON SOURCE LINES 46-64

.. code-block:: default



    def plot_metric(metric, ax=None, xlabel="N", ylabel="k", middle=1.,
                    transpose=False, shrink=1.0, title=None):
        ax, cbar = plot_benchmark_metrics(
            metric, ax=ax, xlabel=xlabel, ylabel=ylabel, middle=middle,
            transpose=transpose, cbar_kw={'shrink': shrink})
        if title is not None:
            ax.set_title(title)
        return ax


    data = {(1, 1): 0.1, (10, 1): 1, (1, 10): 2,
            (10, 10): 100, (100, 1): 100, (100, 10): 1000}

    fig, ax = plt.subplots(1, 2, figsize=(10, 4))
    plot_metric(data, ax[0], shrink=0.6)




.. image-sg:: /gyexamples/images/sphx_glr_plot_op_onnx_topk_001.png
   :alt: plot op onnx topk
   :srcset: /gyexamples/images/sphx_glr_plot_op_onnx_topk_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:xlabel='N', ylabel='k'>



.. GENERATED FROM PYTHON SOURCE LINES 66-70

.. code-block:: default


    plot_metric(data, ax[1], transpose=True)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:xlabel='k', ylabel='N'>



.. GENERATED FROM PYTHON SOURCE LINES 71-77

TopK in ONNX
++++++++++++

The following lines creates an ONNX graph using
one TopK ONNX node. The outcome is the ONNX graph
converted into json.

.. GENERATED FROM PYTHON SOURCE LINES 77-93

.. code-block:: default



    X32 = randn(100000, 100).astype(numpy.float32)

    node = OnnxTopK_11('X', numpy.array([5], dtype=numpy.int64),
                       output_names=['dist', 'ind'])

    model_onnx = node.to_onnx(
        [('X', X32)], target_opset=12,
        # shape inference does not seem to work in onnxruntime
        # so we speccify the output shape
        outputs=[('dist', X32[:1, :5]),
                 ('ind', X32[:1, :5].astype(numpy.int64))])
    model_onnx






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ir_version: 6
    producer_name: "skl2onnx"
    producer_version: "1.11"
    domain: "ai.onnx"
    model_version: 0
    graph {
      node {
        input: "X"
        input: "To_TopKcst"
        output: "dist"
        output: "ind"
        name: "To_TopK"
        op_type: "TopK"
        domain: ""
      }
      name: "OnnxTopK_11"
      initializer {
        dims: 1
        data_type: 7
        int64_data: 5
        name: "To_TopKcst"
      }
      input {
        name: "X"
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
              }
              dim {
                dim_value: 100
              }
            }
          }
        }
      }
      output {
        name: "dist"
        type {
          tensor_type {
            elem_type: 1
            shape {
              dim {
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
      output {
        name: "ind"
        type {
          tensor_type {
            elem_type: 7
            shape {
              dim {
              }
              dim {
                dim_value: 5
              }
            }
          }
        }
      }
    }
    opset_import {
      domain: ""
      version: 11
    }




.. GENERATED FROM PYTHON SOURCE LINES 94-95

That gives...

.. GENERATED FROM PYTHON SOURCE LINES 95-102

.. code-block:: default



    oinf = OnnxInference(model_onnx, runtime="python")
    res = oinf.run({'X': X32})
    dist, ind = res['dist'], res['ind']
    dist[:2], ind[:2]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (array([[1.9605817, 1.7998823, 1.7886105, 1.7392373, 1.661391 ],
           [2.5232842, 1.8653195, 1.4374154, 1.3630458, 1.3488386]],
          dtype=float32), array([[44, 76, 17, 69, 80],
           [90, 25, 62, 58,  6]]))



.. GENERATED FROM PYTHON SOURCE LINES 103-104

With onnxruntime.

.. GENERATED FROM PYTHON SOURCE LINES 104-111

.. code-block:: default



    sess = InferenceSession(model_onnx.SerializeToString())
    dist, ind = sess.run(None, {'X': X32})
    dist[:2], ind[:2]






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (array([[1.9605817, 1.7998823, 1.7886105, 1.7392373, 1.661391 ],
           [2.5232842, 1.8653195, 1.4374154, 1.3630458, 1.3488386]],
          dtype=float32), array([[44, 76, 17, 69, 80],
           [90, 25, 62, 58,  6]], dtype=int64))



.. GENERATED FROM PYTHON SOURCE LINES 112-113

Let's compare two implementations.

.. GENERATED FROM PYTHON SOURCE LINES 113-153

.. code-block:: default



    def benchmark(X, fct1, fct2, N, repeat=10, number=10):

        def ti(n):
            if n <= 1:
                return 50
            if n <= 1000:
                return 2
            if n <= 10000:
                return 0.51
            return 0.11

        # to warm up the engine
        time_kwargs = {n: dict(repeat=10, number=10) for n in N[:2]}
        benchmark_fct(fct1, X, time_kwargs=time_kwargs, skip_long_test=False)
        benchmark_fct(fct2, X, time_kwargs=time_kwargs, skip_long_test=False)
        # real measure
        time_kwargs = {n: dict(repeat=int(repeat * ti(n)),
                               number=int(number * ti(n))) for n in N}
        res1 = benchmark_fct(fct1, X, time_kwargs=time_kwargs,
                             skip_long_test=False)
        res2 = benchmark_fct(fct2, X, time_kwargs=time_kwargs,
                             skip_long_test=False)

        res = {}
        for r in sorted(res1):
            r1 = res1[r]
            r2 = res2[r]
            ratio = r2['ttime'] / r1['ttime']
            res[r] = ratio
        return res


    N = [1, 10, 100, 1000, 10000, 100000]
    res = benchmark(X32, lambda x: sess.run(None, {'X': x}),
                    lambda x: oinf.run({'X': x}), N=N)
    res






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {1: 1.427047083279366, 10: 1.379223083908301, 100: 0.618555110186143, 1000: 0.5470206413920607, 10000: 0.647900909748697, 100000: 0.6971810217148661}



.. GENERATED FROM PYTHON SOURCE LINES 154-165

The implementation in `mlprodict
<https://github.com/sdpython/mlprodict/blob/master/
mlprodict/onnxrt/ops_cpu/_op_onnx_numpy.cpp#L246>`_
is faster when the number of rows grows. It is faster
for 1 rows, for many rows, the implementation
uses openmp to parallelize.

C++ implementation vs numpy
+++++++++++++++++++++++++++

:epkg:`scikit-learn` uses :epkg:`numpy` to compute the top *k* elements.

.. GENERATED FROM PYTHON SOURCE LINES 165-172

.. code-block:: default



    res = benchmark(X32, lambda x: topk_sorted_implementation(x, 5, 1, 0),
                    lambda x: topk_sorted_implementation_cpp(x, 5, 1, 0), N=N)
    res






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {1: 0.2999311084900418, 10: 0.29572802118197, 100: 0.15971898092830578, 1000: 0.07174160952343876, 10000: 0.07951826069037438, 100000: 0.07669007274294}



.. GENERATED FROM PYTHON SOURCE LINES 173-174

It seems to be faster too. Let's profile.

.. GENERATED FROM PYTHON SOURCE LINES 174-181

.. code-block:: default



    xr = randn(1000000, 100)
    text = profile(lambda: topk_sorted_implementation(xr, 5, 1, 0),
                   pyinst_format='text')[1]
    print(text)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


      _     ._   __/__   _ _  _  _ _/_   Recorded: 09:32:16 AM Samples:  5
     /_//_/// /_\ / //_// / //_'/ //     Duration: 6.449     CPU time: 6.397
    /   _/                      v4.1.1

    Program: /var/lib/jenkins/workspace/mlprodict/mlprodict_UT_39_std/_doc/examples/plot_op_onnx_topk.py

    6.448 profile  ../pycode/profiling.py:457
    `- 6.448 <lambda>  plot_op_onnx_topk.py:177
          [15 frames hidden]  plot_op_onnx_topk, mlprodict, <__arra...
             5.153 ndarray.argpartition  <built-in>:0






.. GENERATED FROM PYTHON SOURCE LINES 182-187

Parallelisation
+++++++++++++++

We need to disable the parallelisation to
really compare both implementation.

.. GENERATED FROM PYTHON SOURCE LINES 187-215

.. code-block:: default


    # In[11]:


    def benchmark_test(X, fct1, fct2, N, K, repeat=10, number=10):
        res = {}
        for k in tqdm(K):
            def f1(x, k=k): return fct1(x, k=k)
            def f2(x, k=k): return fct2(x, k=k)
            r = benchmark(X32, f1, f2, N=N, repeat=repeat, number=number)
            for n, v in r.items():
                res[n, k] = v
        return res


    K = [1, 2, 5, 10, 15]
    N = [1, 2, 3, 10, 100, 1000, 10000]

    bench_para = benchmark_test(
        X32, (lambda x, k: topk_sorted_implementation_cpp(
            x, k=k, axis=1, largest=0, th_para=100000000)),
        (lambda x, k: topk_sorted_implementation_cpp(
            x, k=k, axis=1, largest=0, th_para=1)),
        N=N, K=K)

    bench_para






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:17<01:11, 17.83s/it]     40%|####      | 2/5 [00:35<00:54, 18.02s/it]     60%|######    | 3/5 [00:55<00:36, 18.49s/it]     80%|########  | 4/5 [01:15<00:19, 19.29s/it]    100%|##########| 5/5 [01:37<00:00, 20.13s/it]    100%|##########| 5/5 [01:37<00:00, 19.43s/it]

    {(1, 1): 0.9983938946403873, (2, 1): 1.2228341898498405, (3, 1): 1.2128530748481108, (10, 1): 1.151329519905578, (100, 1): 0.7615625169083935, (1000, 1): 0.4258336812554681, (10000, 1): 0.2948296279230108, (1, 2): 0.9963987449861827, (2, 2): 1.2093517665974964, (3, 2): 1.2188602403437225, (10, 2): 1.109932004801602, (100, 2): 0.5458353318293698, (1000, 2): 0.21611882100474408, (10000, 2): 0.22609961132487805, (1, 5): 1.0043536204261392, (2, 5): 1.219770365443342, (3, 5): 1.2485389484516864, (10, 5): 1.0258178072075352, (100, 5): 0.41654744261002574, (1000, 5): 0.19373213553698282, (10000, 5): 0.1954567786964541, (1, 10): 1.0037755308384526, (2, 10): 1.1820516987433964, (3, 10): 1.0792814411288245, (10, 10): 0.8449310649031962, (100, 10): 0.3014486630361006, (1000, 10): 0.16593477763147457, (10000, 10): 0.16955201848356116, (1, 15): 1.0002151411646811, (2, 15): 1.1651578021051592, (3, 15): 1.0632424283346245, (10, 15): 0.7406298359393169, (100, 15): 0.26439997934987347, (1000, 15): 0.16325616751447475, (10000, 15): 0.16001674688901196}



.. GENERATED FROM PYTHON SOURCE LINES 216-217

As a graph.

.. GENERATED FROM PYTHON SOURCE LINES 217-222

.. code-block:: default



    plot_metric(bench_para, transpose=False, title="TopK and parallelisation\n"
                "< 1 means parallelisation is faster", shrink=0.75)




.. image-sg:: /gyexamples/images/sphx_glr_plot_op_onnx_topk_002.png
   :alt: TopK and parallelisation < 1 means parallelisation is faster
   :srcset: /gyexamples/images/sphx_glr_plot_op_onnx_topk_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'TopK and parallelisation\n< 1 means parallelisation is faster'}, xlabel='N', ylabel='k'>



.. GENERATED FROM PYTHON SOURCE LINES 223-232

This is somehow expected. First column is closed to
1 as it is the same code being compared. Next columns
are red, meaning the parallelisation does not help,
the parallelisation helps for bigger N, as least more than 100.

Parallellisation with ONNX
++++++++++++++++++++++++++

We replicate the same experiment with an ONNX graph.

.. GENERATED FROM PYTHON SOURCE LINES 232-246

.. code-block:: default



    k_ = numpy.array([3], dtype=numpy.int64)
    node = OnnxTopK_11('X', 'k',
                       output_names=['dist', 'ind'])

    model_onnx = node.to_onnx(
        [('X', X32), ('k', k_)], target_opset=12,
        # shape inference does not seem to work in onnxruntime
        # so we speccify the output shape
        outputs=[('dist', X32[:1, :5]),
                 ('ind', X32[:1, :5].astype(numpy.int64))])









.. GENERATED FROM PYTHON SOURCE LINES 247-248

Test

.. GENERATED FROM PYTHON SOURCE LINES 248-256

.. code-block:: default



    oinf_no_para = OnnxInference(model_onnx, runtime="python")
    res = oinf_no_para.run({'X': X32, 'k': k_})
    dist, ind = res['dist'], res['ind']
    dist[:2], ind[:2]






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (array([[1.9605817, 1.7998823, 1.7886105],
           [2.5232842, 1.8653195, 1.4374154]], dtype=float32), array([[44, 76, 17],
           [90, 25, 62]]))



.. GENERATED FROM PYTHON SOURCE LINES 257-258

Let's play with the thresholds triggering the parallelisation.

.. GENERATED FROM PYTHON SOURCE LINES 258-264

.. code-block:: default


    oinf_para = OnnxInference(model_onnx, runtime="python")
    oinf_no_para.sequence_[0].ops_.th_para = 100000000
    oinf_para.sequence_[0].ops_.th_para = 1









.. GENERATED FROM PYTHON SOURCE LINES 265-266

Results.

.. GENERATED FROM PYTHON SOURCE LINES 266-277

.. code-block:: default



    bench_onnx_para = benchmark_test(
        X32, (lambda x, k: oinf_no_para.run(
            {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),
        (lambda x, k: oinf_para.run(
            {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),
        N=N, K=K)
    bench_onnx_para






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:55<03:41, 55.38s/it]     40%|####      | 2/5 [01:52<02:48, 56.24s/it]     60%|######    | 3/5 [02:49<01:53, 56.64s/it]     80%|########  | 4/5 [03:48<00:57, 57.69s/it]    100%|##########| 5/5 [04:49<00:00, 58.90s/it]    100%|##########| 5/5 [04:49<00:00, 57.94s/it]

    {(1, 1): 0.9921322719642409, (2, 1): 1.0687037882586183, (3, 1): 1.0676351547172755, (10, 1): 1.0416819615231325, (100, 1): 0.9242747416650392, (1000, 1): 0.41296914540867374, (10000, 1): 0.3297305173465462, (1, 2): 1.001012813233277, (2, 2): 1.0598574635159204, (3, 2): 1.423638281745347, (10, 2): 1.0414817074829197, (100, 2): 0.7532546663255459, (1000, 2): 0.31377727135122374, (10000, 2): 0.2545427007074975, (1, 5): 1.0157595495420473, (2, 5): 1.038443447443911, (3, 5): 1.0558862028965048, (10, 5): 0.996125627904745, (100, 5): 0.5791350289191828, (1000, 5): 0.25523169391861494, (10000, 5): 0.21446637736926996, (1, 10): 1.0002297254910923, (2, 10): 1.0432940683971337, (3, 10): 0.9742354267302125, (10, 10): 0.8763146966598924, (100, 10): 0.440128245755049, (1000, 10): 0.21288223959870267, (10000, 10): 0.18205062821906287, (1, 15): 1.0032868436453437, (2, 15): 1.0327906406075296, (3, 15): 1.0186239883487176, (10, 15): 0.8702656403042017, (100, 15): 0.38179984665044536, (1000, 15): 0.20282921308329924, (10000, 15): 0.1731738129343039}



.. GENERATED FROM PYTHON SOURCE LINES 278-279

As a graph.

.. GENERATED FROM PYTHON SOURCE LINES 279-285

.. code-block:: default



    plot_metric(bench_onnx_para, transpose=False,
                title="TopK and parallelisation with ONNX\n< 1 means "
                "parallelisation is faster", shrink=0.75)




.. image-sg:: /gyexamples/images/sphx_glr_plot_op_onnx_topk_003.png
   :alt: TopK and parallelisation with ONNX < 1 means parallelisation is faster
   :srcset: /gyexamples/images/sphx_glr_plot_op_onnx_topk_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'TopK and parallelisation with ONNX\n< 1 means parallelisation is faster'}, xlabel='N', ylabel='k'>



.. GENERATED FROM PYTHON SOURCE LINES 286-290

Pretty much the same results.

onnxruntime vs mlprodict (no parallelisation)
+++++++++++++++++++++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 290-302

.. code-block:: default


    sess = InferenceSession(model_onnx.SerializeToString())


    bench_ort = benchmark_test(
        X32, (lambda x, k: sess.run(
            None, {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),
        (lambda x, k: oinf_no_para.run(
            {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),
        N=N, K=K)
    bench_ort





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:49<03:17, 49.31s/it]     40%|####      | 2/5 [01:39<02:28, 49.61s/it]     60%|######    | 3/5 [02:31<01:41, 50.70s/it]     80%|########  | 4/5 [03:24<00:51, 51.58s/it]    100%|##########| 5/5 [04:19<00:00, 53.05s/it]    100%|##########| 5/5 [04:19<00:00, 51.94s/it]

    {(1, 1): 1.2975345228598396, (2, 1): 1.3474804165055412, (3, 1): 1.3382848582924045, (10, 1): 1.3254787060563182, (100, 1): 1.1709201047745814, (1000, 1): 0.8699299375009901, (10000, 1): 2.9226354455140062, (1, 2): 1.2871294066610695, (2, 2): 1.333707952785655, (3, 2): 1.3289550131706127, (10, 2): 1.307641224892377, (100, 2): 1.1563412808663085, (1000, 2): 0.9614777973714757, (10000, 2): 3.4202055906372433, (1, 5): 1.3142324520855317, (2, 5): 1.3125112827505536, (3, 5): 1.312510765386396, (10, 5): 1.2647958214397124, (100, 5): 1.0700269823323025, (1000, 5): 2.4216892664922742, (10000, 5): 3.3570268993789014, (1, 10): 1.2765866056978905, (2, 10): 1.3071154062475645, (3, 10): 1.2939416173974825, (10, 10): 1.2252151781620255, (100, 10): 1.0007222053438924, (1000, 10): 3.130965019390217, (10000, 10): 3.3167528261326646, (1, 15): 1.3075954741155846, (2, 15): 1.3149769928189214, (3, 15): 1.2937248240362569, (10, 15): 1.204703172310864, (100, 15): 0.9680849753400181, (1000, 15): 3.078443683908136, (10000, 15): 3.2179231952015552}



.. GENERATED FROM PYTHON SOURCE LINES 303-304

As a graph.

.. GENERATED FROM PYTHON SOURCE LINES 304-309

.. code-block:: default


    plot_metric(bench_ort, transpose=False,
                title="TopK, onnxruntime vs mlprodict\n< 1 means mlprodict "
                "is faster\nno parallelisation", shrink=0.75)




.. image-sg:: /gyexamples/images/sphx_glr_plot_op_onnx_topk_004.png
   :alt: TopK, onnxruntime vs mlprodict < 1 means mlprodict is faster no parallelisation
   :srcset: /gyexamples/images/sphx_glr_plot_op_onnx_topk_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'TopK, onnxruntime vs mlprodict\n< 1 means mlprodict is faster\nno parallelisation'}, xlabel='N', ylabel='k'>



.. GENERATED FROM PYTHON SOURCE LINES 310-314

It seems the implementation of operator TopK in
onnxruntime 1.1.1 can be improved.

Versions:

.. GENERATED FROM PYTHON SOURCE LINES 314-316

.. code-block:: default

    ort_version, mlp_version





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ('1.10.91', '0.8.1747')



.. GENERATED FROM PYTHON SOURCE LINES 317-318

And with parallelisation above 50 rows.

.. GENERATED FROM PYTHON SOURCE LINES 318-329

.. code-block:: default


    oinf_para.sequence_[0].ops_.th_para = 50
    bench_ort_para = benchmark_test(
        X32, (lambda x, k: sess.run(
            None, {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),
        (lambda x, k: oinf_para.run(
            {'X': x, 'k': numpy.array([k], dtype=numpy.int64)})),
        N=N, K=K)
    bench_ort_para






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0/5 [00:00<?, ?it/s]     20%|##        | 1/5 [00:49<03:18, 49.61s/it]     40%|####      | 2/5 [01:39<02:29, 49.85s/it]     60%|######    | 3/5 [02:30<01:40, 50.26s/it]     80%|########  | 4/5 [03:21<00:50, 50.73s/it]    100%|##########| 5/5 [04:14<00:00, 51.39s/it]    100%|##########| 5/5 [04:14<00:00, 50.88s/it]

    {(1, 1): 1.2952951907034427, (2, 1): 1.3309428333483104, (3, 1): 1.3188609472296797, (10, 1): 1.3035785377703304, (100, 1): 1.0468099324066384, (1000, 1): 0.3601991786034892, (10000, 1): 0.9545689524567582, (1, 2): 1.3045849573326034, (2, 2): 1.3234588309863522, (3, 2): 1.3227452926172825, (10, 2): 1.2949538334290824, (100, 2): 0.8494251021064336, (1000, 2): 0.3028632999876258, (10000, 2): 0.8805834633731231, (1, 5): 1.287918478634543, (2, 5): 1.3099633361807401, (3, 5): 1.3009617154830515, (10, 5): 1.253933204481738, (100, 5): 0.6263503607410738, (1000, 5): 0.6755102563865992, (10000, 5): 0.7168664477205556, (1, 10): 1.2640207317564252, (2, 10): 1.2289833936702113, (3, 10): 1.2235908943912461, (10, 10): 1.1619090075203258, (100, 10): 0.4378843468732403, (1000, 10): 0.6689125007531994, (10000, 10): 0.601810664640278, (1, 15): 1.263769704777327, (2, 15): 1.2248521224158424, (3, 15): 1.2106312159061359, (10, 15): 1.1303608958484737, (100, 15): 0.36051544574185057, (1000, 15): 0.6488979691147632, (10000, 15): 0.5543214356079255}



.. GENERATED FROM PYTHON SOURCE LINES 330-331

As a graph.

.. GENERATED FROM PYTHON SOURCE LINES 331-337

.. code-block:: default



    plot_metric(bench_ort_para, transpose=False,
                title="TopK, onnxruntime vs mlprodict\n< 1 means mlprodict "
                "is faster\nparallelisation above 50 rows", shrink=0.75)




.. image-sg:: /gyexamples/images/sphx_glr_plot_op_onnx_topk_005.png
   :alt: TopK, onnxruntime vs mlprodict < 1 means mlprodict is faster parallelisation above 50 rows
   :srcset: /gyexamples/images/sphx_glr_plot_op_onnx_topk_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <AxesSubplot:title={'center':'TopK, onnxruntime vs mlprodict\n< 1 means mlprodict is faster\nparallelisation above 50 rows'}, xlabel='N', ylabel='k'>



.. GENERATED FROM PYTHON SOURCE LINES 338-348

onnxruntime and mlprodict implement the same algorithm.
 The only difference comes from the threshold which
 trigger the parallelisation. It should depend on the machine.
 That explains the difference in time for 100 observations.

#############################
 Interesting...

 Comparison with onnxruntime
 +++++++++++++++++++++++++++

.. GENERATED FROM PYTHON SOURCE LINES 348-367

.. code-block:: default



    X = numpy.array([
        [0, 1, 2, 3],
        [4, 5, 6, 7],
        [8, 9, 10, 11],
    ], dtype=numpy.float32)

    K = numpy.array([3], dtype=numpy.int64)


    node = OnnxTopK('X', K, output_names=['values', 'indices'],
                    op_version=12)
    onx = node.to_onnx([('X', FloatTensorType())])

    py_topk = OnnxInference(onx, runtime="python_compiled")
    ort_topk = InferenceSession(onx.SerializeToString())









.. GENERATED FROM PYTHON SOURCE LINES 368-369

Check the outputs.

.. GENERATED FROM PYTHON SOURCE LINES 369-375

.. code-block:: default



    r1 = py_topk.run({'X': X})
    r1






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {'values': array([[ 3.,  2.,  1.],
           [ 7.,  6.,  5.],
           [11., 10.,  9.]], dtype=float32), 'indices': array([[3, 2, 1],
           [3, 2, 1],
           [3, 2, 1]])}



.. GENERATED FROM PYTHON SOURCE LINES 377-382

.. code-block:: default


    r2 = ort_topk.run(None, {'X': X})
    r2






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    [array([[ 3.,  2.,  1.],
           [ 7.,  6.,  5.],
           [11., 10.,  9.]], dtype=float32), array([[3, 2, 1],
           [3, 2, 1],
           [3, 2, 1]], dtype=int64)]



.. GENERATED FROM PYTHON SOURCE LINES 383-384

Some figures.

.. GENERATED FROM PYTHON SOURCE LINES 384-391

.. code-block:: default


    bs = []
    bs.append(measure_time(lambda: py_topk.run({'X': X}),
                           context=globals(), div_by_number=True))
    bs[-1]['c'] = 'py'
    bs[-1]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {'average': 6.341060437262058e-05, 'deviation': 5.274479418653327e-07, 'min_exec': 6.289154291152954e-05, 'max_exec': 6.482752040028572e-05, 'repeat': 10, 'number': 50, 'ttime': 0.0006341060437262059, 'context_size': 2272, 'c': 'py'}



.. GENERATED FROM PYTHON SOURCE LINES 393-399

.. code-block:: default


    bs.append(measure_time(lambda: ort_topk.run(None, {'X': X}),
                           context=globals(), div_by_number=True))
    bs[-1]['c'] = 'or'
    bs[-1]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {'average': 6.353356502950191e-05, 'deviation': 8.17751028208646e-07, 'min_exec': 6.30539283156395e-05, 'max_exec': 6.595490500330924e-05, 'repeat': 10, 'number': 50, 'ttime': 0.0006353356502950191, 'context_size': 2272, 'c': 'or'}



.. GENERATED FROM PYTHON SOURCE LINES 401-411

.. code-block:: default


    X = numpy.random.randn(10000, 100).astype(numpy.float32)


    bs.append(measure_time(lambda: py_topk.run({'X': X}),
                           context=globals(), div_by_number=True))
    bs[-1]['c'] = 'py-100'
    bs[-1]






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {'average': 0.0021524884402751923, 'deviation': 0.00015494265857053126, 'min_exec': 0.0020357331074774263, 'max_exec': 0.002487112432718277, 'repeat': 10, 'number': 50, 'ttime': 0.021524884402751923, 'context_size': 2272, 'c': 'py-100'}



.. GENERATED FROM PYTHON SOURCE LINES 413-420

.. code-block:: default



    bs.append(measure_time(lambda: ort_topk.run(None, {'X': X}),
                           context=globals(), div_by_number=True))
    bs[-1]['c'] = 'ort-100'
    bs[-1]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    {'average': 0.0025841014049947262, 'deviation': 2.1821455108537475e-05, 'min_exec': 0.002571027372032404, 'max_exec': 0.002648614775389433, 'repeat': 10, 'number': 50, 'ttime': 0.02584101404994726, 'context_size': 2272, 'c': 'ort-100'}



.. GENERATED FROM PYTHON SOURCE LINES 422-425

.. code-block:: default


    df = DataFrame(bs)
    df





.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>average</th>
          <th>deviation</th>
          <th>min_exec</th>
          <th>max_exec</th>
          <th>repeat</th>
          <th>number</th>
          <th>ttime</th>
          <th>context_size</th>
          <th>c</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.000063</td>
          <td>5.274479e-07</td>
          <td>0.000063</td>
          <td>0.000065</td>
          <td>10</td>
          <td>50</td>
          <td>0.000634</td>
          <td>2272</td>
          <td>py</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.000064</td>
          <td>8.177510e-07</td>
          <td>0.000063</td>
          <td>0.000066</td>
          <td>10</td>
          <td>50</td>
          <td>0.000635</td>
          <td>2272</td>
          <td>or</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.002152</td>
          <td>1.549427e-04</td>
          <td>0.002036</td>
          <td>0.002487</td>
          <td>10</td>
          <td>50</td>
          <td>0.021525</td>
          <td>2272</td>
          <td>py-100</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.002584</td>
          <td>2.182146e-05</td>
          <td>0.002571</td>
          <td>0.002649</td>
          <td>10</td>
          <td>50</td>
          <td>0.025841</td>
          <td>2272</td>
          <td>ort-100</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 16 minutes  54.965 seconds)


.. _sphx_glr_download_gyexamples_plot_op_onnx_topk.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_op_onnx_topk.py <plot_op_onnx_topk.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_op_onnx_topk.ipynb <plot_op_onnx_topk.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
