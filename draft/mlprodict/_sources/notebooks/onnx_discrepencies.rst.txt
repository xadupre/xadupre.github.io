
.. _onnxdiscrepenciesrst:

=======================
Discrepencies with ONNX
=======================


.. only:: html

    **Links:** :download:`notebook <onnx_discrepencies.ipynb>`, :downloadlink:`html <onnx_discrepencies2html.html>`, :download:`PDF <onnx_discrepencies.pdf>`, :download:`python <onnx_discrepencies.py>`, :downloadlink:`slides <onnx_discrepencies.slides.html>`, :githublink:`GitHub|_doc/notebooks/onnx_discrepencies.ipynb|*`


The notebook shows one example where the conversion leads with
discrepencies if default options are used. It converts a pipeline with
two steps, a scaler followed by a tree.

The bug this notebook is tracking does not always appear, it has a
better chance to happen with integer features but that’s not always the
case. The notebook must be run again in that case.

.. code:: ipython3

    from jyquickhelper import add_notebook_menu
    add_notebook_menu()






.. contents::
    :local:





.. code:: ipython3

    %matplotlib inline

Data and first model
--------------------

We take a random datasets with mostly integers.

.. code:: ipython3

    import math
    import numpy
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split
    
    X, y = make_regression(10000, 10)
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    
    Xi_train, yi_train = X_train.copy(), y_train.copy()
    Xi_test, yi_test = X_test.copy(), y_test.copy()
    for i in range(X.shape[1]):
        Xi_train[:, i] = (Xi_train[:, i] * math.pi * 2 ** i).astype(numpy.int64)
        Xi_test[:, i] = (Xi_test[:, i] * math.pi * 2 ** i).astype(numpy.int64)

.. code:: ipython3

    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.tree import DecisionTreeRegressor
    
    max_depth = 10
    
    model = Pipeline([
        ('scaler', StandardScaler()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    
    model.fit(Xi_train, yi_train)




.. parsed-literal::
    Pipeline(steps=[('scaler', StandardScaler()),
                    ('dt', DecisionTreeRegressor(max_depth=10))])



.. code:: ipython3

    model.predict(Xi_test[:5])




.. parsed-literal::
    array([-283.03708629,  263.17931397, -160.34784206, -126.59514441,
           -150.1963714 ])



Other models:

.. code:: ipython3

    model2 = Pipeline([
        ('scaler', StandardScaler()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    model3 = Pipeline([
        ('scaler', StandardScaler()),
        ('dt', DecisionTreeRegressor(max_depth=3))
    ])
    
    
    models = [
        ('bug', Xi_test.astype(numpy.float32), model),
        ('no scaler', Xi_test.astype(numpy.float32), 
         DecisionTreeRegressor(max_depth=max_depth).fit(Xi_train, yi_train)),
        ('float', X_test.astype(numpy.float32),
         model2.fit(X_train, y_train)),
        ('max_depth=3', X_test.astype(numpy.float32),
         model3.fit(X_train, y_train))
    ]

Conversion to ONNX
------------------

.. code:: ipython3

    import numpy
    from mlprodict.onnx_conv import to_onnx
    
    onx = to_onnx(model, X_train[:1].astype(numpy.float32))

.. code:: ipython3

    from mlprodict.onnxrt import OnnxInference
    
    oinfpy = OnnxInference(onx, runtime="python_compiled")
    print(oinfpy)


.. parsed-literal::
    OnnxInference(...)
        def compiled_run(dict_inputs):
            # inputs
            X = dict_inputs['X']
            (variable1, ) = n0_scaler(X)
            (variable, ) = n1_treeensembleregressor(variable1)
            return {
                'variable': variable,
            }


.. code:: ipython3

    import pandas
    
    X32 = Xi_test.astype(numpy.float32)
    y_skl = model.predict(X32)
    
    obs = [dict(runtime='sklearn', diff=0)]
    for runtime in ['python', 'python_compiled', 'onnxruntime1']:
        oinf = OnnxInference(onx, runtime=runtime)
        y_onx = oinf.run({'X': X32})['variable']
        delta = numpy.abs(y_skl - y_onx.ravel())
        am = delta.argmax()
        obs.append(dict(runtime=runtime, diff=delta.max()))
        obs[-1]['v[%d]' % am] = y_onx.ravel()[am]
        obs[0]['v[%d]' % am] = y_skl.ravel()[am]
    
    pandas.DataFrame(obs)






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>v[1583]</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>-439.590635</td>
        </tr>
        <tr>
          <th>1</th>
          <td>python</td>
          <td>133.641599</td>
          <td>-305.949036</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python_compiled</td>
          <td>133.641599</td>
          <td>-305.949036</td>
        </tr>
        <tr>
          <th>3</th>
          <td>onnxruntime1</td>
          <td>133.641599</td>
          <td>-305.949036</td>
        </tr>
      </tbody>
    </table>
    </div>



The pipeline shows huge discrepencies. They appear for a pipeline
*StandardScaler* + *DecisionTreeRegressor* applied in integer features.
They disappear if floats are used, or if the scaler is removed. The bug
also disappear if the tree is not big enough (max_depth=4 instread of
5).

.. code:: ipython3

    obs = [dict(runtime='sklearn', diff=0, name='sklearn')]
    for name, x32, mod in models:
        for runtime in ['python', 'python_compiled', 'onnxruntime1']:
            lonx = to_onnx(mod, x32[:1])
            loinf = OnnxInference(lonx, runtime=runtime)
            y_skl = mod.predict(X32)
            y_onx = loinf.run({'X': X32})['variable']
            delta = numpy.abs(y_skl - y_onx.ravel())
            am = delta.argmax()
            obs.append(dict(runtime=runtime, diff=delta.max(), name=name))
            obs[-1]['v[%d]' % am] = y_onx.ravel()[am]
            obs[0]['v[%d]' % am] = y_skl.ravel()[am]
    
    df = pandas.DataFrame(obs)
    df






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>name</th>
          <th>v[1583]</th>
          <th>v[1109]</th>
          <th>v[19]</th>
          <th>v[4]</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>sklearn</td>
          <td>-439.590635</td>
          <td>516.084502</td>
          <td>-549.753386</td>
          <td>-97.726497</td>
        </tr>
        <tr>
          <th>1</th>
          <td>python</td>
          <td>133.641599</td>
          <td>bug</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python_compiled</td>
          <td>133.641599</td>
          <td>bug</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>onnxruntime1</td>
          <td>133.641599</td>
          <td>bug</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>python</td>
          <td>0.000029</td>
          <td>no scaler</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>5</th>
          <td>python_compiled</td>
          <td>0.000029</td>
          <td>no scaler</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>6</th>
          <td>onnxruntime1</td>
          <td>0.000029</td>
          <td>no scaler</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>7</th>
          <td>python</td>
          <td>0.000029</td>
          <td>float</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-549.753357</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>8</th>
          <td>python_compiled</td>
          <td>0.000029</td>
          <td>float</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-549.753357</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>9</th>
          <td>onnxruntime1</td>
          <td>0.000029</td>
          <td>float</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-549.753357</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>10</th>
          <td>python</td>
          <td>0.000003</td>
          <td>max_depth=3</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-97.726494</td>
        </tr>
        <tr>
          <th>11</th>
          <td>python_compiled</td>
          <td>0.000003</td>
          <td>max_depth=3</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-97.726494</td>
        </tr>
        <tr>
          <th>12</th>
          <td>onnxruntime1</td>
          <td>0.000003</td>
          <td>max_depth=3</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-97.726494</td>
        </tr>
      </tbody>
    </table>
    </div>



.. code:: ipython3

    df.pivot("runtime", "name", "diff")






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th>name</th>
          <th>bug</th>
          <th>float</th>
          <th>max_depth=3</th>
          <th>no scaler</th>
          <th>sklearn</th>
        </tr>
        <tr>
          <th>runtime</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>onnxruntime1</th>
          <td>133.641599</td>
          <td>0.000029</td>
          <td>0.000003</td>
          <td>0.000029</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>python</th>
          <td>133.641599</td>
          <td>0.000029</td>
          <td>0.000003</td>
          <td>0.000029</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>python_compiled</th>
          <td>133.641599</td>
          <td>0.000029</td>
          <td>0.000003</td>
          <td>0.000029</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>sklearn</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>0.0</td>
        </tr>
      </tbody>
    </table>
    </div>



Other way to convert
--------------------

ONNX does not support double for TreeEnsembleRegressor but that a new
operator TreeEnsembleRegressorDouble was implemented into *mlprodict*.
We need to update the conversion.

.. code:: ipython3

    %load_ext mlprodict

.. code:: ipython3

    onx32 = to_onnx(model, X_train[:1].astype(numpy.float32))
    onx64 = to_onnx(model, X_train[:1].astype(numpy.float64), 
                    rewrite_ops=True)
    %onnxview onx64






.. raw:: html

    <div id="M365443388f39454e88273854e65f7098-cont"><div id="M365443388f39454e88273854e65f7098" style="width:100%;height:100%;"></div></div>
    <script>

    require(['http://www.xavierdupre.fr/js/vizjs/viz.js'], function() { var svgGraph = Viz("digraph{\n  nodesep=0.05;\n  ranksep=0.25;\n  orientation=portrait;\n\n  X [shape=box color=red label=\"X\ndouble((0, 10))\" fontsize=10];\n\n  variable [shape=box color=green label=\"variable\ndouble((0, 1))\" fontsize=10];\n\n\n  variable1 [shape=box label=\"variable1\" fontsize=10];\n  Scaler [shape=box style=\"filled,rounded\" color=orange label=\"Scaler\n(Scaler)\noffset=[ 2.66666667e-03  5.8666...\nscale=[0.35824136 0.16805995 0....\" fontsize=10];\n  X -> Scaler;\n  Scaler -> variable1;\n\n  TreeEnsembleRegressorDouble [shape=box style=\"filled,rounded\" color=orange label=\"TreeEnsembleRegressorDouble\n(TreeEnsembleRegressorDouble)\nn_targets=1\nnodes_falsenodeids=[ 930  461  ...\nnodes_featureids=[7 8 3 ... 5 0...\nnodes_hitrates=[1. 1. 1. ... 1....\nnodes_missing_value_tracks_true=[0 0 0 ......\nnodes_modes=[b'BRANCH_LEQ' b'BR...\nnodes_nodeids=[   0    1    2 ....\nnodes_treeids=[0 0 0 ... 0 0 0]\nnodes_truenodeids=[   1    2   ...\nnodes_values=[-0.26122029  0.08...\npost_transform=b'NONE'\ntarget_ids=[0 0 0 0 0 0 0 0 0 0...\ntarget_nodeids=[   9   11   12 ...\ntarget_treeids=[0 0 0 0 0 0 0 0...\ntarget_weights=[-6.14043756e+02...\" fontsize=10];\n  variable1 -> TreeEnsembleRegressorDouble;\n  TreeEnsembleRegressorDouble -> variable;\n}");
    document.getElementById('M365443388f39454e88273854e65f7098').innerHTML = svgGraph; });

    </script>



.. code:: ipython3

    X32 = Xi_test.astype(numpy.float32)
    X64 = Xi_test.astype(numpy.float64)
    
    obs = [dict(runtime='sklearn', diff=0)]
    for runtime in ['python', 'python_compiled', 'onnxruntime1']:
        for name, onx, xr in [('float', onx32, X32), ('double', onx64, X64)]:
            try:
                oinf = OnnxInference(onx, runtime=runtime)
            except Exception as e:
                obs.append(dict(runtime=runtime, error=str(e), real=name))
                continue
            y_skl = model.predict(xr)
            y_onx = oinf.run({'X': xr})['variable']
            delta = numpy.abs(y_skl - y_onx.ravel())
            am = delta.argmax()
            obs.append(dict(runtime=runtime, diff=delta.max(), real=name))
            obs[-1]['v[%d]' % am] = y_onx.ravel()[am]
            obs[0]['v[%d]' % am] = y_skl.ravel()[am]
    
    pandas.DataFrame(obs)






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>v[1583]</th>
          <th>v[0]</th>
          <th>real</th>
          <th>error</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>-439.590635</td>
          <td>-283.037086</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>python</td>
          <td>133.641599</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>float</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python</td>
          <td>0.000000</td>
          <td>NaN</td>
          <td>-283.037086</td>
          <td>double</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>python_compiled</td>
          <td>133.641599</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>float</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>python_compiled</td>
          <td>0.000000</td>
          <td>NaN</td>
          <td>-283.037086</td>
          <td>double</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>5</th>
          <td>onnxruntime1</td>
          <td>133.641599</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>float</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>6</th>
          <td>onnxruntime1</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>double</td>
          <td>Unable to create InferenceSession due to '[ONN...</td>
        </tr>
      </tbody>
    </table>
    </div>



We see that the use of double removes the discrepencies.

OnnxPipeline
------------

Another way to reduce the number of discrepencies is to use a pipeline
which converts every steps into ONNX before training the next one. That
way, every steps is either trained on the inputs, either trained on the
outputs produced by ONNX. Let’s see how it works.

.. code:: ipython3

    from mlprodict.sklapi import OnnxPipeline
    
    model_onx = OnnxPipeline([
        ('scaler', StandardScaler()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    model_onx.fit(Xi_train, yi_train)


.. parsed-literal::
    C:\xavierdupre\__home_\github_fork\scikit-learn\sklearn\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.
      FutureWarning)




.. parsed-literal::

    OnnxPipeline(steps=[('scaler',
                         OnnxTransformer(onnx_bytes=b'\x08\x06\x12\x08skl2onnx\x1a\x081.7.1076"\x07ai.onnx(\x002\x00:\xf6\x01\n\xa6\x01\n\x01X\x12\x08variable\x1a\x06Scaler"\x06Scaler*=\n\x06offset=>\xc3.;=+=\xc0;=|\xf2\xb0<=\xcd`\xf9>=\x89\xad3\xbd=RL\xab\xbf=V\xc4V\xbe=6<\x9d\xc0=B>\xa0@=\xbb\x93\xea@\xa0\x01\x06*<\n\x05scale=ik\xb7>=\xe8\x17,>=)\xb5\xa9==\xa7\xd5#==Q\x9e\xa1<=\xf5)$<=\x90<\xa2;=(D%;=a\xa8\xa1:= \x9f$:\xa0\x01\x06:\nai.onnx.ml\x12\x1emlprodict_ONNX(StandardScaler)Z\x11\n\x01X\x12\x0c\n\n\x08\x01\x12\x06\n\x00\n\x02\x08\nb\x18\n\x08variable\x12\x0c\n\n\x08\x01\x12\x06\n\x00\n\x02\x08\nB\x0e\n\nai.onnx.ml\x10\x01')),
                        ('dt', DecisionTreeRegressor(max_depth=10))])



We see that the first steps was replaced by an object *OnnxTransformer*
which wraps an ONNX file into a transformer following the *scikit-learn*
API. The initial steps are still available.

.. code:: ipython3

    model_onx.raw_steps_




.. parsed-literal::
    [('scaler', StandardScaler()), ('dt', DecisionTreeRegressor(max_depth=10))]



.. code:: ipython3

    models = [
        ('bug', Xi_test.astype(numpy.float32), model),
        ('OnnxPipeline', Xi_test.astype(numpy.float32), model_onx),
    ]

.. code:: ipython3

    obs = [dict(runtime='sklearn', diff=0, name='sklearn')]
    for name, x32, mod in models:
        for runtime in ['python', 'python_compiled', 'onnxruntime1']:
            lonx = to_onnx(mod, x32[:1])
            loinf = OnnxInference(lonx, runtime=runtime)
            y_skl = model_onx.predict(X32)  # model_onx is the new baseline
            y_onx = loinf.run({'X': X32})['variable']
            delta = numpy.abs(y_skl - y_onx.ravel())
            am = delta.argmax()
            obs.append(dict(runtime=runtime, diff=delta.max(), name=name))
            obs[-1]['v[%d]' % am] = y_onx.ravel()[am]
            obs[0]['v[%d]' % am] = y_skl.ravel()[am]
    
    df = pandas.DataFrame(obs)
    df






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>name</th>
          <th>v[2276]</th>
          <th>v[1109]</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>sklearn</td>
          <td>272.784708</td>
          <td>516.084502</td>
        </tr>
        <tr>
          <th>1</th>
          <td>python</td>
          <td>234.930666</td>
          <td>bug</td>
          <td>37.854042</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python_compiled</td>
          <td>234.930666</td>
          <td>bug</td>
          <td>37.854042</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>onnxruntime1</td>
          <td>234.930666</td>
          <td>bug</td>
          <td>37.854042</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>python</td>
          <td>0.000029</td>
          <td>OnnxPipeline</td>
          <td>NaN</td>
          <td>516.084473</td>
        </tr>
        <tr>
          <th>5</th>
          <td>python_compiled</td>
          <td>0.000029</td>
          <td>OnnxPipeline</td>
          <td>NaN</td>
          <td>516.084473</td>
        </tr>
        <tr>
          <th>6</th>
          <td>onnxruntime1</td>
          <td>0.000029</td>
          <td>OnnxPipeline</td>
          <td>NaN</td>
          <td>516.084473</td>
        </tr>
      </tbody>
    </table>
    </div>



Training the next steps based on ONNX outputs is better. This is not
completely satisfactory… Let’s check the accuracy.

.. code:: ipython3

    model.score(Xi_test, yi_test), model_onx.score(Xi_test, yi_test)




.. parsed-literal::
    (0.6492778377907853, 0.6536515451871481)



Pretty close.

Final explanation: StandardScalerFloat
--------------------------------------

We proposed two ways to have an ONNX pipeline which produces the same
prediction as *scikit-learn*. Let’s now replace the StandardScaler by a
new one which outputs float and not double. It turns out that class
*StandardScaler* computes ``X /= self.scale_`` but ONNX does
``X *= self.scale_inv_``. We need to implement this exact same operator
with float32 to remove all discrepencies.

.. code:: ipython3

    class StandardScalerFloat(StandardScaler):
        
        def __init__(self, with_mean=True, with_std=True):
            StandardScaler.__init__(self, with_mean=with_mean, with_std=with_std)
        
        def fit(self, X, y=None):
            StandardScaler.fit(self, X, y)
            if self.scale_ is not None:
                self.scale_inv_ = (1. / self.scale_).astype(numpy.float32)
            return self
        
        def transform(self, X):
            X = X.copy()
            if self.with_mean:
                X -= self.mean_
            if self.with_std:
                X *= self.scale_inv_
            return X
    
        
    model_float = Pipeline([
        ('scaler', StandardScalerFloat()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    
    model_float.fit(Xi_train.astype(numpy.float32), yi_train.astype(numpy.float32))




.. parsed-literal::
    Pipeline(steps=[('scaler', StandardScalerFloat()),
                    ('dt', DecisionTreeRegressor(max_depth=10))])



.. code:: ipython3

    try:
        onx_float = to_onnx(model_float, Xi_test[:1].astype(numpy.float))
    except RuntimeError as e:
        print(e)


.. parsed-literal::
    Unable to find a shape calculator for type '<class '__main__.StandardScalerFloat'>'.
    It usually means the pipeline being converted contains a
    transformer or a predictor with no corresponding converter
    implemented in sklearn-onnx. If the converted is implemented
    in another library, you need to register
    the converted so that it can be used by sklearn-onnx (function
    update_registered_converter). If the model is not yet covered
    by sklearn-onnx, you may raise an issue to
    https://github.com/onnx/sklearn-onnx/issues
    to get the converter implemented or even contribute to the
    project. If the model is a custom model, a new converter must
    be implemented. Examples can be found in the gallery.
    


We need to register a new converter so that *sklearn-onnx* knows how to
convert the new scaler. We reuse the existing converters.

.. code:: ipython3

    from skl2onnx import update_registered_converter
    from skl2onnx.operator_converters.scaler_op import convert_sklearn_scaler
    from skl2onnx.shape_calculators.scaler import calculate_sklearn_scaler_output_shapes
    
    
    update_registered_converter(
        StandardScalerFloat, "SklearnStandardScalerFloat",
        calculate_sklearn_scaler_output_shapes,
        convert_sklearn_scaler,
        options={'div': ['std', 'div', 'div_cast']})

.. code:: ipython3

    models = [
        ('bug', Xi_test.astype(numpy.float32), model),
        ('FloatPipeline', Xi_test.astype(numpy.float32), model_float),
    ]

.. code:: ipython3

    obs = [dict(runtime='sklearn', diff=0, name='sklearn')]
    for name, x32, mod in models:
        for runtime in ['python', 'python_compiled', 'onnxruntime1']:
            lonx = to_onnx(mod, x32[:1])
            loinf = OnnxInference(lonx, runtime=runtime)
            y_skl = model_float.predict(X32)  # we use model_float as a baseline
            y_onx = loinf.run({'X': X32})['variable']
            delta = numpy.abs(y_skl - y_onx.ravel())
            am = delta.argmax()
            obs.append(dict(runtime=runtime, diff=delta.max(), name=name))
            obs[-1]['v[%d]' % am] = y_onx.ravel()[am]
            obs[0]['v[%d]' % am] = y_skl.ravel()[am]
    
    df = pandas.DataFrame(obs)
    df






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>name</th>
          <th>v[1489]</th>
          <th>v[1109]</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>sklearn</td>
          <td>378.038116</td>
          <td>516.084493</td>
        </tr>
        <tr>
          <th>1</th>
          <td>python</td>
          <td>273.322334</td>
          <td>bug</td>
          <td>104.715782</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python_compiled</td>
          <td>273.322334</td>
          <td>bug</td>
          <td>104.715782</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>onnxruntime1</td>
          <td>273.322334</td>
          <td>bug</td>
          <td>104.715782</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>python</td>
          <td>0.000020</td>
          <td>FloatPipeline</td>
          <td>NaN</td>
          <td>516.084473</td>
        </tr>
        <tr>
          <th>5</th>
          <td>python_compiled</td>
          <td>0.000020</td>
          <td>FloatPipeline</td>
          <td>NaN</td>
          <td>516.084473</td>
        </tr>
        <tr>
          <th>6</th>
          <td>onnxruntime1</td>
          <td>0.000020</td>
          <td>FloatPipeline</td>
          <td>NaN</td>
          <td>516.084473</td>
        </tr>
      </tbody>
    </table>
    </div>



That means than the differences between ``float32(X / Y)`` and
``float32(X) * float32(1 / Y)`` are big enough to select a different
path in the decision tree. ``float32(X) / float32(Y)`` and
``float32(X) * float32(1 / Y)`` are also different enough to trigger a
different path. Let’s illustrate that on example:

.. code:: ipython3

    a1 = numpy.random.randn(100, 2) * 10
    a2 = a1.copy()
    a2[:, 1] *= 1000
    a3 = a1.copy()
    a3[:, 0] *= 1000
    
    for i, a in enumerate([a1, a2, a3]):
        a = a.astype(numpy.float32)
        max_diff32 = numpy.max([
            numpy.abs(numpy.float32(x[0]) / numpy.float32(x[1]) - 
                numpy.float32(x[0]) * (numpy.float32(1) / numpy.float32(x[1])))
            for x in a])
        max_diff64 = numpy.max([
            numpy.abs(numpy.float64(x[0]) / numpy.float64(x[1]) - 
                numpy.float64(x[0]) * (numpy.float64(1) / numpy.float64(x[1])))
            for x in a])
        print(i, max_diff32, max_diff64)


.. parsed-literal::
    0 1.9073486e-06 7.105427357601002e-15
    1 3.7252903e-09 3.469446951953614e-18
    2 0.00390625 7.275957614183426e-12


The last random set shows very big differences, obviously big enough to
trigger a different path in the graph. The difference for double could
probably be significant in some cases, not enough on this example.

Change the conversion with option *div*
---------------------------------------

Option ``'div'`` was added to the converter for *StandardScaler* to
change the way the scaler is converted.

.. code:: ipython3

    model = Pipeline([
        ('scaler', StandardScaler()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    model.fit(Xi_train, yi_train)




.. parsed-literal::
    Pipeline(steps=[('scaler', StandardScaler()),
                    ('dt', DecisionTreeRegressor(max_depth=10))])



.. code:: ipython3

    onx_std = to_onnx(model, Xi_train[:1].astype(numpy.float32))
    
    %onnxview onx_std






.. raw:: html

    <div id="Mf2605ecdcd5a40f88e17e654b5b25cb3-cont"><div id="Mf2605ecdcd5a40f88e17e654b5b25cb3" style="width:100%;height:100%;"></div></div>
    <script>

    require(['http://www.xavierdupre.fr/js/vizjs/viz.js'], function() { var svgGraph = Viz("digraph{\n  nodesep=0.05;\n  ranksep=0.25;\n  orientation=portrait;\n\n  X [shape=box color=red label=\"X\nfloat((0, 10))\" fontsize=10];\n\n  variable [shape=box color=green label=\"variable\nfloat((0, 1))\" fontsize=10];\n\n\n  variable1 [shape=box label=\"variable1\" fontsize=10];\n  Scaler [shape=box style=\"filled,rounded\" color=orange label=\"Scaler\n(Scaler)\noffset=[ 2.6666666e-03  5.86666...\nscale=[0.35824135 0.16805995 0....\" fontsize=10];\n  X -> Scaler;\n  Scaler -> variable1;\n\n  TreeEnsembleRegressor [shape=box style=\"filled,rounded\" color=orange label=\"TreeEnsembleRegressor\n(TreeEnsembleRegressor)\nn_targets=1\nnodes_falsenodeids=[ 930  461  ...\nnodes_featureids=[7 8 3 ... 7 0...\nnodes_hitrates=[1. 1. 1. ... 1....\nnodes_missing_value_tracks_true=[0 0 0 ......\nnodes_modes=[b'BRANCH_LEQ' b'BR...\nnodes_nodeids=[   0    1    2 ....\nnodes_treeids=[0 0 0 ... 0 0 0]\nnodes_truenodeids=[   1    2   ...\nnodes_values=[-0.2612203   0.08...\npost_transform=b'NONE'\ntarget_ids=[0 0 0 0 0 0 0 0 0 0...\ntarget_nodeids=[  10   11   12 ...\ntarget_treeids=[0 0 0 0 0 0 0 0...\ntarget_weights=[-5.76468811e+02...\" fontsize=10];\n  variable1 -> TreeEnsembleRegressor;\n  TreeEnsembleRegressor -> variable;\n}");
    document.getElementById('Mf2605ecdcd5a40f88e17e654b5b25cb3').innerHTML = svgGraph; });

    </script>



.. code:: ipython3

    onx_div = to_onnx(model, Xi_train[:1].astype(numpy.float32),
                      options={StandardScaler: {'div': 'div'}})
    %onnxview onx_div                  






.. raw:: html

    <div id="Md568d7f27c08496d92427f853145d394-cont"><div id="Md568d7f27c08496d92427f853145d394" style="width:100%;height:100%;"></div></div>
    <script>

    require(['http://www.xavierdupre.fr/js/vizjs/viz.js'], function() { var svgGraph = Viz("digraph{\n  nodesep=0.05;\n  ranksep=0.25;\n  orientation=portrait;\n\n  X [shape=box color=red label=\"X\nfloat((0, 10))\" fontsize=10];\n\n  variable [shape=box color=green label=\"variable\nfloat((0, 1))\" fontsize=10];\n\n  Su_Subcst [shape=box label=\"Su_Subcst\nfloat32((10,))\n[ 2.6666666e-03  5.8666668e-03  2.1600001e-02  4.8...\" fontsize=10];\n  Di_Divcst [shape=box label=\"Di_Divcst\nfloat32((10,))\n[   2.791414    5.950258   12.067811   25.000826  ...\" fontsize=10];\n\n  Su_C0 [shape=box label=\"Su_C0\" fontsize=10];\n  Su_Sub [shape=box style=\"filled,rounded\" color=orange label=\"Sub\n(Su_Sub)\" fontsize=10];\n  X -> Su_Sub;\n  Su_Subcst -> Su_Sub;\n  Su_Sub -> Su_C0;\n\n  variable1 [shape=box label=\"variable1\" fontsize=10];\n  Di_Div [shape=box style=\"filled,rounded\" color=orange label=\"Div\n(Di_Div)\" fontsize=10];\n  Su_C0 -> Di_Div;\n  Di_Divcst -> Di_Div;\n  Di_Div -> variable1;\n\n  TreeEnsembleRegressor [shape=box style=\"filled,rounded\" color=orange label=\"TreeEnsembleRegressor\n(TreeEnsembleRegressor)\nn_targets=1\nnodes_falsenodeids=[ 930  461  ...\nnodes_featureids=[7 8 3 ... 7 0...\nnodes_hitrates=[1. 1. 1. ... 1....\nnodes_missing_value_tracks_true=[0 0 0 ......\nnodes_modes=[b'BRANCH_LEQ' b'BR...\nnodes_nodeids=[   0    1    2 ....\nnodes_treeids=[0 0 0 ... 0 0 0]\nnodes_truenodeids=[   1    2   ...\nnodes_values=[-0.2612203   0.08...\npost_transform=b'NONE'\ntarget_ids=[0 0 0 0 0 0 0 0 0 0...\ntarget_nodeids=[  10   11   12 ...\ntarget_treeids=[0 0 0 0 0 0 0 0...\ntarget_weights=[-5.76468811e+02...\" fontsize=10];\n  variable1 -> TreeEnsembleRegressor;\n  TreeEnsembleRegressor -> variable;\n}");
    document.getElementById('Md568d7f27c08496d92427f853145d394').innerHTML = svgGraph; });

    </script>



.. code:: ipython3

    onx_div_cast = to_onnx(model, Xi_train[:1].astype(numpy.float32),
                           options={StandardScaler: {'div': 'div_cast'}})
    %onnxview onx_div_cast






.. raw:: html

    <div id="M4b52eb3ba264417ba7be4c07b28ae471-cont"><div id="M4b52eb3ba264417ba7be4c07b28ae471" style="width:100%;height:100%;"></div></div>
    <script>

    require(['http://www.xavierdupre.fr/js/vizjs/viz.js'], function() { var svgGraph = Viz("digraph{\n  nodesep=0.05;\n  ranksep=0.25;\n  orientation=portrait;\n\n  X [shape=box color=red label=\"X\nfloat((0, 10))\" fontsize=10];\n\n  variable [shape=box color=green label=\"variable\nfloat((0, 1))\" fontsize=10];\n\n  Su_Subcst [shape=box label=\"Su_Subcst\nfloat64((10,))\n[ 2.66666664e-03  5.86666679e-03  2.16000006e-02  ...\" fontsize=10];\n  Di_Divcst [shape=box label=\"Di_Divcst\nfloat64((10,))\n[   2.79141414    5.95025761   12.06781118   25.00...\" fontsize=10];\n\n  Ca_output0 [shape=box label=\"Ca_output0\" fontsize=10];\n  Ca_Cast [shape=box style=\"filled,rounded\" color=orange label=\"Cast\n(Ca_Cast)\nto=11\" fontsize=10];\n  X -> Ca_Cast;\n  Ca_Cast -> Ca_output0;\n\n  Su_C0 [shape=box label=\"Su_C0\" fontsize=10];\n  Su_Sub [shape=box style=\"filled,rounded\" color=orange label=\"Sub\n(Su_Sub)\" fontsize=10];\n  Ca_output0 -> Su_Sub;\n  Su_Subcst -> Su_Sub;\n  Su_Sub -> Su_C0;\n\n  Di_C0 [shape=box label=\"Di_C0\" fontsize=10];\n  Di_Div [shape=box style=\"filled,rounded\" color=orange label=\"Div\n(Di_Div)\" fontsize=10];\n  Su_C0 -> Di_Div;\n  Di_Divcst -> Di_Div;\n  Di_Div -> Di_C0;\n\n  variable1 [shape=box label=\"variable1\" fontsize=10];\n  Ca_Cast1 [shape=box style=\"filled,rounded\" color=orange label=\"Cast\n(Ca_Cast1)\nto=1\" fontsize=10];\n  Di_C0 -> Ca_Cast1;\n  Ca_Cast1 -> variable1;\n\n  TreeEnsembleRegressor [shape=box style=\"filled,rounded\" color=orange label=\"TreeEnsembleRegressor\n(TreeEnsembleRegressor)\nn_targets=1\nnodes_falsenodeids=[ 930  461  ...\nnodes_featureids=[7 8 3 ... 7 0...\nnodes_hitrates=[1. 1. 1. ... 1....\nnodes_missing_value_tracks_true=[0 0 0 ......\nnodes_modes=[b'BRANCH_LEQ' b'BR...\nnodes_nodeids=[   0    1    2 ....\nnodes_treeids=[0 0 0 ... 0 0 0]\nnodes_truenodeids=[   1    2   ...\nnodes_values=[-0.2612203   0.08...\npost_transform=b'NONE'\ntarget_ids=[0 0 0 0 0 0 0 0 0 0...\ntarget_nodeids=[  10   11   12 ...\ntarget_treeids=[0 0 0 0 0 0 0 0...\ntarget_weights=[-5.76468811e+02...\" fontsize=10];\n  variable1 -> TreeEnsembleRegressor;\n  TreeEnsembleRegressor -> variable;\n}");
    document.getElementById('M4b52eb3ba264417ba7be4c07b28ae471').innerHTML = svgGraph; });

    </script>



The ONNX graph is different and using division. Let’s measure the
discrepencies.

.. code:: ipython3

    X32 = Xi_test.astype(numpy.float32)
    X64 = Xi_test.astype(numpy.float64)
    models = [('bug', model, onx_std),
              ('div', model, onx_div),
              ('div_cast', model, onx_div_cast),]
    
    obs = [dict(runtime='sklearn', diff=0, name='sklearn')]
    for name, mod, onx in models:
        for runtime in ['python', 'python_compiled', 'onnxruntime1']:
            oinf = OnnxInference(onx, runtime=runtime)
            y_skl32 = mod.predict(X32)
            y_skl64 = mod.predict(X64)
            y_onx = oinf.run({'X': X32})['variable']
    
            delta32 = numpy.abs(y_skl32 - y_onx.ravel())
            am32 = delta32.argmax()
            delta64 = numpy.abs(y_skl64 - y_onx.ravel())
            am64 = delta64.argmax()
    
            obs.append(dict(runtime=runtime, diff32=delta32.max(), 
                            diff64=delta64.max(), name=name))
            obs[0]['v32[%d]' % am32] = y_skl32.ravel()[am32]
            obs[0]['v64[%d]' % am64] = y_skl64.ravel()[am64]
            obs[-1]['v32[%d]' % am32] = y_onx.ravel()[am32]
            obs[-1]['v64[%d]' % am64] = y_onx.ravel()[am64]
    
    df = pandas.DataFrame(obs)
    df






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>name</th>
          <th>v32[1583]</th>
          <th>v64[1246]</th>
          <th>v32[1246]</th>
          <th>v64[2080]</th>
          <th>v64[1109]</th>
          <th>diff32</th>
          <th>diff64</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.0</td>
          <td>sklearn</td>
          <td>-439.590635</td>
          <td>-364.555875</td>
          <td>-203.438616</td>
          <td>171.604023</td>
          <td>516.084502</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>python</td>
          <td>NaN</td>
          <td>bug</td>
          <td>-305.949036</td>
          <td>-203.438614</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>133.641599</td>
          <td>161.117261</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python_compiled</td>
          <td>NaN</td>
          <td>bug</td>
          <td>-305.949036</td>
          <td>-203.438614</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>133.641599</td>
          <td>161.117261</td>
        </tr>
        <tr>
          <th>3</th>
          <td>onnxruntime1</td>
          <td>NaN</td>
          <td>bug</td>
          <td>-305.949036</td>
          <td>-203.438614</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>133.641599</td>
          <td>161.117261</td>
        </tr>
        <tr>
          <th>4</th>
          <td>python</td>
          <td>NaN</td>
          <td>div</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>329.592377</td>
          <td>NaN</td>
          <td>161.117261</td>
          <td>157.988354</td>
        </tr>
        <tr>
          <th>5</th>
          <td>python_compiled</td>
          <td>NaN</td>
          <td>div</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>329.592377</td>
          <td>NaN</td>
          <td>161.117261</td>
          <td>157.988354</td>
        </tr>
        <tr>
          <th>6</th>
          <td>onnxruntime1</td>
          <td>NaN</td>
          <td>div</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>329.592377</td>
          <td>NaN</td>
          <td>161.117261</td>
          <td>157.988354</td>
        </tr>
        <tr>
          <th>7</th>
          <td>python</td>
          <td>NaN</td>
          <td>div_cast</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>161.117261</td>
          <td>0.000029</td>
        </tr>
        <tr>
          <th>8</th>
          <td>python_compiled</td>
          <td>NaN</td>
          <td>div_cast</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>161.117261</td>
          <td>0.000029</td>
        </tr>
        <tr>
          <th>9</th>
          <td>onnxruntime1</td>
          <td>NaN</td>
          <td>div_cast</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>161.117261</td>
          <td>0.000029</td>
        </tr>
      </tbody>
    </table>
    </div>



The only combination which works is the model converted with option
*div_cast* (use of division in double precision), float input for ONNX,
double input for *scikit-learn*.

Explanation in practice
-----------------------

Based on previous sections, the following example buids a case where
discreprencies are significant.

.. code:: ipython3

    std = StandardScaler()
    std.fit(Xi_train)
    xt32 = Xi_test.astype(numpy.float32)
    xt64 = Xi_test.astype(numpy.float64)
    pred = std.transform(xt32)

.. code:: ipython3

    from onnxruntime import InferenceSession
    
    onx32 = to_onnx(std, Xi_train[:1].astype(numpy.float32))
    sess32 = InferenceSession(onx32.SerializeToString())
    got32 = sess32.run(0, {'X': xt32})[0]
    d32 = numpy.max(numpy.abs(pred.ravel() - got32.ravel()))
    d32




.. parsed-literal::
    2.3841858e-07



.. code:: ipython3

    oinf32 = OnnxInference(onx32.SerializeToString())
    gotpy32 = oinf32.run({'X': xt32})['variable']
    dpy32 = numpy.max(numpy.abs(pred.ravel() - gotpy32.ravel()))
    dpy32




.. parsed-literal::
    2.3841858e-07



We tried to cast float into double before applying the normalisation and
to cast back into single float. It does not help much.

.. code:: ipython3

    onx64 = to_onnx(std, Xi_train[:1].astype(numpy.float32),
                    options={id(std): {'div': 'div'}})        
    sess64 = InferenceSession(onx64.SerializeToString())
    got64 = sess64.run(0, {'X': xt32})[0]
    d64 = numpy.max(numpy.abs(pred.ravel() - got64.ravel()))
    d64




.. parsed-literal::
    2.3841858e-07



Last experiment, we try to use double all along.

.. code:: ipython3

    from onnxruntime.capi.onnxruntime_pybind11_state import InvalidGraph
    
    onx64_2 = to_onnx(std, Xi_train[:1].astype(numpy.float64))
    try:
        sess64_2 = InferenceSession(onx64_2.SerializeToString())
    except InvalidGraph as e:
        print(e)


.. parsed-literal::
    [ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Error in Node:Scaler : Mismatched attribute type in 'Scaler : offset'


*onnxruntime* does not support this. Let’s switch to *mlprodict*.

.. code:: ipython3

    onx64_2 = to_onnx(std, Xi_train[:1].astype(numpy.float64))
    sess64_2 = OnnxInference(onx64_2, runtime="python")
    pred64 = std.transform(xt64)
    got64_2 = sess64_2.run({'X': xt64})['variable']
    d64_2 = numpy.max(numpy.abs(pred64.ravel() - got64_2.ravel()))
    d64_2




.. parsed-literal::
    4.440892098500626e-16



Differences are lower if every operator is done with double.

Conclusion
----------

Maybe the best option is just to introduce a transform which just cast
inputs into floats.

.. code:: ipython3

    model1 = Pipeline([
        ('scaler', StandardScaler()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    
    model1.fit(Xi_train, yi_train)




.. parsed-literal::
    Pipeline(steps=[('scaler', StandardScaler()),
                    ('dt', DecisionTreeRegressor(max_depth=10))])



.. code:: ipython3

    from skl2onnx.sklapi import CastTransformer
    
    model2 = Pipeline([
        ('cast64', CastTransformer(dtype=numpy.float64)),
        ('scaler', StandardScaler()),
        ('cast', CastTransformer()),
        ('dt', DecisionTreeRegressor(max_depth=max_depth))
    ])
    
    model2.fit(Xi_train, yi_train)




.. parsed-literal::
    Pipeline(steps=[('cast64', CastTransformer(dtype=<class 'numpy.float64'>)),
                    ('scaler', StandardScaler()), ('cast', CastTransformer()),
                    ('dt', DecisionTreeRegressor(max_depth=10))])



.. code:: ipython3

    X32 = Xi_test.astype(numpy.float32)
    models = [('model1', model1, X32), ('model2', model2, X32)]
    options = [('-', None),
               ('div_cast', {StandardScaler: {'div': 'div_cast'}})]
    
    obs = [dict(runtime='sklearn', diff=0, name='model1'),
           dict(runtime='sklearn', diff=0, name='model2')]
    for name, mod, x32 in models:
        for no, opts in options:
            onx = to_onnx(mod, Xi_train[:1].astype(numpy.float32),
                          options=opts)
            for runtime in ['python', 'python_compiled', 'onnxruntime1']:
                try:
                    oinf = OnnxInference(onx, runtime=runtime)
                except Exception as e:
                    obs.append(dict(runtime=runtime, err=str(e),
                                    name=name, options=no))
                    continue
                    
                y_skl = mod.predict(x32)
                try:
                    y_onx = oinf.run({'X': x32})['variable']
                except Exception as e:
                    obs.append(dict(runtime=runtime, err=str(e),
                                    name=name, options=no))
                    continue
    
                delta = numpy.abs(y_skl - y_onx.ravel())
                am = delta.argmax()
    
                obs.append(dict(runtime=runtime, diff=delta.max(),
                                name=name, options=no))
                obs[-1]['v[%d]' % am] = y_onx.ravel()[am]
                if name == 'model1':
                    obs[0]['v[%d]' % am] = y_skl.ravel()[am]
                    obs[1]['v[%d]' % am] = model2.predict(Xi_test).ravel()[am]
                elif name == 'model2':
                    obs[0]['v[%d]' % am] = model1.predict(Xi_test).ravel()[am]
                    obs[1]['v[%d]' % am] = y_skl.ravel()[am]
    
    df = pandas.DataFrame(obs)
    df






.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>runtime</th>
          <th>diff</th>
          <th>name</th>
          <th>v[1583]</th>
          <th>v[1246]</th>
          <th>v[1109]</th>
          <th>options</th>
          <th>err</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>model1</td>
          <td>-439.590635</td>
          <td>-162.952888</td>
          <td>516.084502</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>sklearn</td>
          <td>0.000000</td>
          <td>model2</td>
          <td>-439.590635</td>
          <td>-364.555875</td>
          <td>516.084502</td>
          <td>NaN</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>python</td>
          <td>133.641599</td>
          <td>model1</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>python_compiled</td>
          <td>133.641599</td>
          <td>model1</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>onnxruntime1</td>
          <td>133.641599</td>
          <td>model1</td>
          <td>-305.949036</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>5</th>
          <td>python</td>
          <td>201.602989</td>
          <td>model1</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>NaN</td>
          <td>div_cast</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>6</th>
          <td>python_compiled</td>
          <td>201.602989</td>
          <td>model1</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>NaN</td>
          <td>div_cast</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>7</th>
          <td>onnxruntime1</td>
          <td>201.602989</td>
          <td>model1</td>
          <td>NaN</td>
          <td>-364.555878</td>
          <td>NaN</td>
          <td>div_cast</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>8</th>
          <td>python</td>
          <td>0.000029</td>
          <td>model2</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>-</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>9</th>
          <td>python_compiled</td>
          <td>0.000029</td>
          <td>model2</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>-</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>10</th>
          <td>onnxruntime1</td>
          <td>NaN</td>
          <td>model2</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>-</td>
          <td>Unable to create InferenceSession due to '[ONN...</td>
        </tr>
        <tr>
          <th>11</th>
          <td>python</td>
          <td>0.000029</td>
          <td>model2</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>div_cast</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>12</th>
          <td>python_compiled</td>
          <td>0.000029</td>
          <td>model2</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>div_cast</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>13</th>
          <td>onnxruntime1</td>
          <td>0.000029</td>
          <td>model2</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>516.084473</td>
          <td>div_cast</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>



It seems to work that way.
