
.. _l-onnx-doc-SparseSoftmaxCrossEntropyGrad:

=============================
SparseSoftmaxCrossEntropyGrad
=============================

.. contents::
    :local:


.. _l-onnx-op-sparsesoftmaxcrossentropygrad-9:

SparseSoftmaxCrossEntropyGrad - 9
=================================

**Version**

* **name**: `SparseSoftmaxCrossEntropyGrad (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#SparseSoftmaxCrossEntropyGrad>`_
* **domain**: **main**
* **since_version**: **9**
* **function**:
* **support_level**: SupportType.COMMON
* **shape inference**: False

This version of the operator has been available
**since version 9**.

**Summary**

**Attributes**

* **reduction - STRING** :   Type of reduction to apply to loss: none, sum, mean(default).
  'none': the output is the loss for each sample in the batch.'sum':
  the output will be summed. 'mean': the sum of the output will be
  divided by the batch_size.

**Inputs**

Between 3 and 4 inputs.

* **dY** (heterogeneous) - **T**:

* **log_prob** (heterogeneous) - **T**:

* **label** (heterogeneous) - **Tind**:

* **weight** (optional, heterogeneous) - **T**:

**Outputs**

* **d_logits** (heterogeneous) - **T**:

**Type Constraints**

* **T** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain to float, float16 and double tensors.
* **Tind** in (
  tensor(int32),
  tensor(int64)
  ):
  Constrain indices to integer types

**Examples**
