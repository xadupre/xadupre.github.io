
.. _l-onnx-doccom-microsoft-AdamOptimizer:

=============================
com.microsoft - AdamOptimizer
=============================

.. contents::
    :local:


.. _l-onnx-opcom-microsoft-adamoptimizer-1:

AdamOptimizer - 1
=================

**Version**

* **name**: `AdamOptimizer (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#com.microsoft.AdamOptimizer>`_
* **domain**: **com.microsoft**
* **since_version**: **1**
* **function**:
* **support_level**: SupportType.COMMON
* **shape inference**: False

This version of the operator has been available
**since version 1 of domain com.microsoft**.

**Summary**

**Attributes**

* **alpha - FLOAT** :   Coefficient of previous gradient in running average.
* **beta - FLOAT** :   Coefficient of previous squared gradient in running average.The
  effective learning rate is computed by r = R / (1 + T *
  decay_factor). Default to 0 so that increasing update counts doesn't
  reduce the learning rate.
* **do_bias_correction - INT** :   Compute unbiased 1st and 2nd momentums.
* **epsilon - FLOAT** :   Small scalar to avoid dividing by zero.
* **lambda - FLOAT** :   Regularization coefficient of 0.5 * lambda * ||X||_2^2. Default to
  0, which means no regularization.
* **max_norm_clip - FLOAT** :   clip threshold of gradients.
* **weight_decay_mode - INT** :   Modes for applying weight decay, 0 means applying decay before
  weight update, 1 means applying decay after weight update.

**Inputs**

Between 6 and 10 inputs.

* **R** (heterogeneous) - **T1**:

* **T** (heterogeneous) - **T2**:

* **weights** (heterogeneous) - **T3**:

* **gradients** (heterogeneous) - **T_GRAD**:

* **moment_1** (heterogeneous) - **T4**:

* **moment_2** (heterogeneous) - **T4**:

* **mixed_precision_weights** (optional, heterogeneous) - **T_MIXED_PRECISION_FP**:

* **loss_scale** (optional, heterogeneous) - **T3**:

* **global_gradient_norm** (optional, heterogeneous) - **T_GRAD_NORM**:

* **update_signal** (optional, heterogeneous) - **T_BOOL**:

**Outputs**

Between 3 and 6 outputs.

* **new_T** (heterogeneous) - **T2**:

* **new_moment_1** (heterogeneous) - **T4**:

* **new_moment_2** (heterogeneous) - **T4**:

* **new_weights** (optional, heterogeneous) - **T3**:

* **new_gradients** (optional, heterogeneous) - **T_GRAD**:

* **new_mixed_precision_weights** (optional, heterogeneous) - **T_MIXED_PRECISION_FP**:

**Type Constraints**

* **T1** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain learning rate to float
* **T2** in (
  int64
  ):
  Constrain step count to 64-bit integer
* **T3** in (
  tensor(double),
  tensor(float)
  ):
  Constrain input types to float tensors.
* **T4** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input types to float tensors.
* **T_GRAD** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input types to float tensors.
* **T_MIXED_PRECISION_FP** in (
  tensor(bfloat16),
  tensor(float16)
  ):
  Constrain input types to float16 or bfloat16 tensors.
* **T_GRAD_NORM** in (
  tensor(bfloat16),
  tensor(double),
  tensor(float),
  tensor(float16)
  ):
  Constrain input types to float tensors.
* **T_BOOL** in (
  tensor(bool)
  ):
  Constrain types to boolean tensors.

**Examples**
