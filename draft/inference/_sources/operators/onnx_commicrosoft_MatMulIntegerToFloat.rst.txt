
.. _l-onnx-doccom-microsoft-MatMulIntegerToFloat:

====================================
com.microsoft - MatMulIntegerToFloat
====================================


.. _l-onnx-opcom-microsoft-matmulintegertofloat-1:

MatMulIntegerToFloat - 1
========================

**Version**

* **name**: `MatMulIntegerToFloat (GitHub) <https://github.com/onnx/onnx/blob/main/docs/Operators.md#com.microsoft.MatMulIntegerToFloat>`_
* **domain**: **com.microsoft**
* **since_version**: **1**
* **function**:
* **support_level**: SupportType.COMMON
* **shape inference**: True

This version of the operator has been available
**since version 1 of domain com.microsoft**.

**Summary**

**Inputs**

Between 4 and 7 inputs.

* **A** (heterogeneous) - **T1**:

* **B** (heterogeneous) - **T2**:

* **a_scale** (heterogeneous) - **T3**:

* **b_scale** (heterogeneous) - **T3**:

* **a_zero_point** (optional, heterogeneous) - **T1**:

* **b_zero_point** (optional, heterogeneous) - **T2**:

* **bias** (optional, heterogeneous) - **T3**:

**Outputs**

* **Y** (heterogeneous) - **T3**:

**Type Constraints**

* **T1** in (
  tensor(int8),
  tensor(uint8)
  ):
  Constrain input A data type to 8-bit integer tensor.
* **T2** in (
  tensor(int8),
  tensor(uint8)
  ):
  Constrain input B data type to 8-bit integer tensor.
* **T3** in (
  tensor(float)
  ):
  Constrain input a_scale, b_scale and output Y data type as float
  tensor.

**Examples**
