<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156955408-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-156955408-1');
    </script>
    <link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Gallery of examples" href="auto_examples/index.html" /><link rel="prev" title="Tutorial" href="tutorial.html" />

    <!-- Generated with Sphinx 6.1.3 and Furo 2022.12.07 -->
        <title>API - ONNX Runtime 1.15.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">ONNX Runtime 1.15.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="_static/ONNX_Runtime_icon.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">ONNX Runtime 1.15.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">API</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="auto_examples/index.html">Gallery of examples</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_pipeline.html">Draw a pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_load_and_predict.html">Load and predict with ONNX Runtime and a very simple model</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_backend.html">ONNX Runtime Backend for ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_metadata.html">Metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_profiling.html">Profile the execution of a simple model</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_convert_pipeline_vectorizer.html">Train, convert and predict with ONNX Runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_common_errors.html">Common errors with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_examples/plot_train_convert_predict.html">Train, convert and predict with ONNX Runtime</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="operators/index.html">ONNX Operators in onnxruntime</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Abs.html">Abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Acos.html">Acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Acosh.html">Acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Add.html">Add</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Affine.html">Affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__And.html">And</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ArgMax.html">ArgMax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ArgMin.html">ArgMin</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Asin.html">Asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Asinh.html">Asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Atan.html">Atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Atanh.html">Atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__AveragePool.html">AveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BatchNormalization.html">BatchNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Bernoulli.html">Bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BitShift.html">BitShift</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BitwiseAnd.html">BitwiseAnd</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BitwiseNot.html">BitwiseNot</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BitwiseOr.html">BitwiseOr</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BitwiseXor.html">BitwiseXor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__BlackmanWindow.html">BlackmanWindow</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Cast.html">Cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__CastLike.html">CastLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Ceil.html">Ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Celu.html">Celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__CenterCropPad.html">CenterCropPad</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Clip.html">Clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Col2Im.html">Col2Im</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Compress.html">Compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Concat.html">Concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ConcatFromSequence.html">ConcatFromSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Constant.html">Constant</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ConstantOfShape.html">ConstantOfShape</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Conv.html">Conv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ConvInteger.html">ConvInteger</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ConvTranspose.html">ConvTranspose</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Cos.html">Cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Cosh.html">Cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Crop.html">Crop</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__CumSum.html">CumSum</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__DFT.html">DFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__DepthToSpace.html">DepthToSpace</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__DequantizeLinear.html">DequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Det.html">Det</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__DisentangledAttention_TRT.html">DisentangledAttention_TRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Div.html">Div</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__DynamicQuantizeLinear.html">DynamicQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__DynamicSlice.html">DynamicSlice</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__EfficientNMS_TRT.html">EfficientNMS_TRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Einsum.html">Einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Elu.html">Elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Equal.html">Equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Erf.html">Erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Exp.html">Exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Expand.html">Expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__EyeLike.html">EyeLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Floor.html">Floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GRUUnit.html">GRUUnit</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Gather.html">Gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GatherElements.html">GatherElements</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GatherND.html">GatherND</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Gemm.html">Gemm</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GivenTensorFill.html">GivenTensorFill</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GlobalAveragePool.html">GlobalAveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GlobalLpPool.html">GlobalLpPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GlobalMaxPool.html">GlobalMaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Greater.html">Greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GreaterOrEqual.html">GreaterOrEqual</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GridSample.html">GridSample</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__GroupNormalization.html">GroupNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__HammingWindow.html">HammingWindow</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__HannWindow.html">HannWindow</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__HardSigmoid.html">HardSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__HardSwish.html">HardSwish</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Hardmax.html">Hardmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__If.html">If</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ImageScaler.html">ImageScaler</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__InstanceNormalization.html">InstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__IsInf.html">IsInf</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__IsNaN.html">IsNaN</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LRN.html">LRN</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LayerNormalization.html">LayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LeakyRelu.html">LeakyRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Less.html">Less</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LessOrEqual.html">LessOrEqual</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Log.html">Log</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Loop.html">Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LpNormalization.html">LpNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__LpPool.html">LpPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MatMul.html">MatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MatMulInteger.html">MatMulInteger</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Max.html">Max</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MaxPool.html">MaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MaxRoiPool.html">MaxRoiPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MaxUnpool.html">MaxUnpool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Mean.html">Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MeanVarianceNormalization.html">MeanVarianceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MelWeightMatrix.html">MelWeightMatrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MemcpyFromHost.html">MemcpyFromHost</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MemcpyToHost.html">MemcpyToHost</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Min.html">Min</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Mod.html">Mod</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Mul.html">Mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__MultilevelCropAndResize_TRT.html">MultilevelCropAndResize_TRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Multinomial.html">Multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Neg.html">Neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__NegativeLogLikelihoodLoss.html">NegativeLogLikelihoodLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__NonMaxSuppression.html">NonMaxSuppression</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__NonZero.html">NonZero</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Not.html">Not</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__OneHot.html">OneHot</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Optional.html">Optional</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__OptionalGetElement.html">OptionalGetElement</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__OptionalHasElement.html">OptionalHasElement</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Or.html">Or</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__PRelu.html">PRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Pad.html">Pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ParametricSoftplus.html">ParametricSoftplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Pow.html">Pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__PyramidROIAlign_TRT.html">PyramidROIAlign_TRT</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__QLinearConv.html">QLinearConv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__QLinearMatMul.html">QLinearMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__QuantizeLinear.html">QuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__RandomNormal.html">RandomNormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__RandomNormalLike.html">RandomNormalLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__RandomUniform.html">RandomUniform</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__RandomUniformLike.html">RandomUniformLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Range.html">Range</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Reciprocal.html">Reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceL1.html">ReduceL1</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceL2.html">ReduceL2</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceLogSum.html">ReduceLogSum</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceLogSumExp.html">ReduceLogSumExp</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceMax.html">ReduceMax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceMean.html">ReduceMean</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceMin.html">ReduceMin</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceProd.html">ReduceProd</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceSum.html">ReduceSum</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReduceSumSquare.html">ReduceSumSquare</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Relu.html">Relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Reshape.html">Reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Resize.html">Resize</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ReverseSequence.html">ReverseSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__RoiAlign.html">RoiAlign</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Round.html">Round</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__STFT.html">STFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Scale.html">Scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ScaledTanh.html">ScaledTanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Scan.html">Scan</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Scatter.html">Scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ScatterElements.html">ScatterElements</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ScatterND.html">ScatterND</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Selu.html">Selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceAt.html">SequenceAt</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceConstruct.html">SequenceConstruct</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceEmpty.html">SequenceEmpty</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceErase.html">SequenceErase</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceInsert.html">SequenceInsert</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceLength.html">SequenceLength</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SequenceMap.html">SequenceMap</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Shape.html">Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Shrink.html">Shrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sign.html">Sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SimplifiedLayerNormalization.html">SimplifiedLayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sin.html">Sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sinh.html">Sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Size.html">Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Slice.html">Slice</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SoftmaxCrossEntropyLoss.html">SoftmaxCrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SpaceToDepth.html">SpaceToDepth</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Split.html">Split</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__SplitToSequence.html">SplitToSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sqrt.html">Sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Squeeze.html">Squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__StringNormalizer.html">StringNormalizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sub.html">Sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Sum.html">Sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Tan.html">Tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__TfIdfVectorizer.html">TfIdfVectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__ThresholdedRelu.html">ThresholdedRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Tile.html">Tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__TopK.html">TopK</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Transpose.html">Transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Trilu.html">Trilu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Unique.html">Unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Unsqueeze.html">Unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Where.html">Where</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx__Xor.html">Xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_ArrayFeatureExtractor.html">ai.onnx.ml - ArrayFeatureExtractor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_Binarizer.html">ai.onnx.ml - Binarizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_CastMap.html">ai.onnx.ml - CastMap</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_CategoryMapper.html">ai.onnx.ml - CategoryMapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_DictVectorizer.html">ai.onnx.ml - DictVectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_FeatureVectorizer.html">ai.onnx.ml - FeatureVectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_Imputer.html">ai.onnx.ml - Imputer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_LabelEncoder.html">ai.onnx.ml - LabelEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_LinearClassifier.html">ai.onnx.ml - LinearClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_LinearRegressor.html">ai.onnx.ml - LinearRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_Normalizer.html">ai.onnx.ml - Normalizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_OneHotEncoder.html">ai.onnx.ml - OneHotEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_SVMClassifier.html">ai.onnx.ml - SVMClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_SVMRegressor.html">ai.onnx.ml - SVMRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_Scaler.html">ai.onnx.ml - Scaler</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_TreeEnsembleClassifier.html">ai.onnx.ml - TreeEnsembleClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_TreeEnsembleRegressor.html">ai.onnx.ml - TreeEnsembleRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_aionnxml_ZipMap.html">ai.onnx.ml - ZipMap</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Attention.html">com.microsoft - Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_AttnLSTM.html">com.microsoft - AttnLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BeamSearch.html">com.microsoft - BeamSearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BiasAdd.html">com.microsoft - BiasAdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BiasDropout.html">com.microsoft - BiasDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BiasGelu.html">com.microsoft - BiasGelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BiasSoftmax.html">com.microsoft - BiasSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BiasSplitGelu.html">com.microsoft - BiasSplitGelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BifurcationDetector.html">com.microsoft - BifurcationDetector</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BitmaskBiasDropout.html">com.microsoft - BitmaskBiasDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_BitmaskDropout.html">com.microsoft - BitmaskDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_CDist.html">com.microsoft - CDist</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_ComplexMul.html">com.microsoft - ComplexMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_ComplexMulConj.html">com.microsoft - ComplexMulConj</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_ConvTransposeWithDynamicPads.html">com.microsoft - ConvTransposeWithDynamicPads</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_CropAndResize.html">com.microsoft - CropAndResize</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DecoderAttention.html">com.microsoft - DecoderAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DecoderMaskedMultiheadAttention.html">com.microsoft - DecoderMaskedMultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DequantizeBFP.html">com.microsoft - DequantizeBFP</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DequantizeLinear.html">com.microsoft - DequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DequantizeWithOrder.html">com.microsoft - DequantizeWithOrder</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DynamicQuantizeLSTM.html">com.microsoft - DynamicQuantizeLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_DynamicQuantizeMatMul.html">com.microsoft - DynamicQuantizeMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_EmbedLayerNormalization.html">com.microsoft - EmbedLayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_ExpandDims.html">com.microsoft - ExpandDims</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_FastGelu.html">com.microsoft - FastGelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_FusedConv.html">com.microsoft - FusedConv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_FusedGemm.html">com.microsoft - FusedGemm</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_FusedMatMul.html">com.microsoft - FusedMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_GatedRelativePositionBias.html">com.microsoft - GatedRelativePositionBias</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_GatherND.html">com.microsoft - GatherND</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Gelu.html">com.microsoft - Gelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_GemmFastGelu.html">com.microsoft - GemmFastGelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_GreedySearch.html">com.microsoft - GreedySearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_GridSample.html">com.microsoft - GridSample</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_GroupNorm.html">com.microsoft - GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Inverse.html">com.microsoft - Inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Irfft.html">com.microsoft - Irfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_IsAllFinite.html">com.microsoft - IsAllFinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_LongformerAttention.html">com.microsoft - LongformerAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_MatMulInteger16.html">com.microsoft - MatMulInteger16</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_MatMulIntegerToFloat.html">com.microsoft - MatMulIntegerToFloat</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_MaxpoolWithMask.html">com.microsoft - MaxpoolWithMask</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_MulInteger.html">com.microsoft - MulInteger</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_MultiHeadAttention.html">com.microsoft - MultiHeadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_MurmurHash3.html">com.microsoft - MurmurHash3</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_NGramRepeatBlock.html">com.microsoft - NGramRepeatBlock</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_NhwcConv.html">com.microsoft - NhwcConv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_NhwcMaxPool.html">com.microsoft - NhwcMaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Pad.html">com.microsoft - Pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QAttention.html">com.microsoft - QAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QEmbedLayerNormalization.html">com.microsoft - QEmbedLayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QGemm.html">com.microsoft - QGemm</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearAdd.html">com.microsoft - QLinearAdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearAveragePool.html">com.microsoft - QLinearAveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearConcat.html">com.microsoft - QLinearConcat</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearConv.html">com.microsoft - QLinearConv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearGlobalAveragePool.html">com.microsoft - QLinearGlobalAveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearLeakyRelu.html">com.microsoft - QLinearLeakyRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearMul.html">com.microsoft - QLinearMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearReduceMean.html">com.microsoft - QLinearReduceMean</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearSigmoid.html">com.microsoft - QLinearSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearSoftmax.html">com.microsoft - QLinearSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QLinearWhere.html">com.microsoft - QLinearWhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QOrderedAttention.html">com.microsoft - QOrderedAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QOrderedGelu.html">com.microsoft - QOrderedGelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QOrderedLayerNormalization.html">com.microsoft - QOrderedLayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QOrderedLongformerAttention.html">com.microsoft - QOrderedLongformerAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QOrderedMatMul.html">com.microsoft - QOrderedMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QuantizeBFP.html">com.microsoft - QuantizeBFP</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QuantizeLinear.html">com.microsoft - QuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QuantizeWithOrder.html">com.microsoft - QuantizeWithOrder</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_QuickGelu.html">com.microsoft - QuickGelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Range.html">com.microsoft - Range</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_ReduceSumInteger.html">com.microsoft - ReduceSumInteger</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_RelativePositionBias.html">com.microsoft - RelativePositionBias</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_RemovePadding.html">com.microsoft - RemovePadding</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_RestorePadding.html">com.microsoft - RestorePadding</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Rfft.html">com.microsoft - Rfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_SampleOp.html">com.microsoft - SampleOp</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Sampling.html">com.microsoft - Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_SkipLayerNormalization.html">com.microsoft - SkipLayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_SkipSimplifiedLayerNormalization.html">com.microsoft - SkipSimplifiedLayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Snpe.html">com.microsoft - Snpe</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_SparseToDenseMatMul.html">com.microsoft - SparseToDenseMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Tokenizer.html">com.microsoft - Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_TorchEmbedding.html">com.microsoft - TorchEmbedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_TransposeMatMul.html">com.microsoft - TransposeMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Trilu.html">com.microsoft - Trilu</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_Unique.html">com.microsoft - Unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoft_WordConvEmbedding.html">com.microsoft - WordConvEmbedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_AveragePool.html">com.microsoft.nchwc - AveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_Conv.html">com.microsoft.nchwc - Conv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_GlobalAveragePool.html">com.microsoft.nchwc - GlobalAveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_GlobalMaxPool.html">com.microsoft.nchwc - GlobalMaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_MaxPool.html">com.microsoft.nchwc - MaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_ReorderInput.html">com.microsoft.nchwc - ReorderInput</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_ReorderOutput.html">com.microsoft.nchwc - ReorderOutput</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commicrosoftnchwc_Upsample.html">com.microsoft.nchwc - Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_AveragePool.html">com.ms.internal.nhwc - AveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_Conv.html">com.ms.internal.nhwc - Conv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_ConvTranspose.html">com.ms.internal.nhwc - ConvTranspose</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_MaxPool.html">com.ms.internal.nhwc - MaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_MaxUnpool.html">com.ms.internal.nhwc - MaxUnpool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_QLinearAveragePool.html">com.ms.internal.nhwc - QLinearAveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_QLinearConv.html">com.ms.internal.nhwc - QLinearConv</a></li>
<li class="toctree-l2"><a class="reference internal" href="operators/onnx_commsinternalnhwc_QLinearConvTranspose.html">com.ms.internal.nhwc - QLinearConvTranspose</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="api">
<h1>API<a class="headerlink" href="#api" title="Permalink to this heading">#</a></h1>
<section id="api-overview">
<h2>API Overview<a class="headerlink" href="#api-overview" title="Permalink to this heading">#</a></h2>
<p><em>ONNX Runtime</em> loads and runs inference on a model in ONNX graph format, or ORT format (for memory and disk constrained environments).</p>
<p>The data consumed and produced by the model can be specified and accessed in the way that best matches your scenario.</p>
<section id="load-and-run-a-model">
<h3>Load and run a model<a class="headerlink" href="#load-and-run-a-model" title="Permalink to this heading">#</a></h3>
<p>InferenceSession is the main class of ONNX Runtime. It is used to load and run an ONNX model,
as well as specify environment and application configuration options.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s1">&#39;model.onnx&#39;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">output</span> <span class="n">names</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>ONNX and ORT format models consist of a graph of computations, modeled as operators,
and implemented as optimized operator kernels for different hardware targets.
ONNX Runtime orchestrates the execution of operator kernels via <cite>execution providers</cite>.
An execution provider contains the set of kernels for a specific execution target (CPU, GPU, IoT etc).
Execution provides are configured using the <cite>providers</cite> parameter. Kernels from different execution
providers are chosen in the priority order given in the list of providers. In the example below
if there is a kernel in the CUDA execution provider ONNX Runtime executes that on GPU. If not
the kernel is executed on CPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The list of available execution providers can be found here: <a class="reference external" href="https://onnxruntime.ai/docs/execution-providers">Execution Providers</a>.</p>
<p>Since ONNX Runtime 1.10, you must explicitly specify the execution provider for your target.
Running on CPU is the only time the API allows no explicit setting of the <cite>provider</cite> parameter.
In the examples that follow, the <cite>CUDAExecutionProvider</cite> and <cite>CPUExecutionProvider</cite> are used, assuming the application is running on NVIDIA GPUs.
Replace these with the execution provider specific to your environment.</p>
<p>You can supply other session configurations via the <cite>session options</cite> parameter. For example, to enable
profiling on the session:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">options</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>
<span class="n">options</span><span class="o">.</span><span class="n">enable_profiling</span><span class="o">=</span><span class="kc">True</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">sess_options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="data-inputs-and-outputs">
<h3>Data inputs and outputs<a class="headerlink" href="#data-inputs-and-outputs" title="Permalink to this heading">#</a></h3>
<p>The ONNX Runtime Inference Session consumes and produces data using its OrtValue class.</p>
<section id="data-on-cpu">
<h4>Data on CPU<a class="headerlink" href="#data-on-cpu" title="Permalink to this heading">#</a></h4>
<p>On CPU (the default), OrtValues can be mapped to and from native Python data structures: numpy arrays, dictionaries and lists of
numpy arrays.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># X is numpy array on cpu</span>
<span class="n">ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ortvalue</span><span class="o">.</span><span class="n">device_name</span><span class="p">()</span>  <span class="c1"># &#39;cpu&#39;</span>
<span class="n">ortvalue</span><span class="o">.</span><span class="n">shape</span><span class="p">()</span>        <span class="c1"># shape of the numpy array X</span>
<span class="n">ortvalue</span><span class="o">.</span><span class="n">data_type</span><span class="p">()</span>    <span class="c1"># &#39;tensor(float)&#39;</span>
<span class="n">ortvalue</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">()</span>    <span class="c1"># &#39;True&#39;</span>
<span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">ortvalue</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">X</span><span class="p">)</span>  <span class="c1"># &#39;True&#39;</span>

<span class="c1"># ortvalue can be provided as part of the input feed to a model</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;Y&quot;</span><span class="p">],</span> <span class="p">{</span><span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="n">ortvalue</span><span class="p">})</span>
</pre></div>
</div>
<p>By default, <em>ONNX Runtime</em> always places input(s) and output(s) on CPU. Having the data on CPU
may not optimal if the input or output is consumed and produced on a device
other than CPU because it introduces data copy between CPU and the device.</p>
</section>
<section id="data-on-device">
<h4>Data on device<a class="headerlink" href="#data-on-device" title="Permalink to this heading">#</a></h4>
<p><em>ONNX Runtime</em> supports a custom data structure that supports all ONNX data formats that allows users
to place the data backing these on a device, for example, on a CUDA supported device. In ONNX Runtime,
this called <cite>IOBinding</cite>.</p>
<p>To use the <cite>IOBinding</cite> feature, replace <cite>InferenceSession.run()</cite> with <cite>InferenceSession.run_with_iobinding()</cite>.</p>
<p>A graph is executed on a device other than CPU, for instance CUDA. Users can
use IOBinding to copy the data onto the GPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># X is numpy array on cpu</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">io_binding</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<span class="c1"># OnnxRuntime will copy the data over to the CUDA device if &#39;input&#39; is consumed by nodes on the CUDA device</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_cpu_input</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_output</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">io_binding</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">io_binding</span><span class="o">.</span><span class="n">copy_outputs_to_cpu</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>The input data is on a device, users directly use the input. The output data is on CPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># X is numpy array on cpu</span>
<span class="n">X_ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">io_binding</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">device_name</span><span class="p">(),</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">element_type</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">shape</span><span class="p">(),</span> <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">())</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_output</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">io_binding</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">io_binding</span><span class="o">.</span><span class="n">copy_outputs_to_cpu</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>The input data and output data are both on a device, users directly use the input and also place output on the device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#X is numpy array on cpu</span>
<span class="n">X_ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">Y_ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_shape_and_type</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Change the shape to the actual shape of the output being bound</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">io_binding</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_input</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">,</span>
        <span class="n">device_type</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">device_name</span><span class="p">(),</span>
        <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">element_type</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">shape</span><span class="p">(),</span>
        <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_output</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">,</span>
        <span class="n">device_type</span><span class="o">=</span><span class="n">Y_ortvalue</span><span class="o">.</span><span class="n">device_name</span><span class="p">(),</span>
        <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">element_type</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">Y_ortvalue</span><span class="o">.</span><span class="n">shape</span><span class="p">(),</span>
        <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">Y_ortvalue</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">io_binding</span><span class="p">)</span>
</pre></div>
</div>
<p>Users can request <em>ONNX Runtime</em> to allocate an output on a device. This is particularly useful for dynamic shaped outputs.
Users can use the <em>get_outputs()</em> API to get access to the <em>OrtValue</em> (s) corresponding to the allocated output(s).
Users can thus consume the <em>ONNX Runtime</em> allocated memory for the output as an <em>OrtValue</em>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#X is numpy array on cpu</span>
<span class="n">X_ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">io_binding</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_input</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">,</span>
        <span class="n">device_type</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">device_name</span><span class="p">(),</span>
        <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">element_type</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">shape</span><span class="p">(),</span>
        <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">X_ortvalue</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
<span class="p">)</span>
<span class="c1">#Request ONNX Runtime to bind and allocate memory on CUDA for &#39;output&#39;</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_output</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">io_binding</span><span class="p">)</span>
<span class="c1"># The following call returns an OrtValue which has data allocated by ONNX Runtime on CUDA</span>
<span class="n">ort_output</span> <span class="o">=</span> <span class="n">io_binding</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>In addition, <em>ONNX Runtime</em> supports directly working with <em>OrtValue</em> (s) while inferencing a model if provided as part of the input feed.</p>
<p>Users can bind <em>OrtValue</em> (s) directly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#X is numpy array on cpu</span>
<span class="c1">#X is numpy array on cpu</span>
<span class="n">X_ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">Y_ortvalue</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">OrtValue</span><span class="o">.</span><span class="n">ortvalue_from_shape_and_type</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Change the shape to the actual shape of the output being bound</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span>
        <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">io_binding</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_ortvalue_input</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">X_ortvalue</span><span class="p">)</span>
<span class="n">io_binding</span><span class="o">.</span><span class="n">bind_ortvalue_output</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">Y_ortvalue</span><span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">io_binding</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also bind inputs and outputs directly to a PyTorch tensor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># X is a PyTorch tensor on device</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s1">&#39;model.onnx&#39;</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">]))</span>
<span class="n">binding</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>

<span class="n">X_tensor</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

<span class="n">binding</span><span class="o">.</span><span class="n">bind_input</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span>
    <span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
    <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">element_type</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">x_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
    <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">x_tensor</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span>
    <span class="p">)</span>

<span class="c1">## Allocate the PyTorch tensor for the model output</span>
<span class="n">Y_shape</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># You need to specify the output PyTorch tensor shape</span>
<span class="n">Y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">Y_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
<span class="n">binding</span><span class="o">.</span><span class="n">bind_output</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span>
    <span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
    <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">element_type</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">Y_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
    <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">Y_tensor</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">session</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">binding</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also see code examples of this API in in the <a class="reference external" href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/python/api/onnxruntime-python-api.py">ONNX Runtime inferences examples</a>.</p>
</section>
</section>
</section>
<section id="api-details">
<h2>API Details<a class="headerlink" href="#api-details" title="Permalink to this heading">#</a></h2>
<section id="inferencesession">
<h3>InferenceSession<a class="headerlink" href="#inferencesession" title="Permalink to this heading">#</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">InferenceSession</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_or_bytes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">providers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">provider_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#InferenceSession"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.InferenceSession" title="Permalink to this definition">#</a></dt>
<dd><p>This is the main class used to run a model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path_or_bytes</strong> – filename or serialized ONNX or ORT format model in a byte string</p></li>
<li><p><strong>sess_options</strong> – session options</p></li>
<li><p><strong>providers</strong> – Optional sequence of providers in order of decreasing
precedence. Values can either be provider names or tuples of
(provider name, options dict). If not provided, then all available
providers are used with the default precedence.</p></li>
<li><p><strong>provider_options</strong> – Optional sequence of options dicts corresponding
to the providers listed in ‘providers’.</p></li>
</ul>
</dd>
</dl>
<p>The model type will be inferred unless explicitly set in the SessionOptions.
To explicitly set:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">so</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>
<span class="c1"># so.add_session_config_entry(&#39;session.load_model_format&#39;, &#39;ONNX&#39;) or</span>
<span class="n">so</span><span class="o">.</span><span class="n">add_session_config_entry</span><span class="p">(</span><span class="s1">&#39;session.load_model_format&#39;</span><span class="p">,</span> <span class="s1">&#39;ORT&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>A file extension of ‘.ort’ will be inferred as an ORT format model.
All other filenames are assumed to be ONNX format models.</p>
<p>‘providers’ can contain either names or names and options. When any options
are given in ‘providers’, ‘provider_options’ should not be used.</p>
<p>The list of providers is ordered by precedence. For example
<cite>[‘CUDAExecutionProvider’, ‘CPUExecutionProvider’]</cite>
means execute a node using <cite>CUDAExecutionProvider</cite>
if capable, otherwise execute using <cite>CPUExecutionProvider</cite>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.disable_fallback">
<span class="sig-name descname"><span class="pre">disable_fallback</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.disable_fallback" title="Permalink to this definition">#</a></dt>
<dd><p>Disable session.run() fallback mechanism.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.enable_fallback">
<span class="sig-name descname"><span class="pre">enable_fallback</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.enable_fallback" title="Permalink to this definition">#</a></dt>
<dd><p>Enable session.Run() fallback mechanism. If session.Run() fails due to an internal Execution Provider failure,
reset the Execution Providers enabled for this session.
If GPU is enabled, fall back to CUDAExecutionProvider.
otherwise fall back to CPUExecutionProvider.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.end_profiling">
<span class="sig-name descname"><span class="pre">end_profiling</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.end_profiling" title="Permalink to this definition">#</a></dt>
<dd><p>End profiling and return results in a file.</p>
<p>The results are stored in a filename if the option
<a class="reference internal" href="#onnxruntime.SessionOptions.enable_profiling" title="onnxruntime.SessionOptions.enable_profiling"><code class="xref py py-meth docutils literal notranslate"><span class="pre">onnxruntime.SessionOptions.enable_profiling()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_inputs">
<span class="sig-name descname"><span class="pre">get_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_inputs" title="Permalink to this definition">#</a></dt>
<dd><p>Return the inputs metadata as a list of <a class="reference internal" href="#onnxruntime.NodeArg" title="onnxruntime.NodeArg"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.NodeArg</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_modelmeta">
<span class="sig-name descname"><span class="pre">get_modelmeta</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_modelmeta" title="Permalink to this definition">#</a></dt>
<dd><p>Return the metadata. See <a class="reference internal" href="#onnxruntime.ModelMetadata" title="onnxruntime.ModelMetadata"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.ModelMetadata</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_outputs">
<span class="sig-name descname"><span class="pre">get_outputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_outputs" title="Permalink to this definition">#</a></dt>
<dd><p>Return the outputs metadata as a list of <a class="reference internal" href="#onnxruntime.NodeArg" title="onnxruntime.NodeArg"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.NodeArg</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_overridable_initializers">
<span class="sig-name descname"><span class="pre">get_overridable_initializers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_overridable_initializers" title="Permalink to this definition">#</a></dt>
<dd><p>Return the inputs (including initializers) metadata as a list of <a class="reference internal" href="#onnxruntime.NodeArg" title="onnxruntime.NodeArg"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.NodeArg</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_profiling_start_time_ns">
<span class="sig-name descname"><span class="pre">get_profiling_start_time_ns</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_profiling_start_time_ns" title="Permalink to this definition">#</a></dt>
<dd><p>Return the nanoseconds of profiling’s start time
Comparable to time.monotonic_ns() after Python 3.3
On some platforms, this timer may not be as precise as nanoseconds
For instance, on Windows and MacOS, the precision will be ~100ns</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_provider_options">
<span class="sig-name descname"><span class="pre">get_provider_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_provider_options" title="Permalink to this definition">#</a></dt>
<dd><p>Return registered execution providers’ configurations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_providers">
<span class="sig-name descname"><span class="pre">get_providers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_providers" title="Permalink to this definition">#</a></dt>
<dd><p>Return list of registered execution providers.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.get_session_options">
<span class="sig-name descname"><span class="pre">get_session_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.get_session_options" title="Permalink to this definition">#</a></dt>
<dd><p>Return the session options. See <a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.SessionOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.SessionOptions</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.io_binding">
<span class="sig-name descname"><span class="pre">io_binding</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.io_binding" title="Permalink to this definition">#</a></dt>
<dd><p>Return an onnxruntime.IOBinding object`.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.run" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_names</strong> – name of the outputs</p></li>
<li><p><strong>input_feed</strong> – dictionary <code class="docutils literal notranslate"><span class="pre">{</span> <span class="pre">input_name:</span> <span class="pre">input_value</span> <span class="pre">}</span></code></p></li>
<li><p><strong>run_options</strong> – See <a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.RunOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.RunOptions</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>list of results, every result is either a numpy array,
a sparse tensor, a list or a dictionary.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">output_name</span><span class="p">],</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.run_with_iobinding">
<span class="sig-name descname"><span class="pre">run_with_iobinding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iobinding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.run_with_iobinding" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>iobinding</strong> – the iobinding object that has graph inputs/outputs bind.</p></li>
<li><p><strong>run_options</strong> – See <a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.RunOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.RunOptions</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.run_with_ort_values">
<span class="sig-name descname"><span class="pre">run_with_ort_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dict_ort_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.run_with_ort_values" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_names</strong> – name of the outputs</p></li>
<li><p><strong>input_dict_ort_values</strong> – dictionary <code class="docutils literal notranslate"><span class="pre">{</span> <span class="pre">input_name:</span> <span class="pre">input_ort_value</span> <span class="pre">}</span></code>
See <code class="docutils literal notranslate"><span class="pre">OrtValue</span></code> class how to create <cite>OrtValue</cite>
from numpy array or <cite>SparseTensor</cite></p></li>
<li><p><strong>run_options</strong> – See <a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.RunOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.RunOptions</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>an array of <cite>OrtValue</cite></p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">output_name</span><span class="p">],</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.run_with_ortvaluevector">
<span class="sig-name descname"><span class="pre">run_with_ortvaluevector</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">run_options</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feeds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fetch_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fetches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fetch_devices</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.run_with_ortvaluevector" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the predictions similar to other run_*() methods but with minimal C++/Python conversion overhead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>run_options</strong> – See <a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.RunOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.RunOptions</span></code></a>.</p></li>
<li><p><strong>feed_names</strong> – list of input names.</p></li>
<li><p><strong>feeds</strong> – list of input OrtValue.</p></li>
<li><p><strong>fetch_names</strong> – list of output names.</p></li>
<li><p><strong>fetches</strong> – list of output OrtValue.</p></li>
<li><p><strong>fetch_devices</strong> – list of output devices.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.InferenceSession.set_providers">
<span class="sig-name descname"><span class="pre">set_providers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">providers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">provider_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.InferenceSession.set_providers" title="Permalink to this definition">#</a></dt>
<dd><p>Register the input list of execution providers. The underlying session is re-created.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>providers</strong> – Optional sequence of providers in order of decreasing
precedence. Values can either be provider names or tuples of
(provider name, options dict). If not provided, then all available
providers are used with the default precedence.</p></li>
<li><p><strong>provider_options</strong> – Optional sequence of options dicts corresponding
to the providers listed in ‘providers’.</p></li>
</ul>
</dd>
</dl>
<p>‘providers’ can contain either names or names and options. When any options
are given in ‘providers’, ‘provider_options’ should not be used.</p>
<p>The list of providers is ordered by precedence. For example
<cite>[‘CUDAExecutionProvider’, ‘CPUExecutionProvider’]</cite>
means execute a node using CUDAExecutionProvider if capable,
otherwise execute using CPUExecutionProvider.</p>
</dd></dl>

</dd></dl>

</section>
<section id="options">
<h3>Options<a class="headerlink" href="#options" title="Permalink to this heading">#</a></h3>
<section id="runoptions">
<h4>RunOptions<a class="headerlink" href="#runoptions" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.RunOptions">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">RunOptions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.RunOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.RunOptions</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.RunOptions" title="Permalink to this definition">#</a></dt>
<dd><p>Configuration information for a single Run.</p>
<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.add_run_config_entry">
<span class="sig-name descname"><span class="pre">add_run_config_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.RunOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.RunOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.RunOptions.add_run_config_entry" title="Permalink to this definition">#</a></dt>
<dd><p>Set a single run configuration entry as a pair of strings.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.get_run_config_entry">
<span class="sig-name descname"><span class="pre">get_run_config_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.RunOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.RunOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></span><a class="headerlink" href="#onnxruntime.RunOptions.get_run_config_entry" title="Permalink to this definition">#</a></dt>
<dd><p>Get a single run configuration value using the given configuration key.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.log_severity_level">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_severity_level</span></span><a class="headerlink" href="#onnxruntime.RunOptions.log_severity_level" title="Permalink to this definition">#</a></dt>
<dd><p>Log severity level for a particular Run() invocation. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. Default is 2.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.log_verbosity_level">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_verbosity_level</span></span><a class="headerlink" href="#onnxruntime.RunOptions.log_verbosity_level" title="Permalink to this definition">#</a></dt>
<dd><p>VLOG level if DEBUG build and run_log_severity_level is 0.
Applies to a particular Run() invocation. Default is 0.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.logid">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logid</span></span><a class="headerlink" href="#onnxruntime.RunOptions.logid" title="Permalink to this definition">#</a></dt>
<dd><p>To identify logs generated by a particular Run() invocation.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.only_execute_path_to_fetches">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">only_execute_path_to_fetches</span></span><a class="headerlink" href="#onnxruntime.RunOptions.only_execute_path_to_fetches" title="Permalink to this definition">#</a></dt>
<dd><p>Only execute the nodes needed by fetch list</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.RunOptions.terminate">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">terminate</span></span><a class="headerlink" href="#onnxruntime.RunOptions.terminate" title="Permalink to this definition">#</a></dt>
<dd><p>Set to True to terminate any currently executing calls that are using this
RunOptions instance. The individual calls will exit gracefully and return an error status.</p>
</dd></dl>

</dd></dl>

</section>
<section id="sessionoptions">
<h4>SessionOptions<a class="headerlink" href="#sessionoptions" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">SessionOptions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.SessionOptions" title="Permalink to this definition">#</a></dt>
<dd><p>Configuration information for a session.</p>
<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.add_external_initializers">
<span class="sig-name descname"><span class="pre">add_external_initializers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.11)"><span class="pre">list</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.11)"><span class="pre">list</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.add_external_initializers" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.add_free_dimension_override_by_denotation">
<span class="sig-name descname"><span class="pre">add_free_dimension_override_by_denotation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.add_free_dimension_override_by_denotation" title="Permalink to this definition">#</a></dt>
<dd><p>Specify the dimension size for each denotation associated with an input’s free dimension.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.add_free_dimension_override_by_name">
<span class="sig-name descname"><span class="pre">add_free_dimension_override_by_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.add_free_dimension_override_by_name" title="Permalink to this definition">#</a></dt>
<dd><p>Specify values of named dimensions within model inputs.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.add_initializer">
<span class="sig-name descname"><span class="pre">add_initializer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.11)"><span class="pre">object</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.add_initializer" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.add_session_config_entry">
<span class="sig-name descname"><span class="pre">add_session_config_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.add_session_config_entry" title="Permalink to this definition">#</a></dt>
<dd><p>Set a single session configuration entry as a pair of strings.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.enable_cpu_mem_arena">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">enable_cpu_mem_arena</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.enable_cpu_mem_arena" title="Permalink to this definition">#</a></dt>
<dd><p>Enables the memory arena on CPU. Arena may pre-allocate memory for future usage.
Set this option to false if you don’t want it. Default is True.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.enable_mem_pattern">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">enable_mem_pattern</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.enable_mem_pattern" title="Permalink to this definition">#</a></dt>
<dd><p>Enable the memory pattern optimization. Default is true.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.enable_mem_reuse">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">enable_mem_reuse</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.enable_mem_reuse" title="Permalink to this definition">#</a></dt>
<dd><p>Enable the memory reuse optimization. Default is true.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.enable_profiling">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">enable_profiling</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.enable_profiling" title="Permalink to this definition">#</a></dt>
<dd><p>Enable profiling for this session. Default is false.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.execution_mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">execution_mode</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.execution_mode" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the execution mode. Default is sequential.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.execution_order">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">execution_order</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.execution_order" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the execution order. Default is basic topological order.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.get_session_config_entry">
<span class="sig-name descname"><span class="pre">get_session_config_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.get_session_config_entry" title="Permalink to this definition">#</a></dt>
<dd><p>Get a single session configuration value using the given configuration key.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.graph_optimization_level">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">graph_optimization_level</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.graph_optimization_level" title="Permalink to this definition">#</a></dt>
<dd><p>Graph optimization level for this session.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.inter_op_num_threads">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">inter_op_num_threads</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.inter_op_num_threads" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the number of threads used to parallelize the execution of the graph (across nodes). Default is 0 to let onnxruntime choose.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.intra_op_num_threads">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">intra_op_num_threads</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.intra_op_num_threads" title="Permalink to this definition">#</a></dt>
<dd><p>Sets the number of threads used to parallelize the execution within nodes. Default is 0 to let onnxruntime choose.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.log_severity_level">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_severity_level</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.log_severity_level" title="Permalink to this definition">#</a></dt>
<dd><p>Log severity level. Applies to session load, initialization, etc.
0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. Default is 2.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.log_verbosity_level">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_verbosity_level</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.log_verbosity_level" title="Permalink to this definition">#</a></dt>
<dd><p>VLOG level if DEBUG build and session_log_severity_level is 0.
Applies to session load, initialization, etc. Default is 0.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.logid">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logid</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.logid" title="Permalink to this definition">#</a></dt>
<dd><p>Logger id to use for session output.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.optimized_model_filepath">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">optimized_model_filepath</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.optimized_model_filepath" title="Permalink to this definition">#</a></dt>
<dd><p>File path to serialize optimized model to.
Optimized model is not serialized unless optimized_model_filepath is set.
Serialized model format will default to ONNX unless:
- add_session_config_entry is used to set ‘session.save_model_format’ to ‘ORT’, or
- there is no ‘session.save_model_format’ config entry and optimized_model_filepath ends in ‘.ort’ (case insensitive)</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.profile_file_prefix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">profile_file_prefix</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.profile_file_prefix" title="Permalink to this definition">#</a></dt>
<dd><p>The prefix of the profile file. The current time will be appended to the file name.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.register_custom_ops_library">
<span class="sig-name descname"><span class="pre">register_custom_ops_library</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions"><span class="pre">onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.11)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#onnxruntime.SessionOptions.register_custom_ops_library" title="Permalink to this definition">#</a></dt>
<dd><p>Specify the path to the shared library containing the custom op kernels required to run a model.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.SessionOptions.use_deterministic_compute">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">use_deterministic_compute</span></span><a class="headerlink" href="#onnxruntime.SessionOptions.use_deterministic_compute" title="Permalink to this definition">#</a></dt>
<dd><p>Whether to use deterministic compute. Default is false.</p>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this heading">#</a></h3>
<section id="ortvalue">
<h4>OrtValue<a class="headerlink" href="#ortvalue" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.OrtValue">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">OrtValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ortvalue</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numpy_obj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue" title="Permalink to this definition">#</a></dt>
<dd><p>A data structure that supports all ONNX data formats (tensors and non-tensors) that allows users
to place the data backing these on a device, for example, on a CUDA supported device.
This class provides APIs to construct and deal with OrtValues.</p>
<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.as_sparse_tensor">
<span class="sig-name descname"><span class="pre">as_sparse_tensor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.as_sparse_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.as_sparse_tensor" title="Permalink to this definition">#</a></dt>
<dd><p>The function will return SparseTensor contained in this OrtValue</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.data_ptr">
<span class="sig-name descname"><span class="pre">data_ptr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.data_ptr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.data_ptr" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the address of the first element in the OrtValue’s data buffer</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.data_type">
<span class="sig-name descname"><span class="pre">data_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.data_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.data_type" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the data type of the data in the OrtValue</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.device_name">
<span class="sig-name descname"><span class="pre">device_name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.device_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.device_name" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the name of the device where the OrtValue’s data buffer resides e.g. cpu, cuda</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.element_type">
<span class="sig-name descname"><span class="pre">element_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.element_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.element_type" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the proto type of the data in the OrtValue
if the OrtValue is a tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.has_value">
<span class="sig-name descname"><span class="pre">has_value</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.has_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.has_value" title="Permalink to this definition">#</a></dt>
<dd><p>Returns True if the OrtValue corresponding to an
optional type contains data, else returns False</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.is_sparse_tensor">
<span class="sig-name descname"><span class="pre">is_sparse_tensor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.is_sparse_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.is_sparse_tensor" title="Permalink to this definition">#</a></dt>
<dd><p>Returns True if the OrtValue contains a SparseTensor, else returns False</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.is_tensor">
<span class="sig-name descname"><span class="pre">is_tensor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.is_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.is_tensor" title="Permalink to this definition">#</a></dt>
<dd><p>Returns True if the OrtValue contains a Tensor, else returns False</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.is_tensor_sequence">
<span class="sig-name descname"><span class="pre">is_tensor_sequence</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.is_tensor_sequence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.is_tensor_sequence" title="Permalink to this definition">#</a></dt>
<dd><p>Returns True if the OrtValue contains a Tensor Sequence, else returns False</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.numpy">
<span class="sig-name descname"><span class="pre">numpy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.numpy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.numpy" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a Numpy object from the OrtValue.
Valid only for OrtValues holding Tensors. Throws for OrtValues holding non-Tensors.
Use accessors to gain a reference to non-Tensor objects such as SparseTensor</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.ort_value_from_sparse_tensor">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ort_value_from_sparse_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.ort_value_from_sparse_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.ort_value_from_sparse_tensor" title="Permalink to this definition">#</a></dt>
<dd><p>The function will construct an OrtValue instance from a valid SparseTensor
The new instance of OrtValue will assume the ownership of sparse_tensor</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.ortvalue_from_numpy">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ortvalue_from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numpy_obj</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.ortvalue_from_numpy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.ortvalue_from_numpy" title="Permalink to this definition">#</a></dt>
<dd><p>Factory method to construct an OrtValue (which holds a Tensor) from a given Numpy object
A copy of the data in the Numpy object is held by the OrtValue only if the device is NOT cpu</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>numpy_obj</strong> – The Numpy object to construct the OrtValue from</p></li>
<li><p><strong>device_type</strong> – e.g. cpu, cuda, cpu by default</p></li>
<li><p><strong>device_id</strong> – device id, e.g. 0</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.ortvalue_from_shape_and_type">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ortvalue_from_shape_and_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">element_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.ortvalue_from_shape_and_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.ortvalue_from_shape_and_type" title="Permalink to this definition">#</a></dt>
<dd><p>Factory method to construct an OrtValue (which holds a Tensor) from given shape and element_type</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> – List of integers indicating the shape of the OrtValue</p></li>
<li><p><strong>element_type</strong> – The data type of the elements in the OrtValue (numpy type)</p></li>
<li><p><strong>device_type</strong> – e.g. cpu, cuda, cpu by default</p></li>
<li><p><strong>device_id</strong> – device id, e.g. 0</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.shape">
<span class="sig-name descname"><span class="pre">shape</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.shape" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the shape of the data in the OrtValue</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.OrtValue.update_inplace">
<span class="sig-name descname"><span class="pre">update_inplace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">np_arr</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtValue.update_inplace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtValue.update_inplace" title="Permalink to this definition">#</a></dt>
<dd><p>Update the OrtValue in place with a new Numpy array. The numpy contents
are copied over to the device memory backing the OrtValue. It can be used
to update the input valuess for an InferenceSession with CUDA graph
enabled or other scenarios where the OrtValue needs to be updated while
the memory address can not be changed.</p>
</dd></dl>

</dd></dl>

</section>
<section id="sparsetensor">
<h4>SparseTensor<a class="headerlink" href="#sparsetensor" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">SparseTensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse_tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor" title="Permalink to this definition">#</a></dt>
<dd><p>A data structure that project the C++ SparseTensor object
The class provides API to work with the object.
Depending on the format, the class will hold more than one buffer
depending on the format</p>
<p>Internal constructor</p>
<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.as_blocksparse_view">
<span class="sig-name descname"><span class="pre">as_blocksparse_view</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.as_blocksparse_view"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.as_blocksparse_view" title="Permalink to this definition">#</a></dt>
<dd><p>The method will return coo representation of the sparse tensor which will enable
querying BlockSparse indices. If the instance did not contain BlockSparse format, it would throw.
You can query coo indices as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">block_sparse_indices</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">as_blocksparse_view</span><span class="p">()</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</pre></div>
</div>
<p>which will return a numpy array that is backed by the native memory</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.as_coo_view">
<span class="sig-name descname"><span class="pre">as_coo_view</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.as_coo_view"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.as_coo_view" title="Permalink to this definition">#</a></dt>
<dd><p>The method will return coo representation of the sparse tensor which will enable
querying COO indices. If the instance did not contain COO format, it would throw.
You can query coo indices as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">coo_indices</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">as_coo_view</span><span class="p">()</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</pre></div>
</div>
<p>which will return a numpy array that is backed by the native memory.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.as_csrc_view">
<span class="sig-name descname"><span class="pre">as_csrc_view</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.as_csrc_view"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.as_csrc_view" title="Permalink to this definition">#</a></dt>
<dd><p>The method will return CSR(C) representation of the sparse tensor which will enable
querying CRS(C) indices. If the instance dit not contain CSR(C) format, it would throw.
You can query indices as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inner_ndices</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">as_csrc_view</span><span class="p">()</span><span class="o">.</span><span class="n">inner</span><span class="p">()</span>
<span class="n">outer_ndices</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">as_csrc_view</span><span class="p">()</span><span class="o">.</span><span class="n">outer</span><span class="p">()</span>
</pre></div>
</div>
<p>returning numpy arrays backed by the native memory.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.data_type">
<span class="sig-name descname"><span class="pre">data_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.data_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.data_type" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a string data type of the data in the OrtValue</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.dense_shape">
<span class="sig-name descname"><span class="pre">dense_shape</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.dense_shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.dense_shape" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a numpy array(int64) containing a dense shape of a sparse tensor</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.device_name">
<span class="sig-name descname"><span class="pre">device_name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.device_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.device_name" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the name of the device where the SparseTensor data buffers reside e.g. cpu, cuda</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.format">
<span class="sig-name descname"><span class="pre">format</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.format"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.format" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a OrtSparseFormat enumeration</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.sparse_coo_from_numpy">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sparse_coo_from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coo_indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ort_device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.sparse_coo_from_numpy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.sparse_coo_from_numpy" title="Permalink to this definition">#</a></dt>
<dd><p>Factory method to construct a SparseTensor in COO format from given arguments</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dense_shape</strong> – 1-D  numpy array(int64) or a python list that contains a dense_shape of the sparse tensor
must be on cpu memory</p></li>
<li><p><strong>values</strong> – a homogeneous, contiguous 1-D numpy array that contains non-zero elements of the tensor
of a type.</p></li>
<li><p><strong>coo_indices</strong> – contiguous numpy array(int64) that contains COO indices for the tensor. coo_indices may
have a 1-D shape when it contains a linear index of non-zero values and its length must be equal to
that of the values. It can also be of 2-D shape, in which has it contains pairs of coordinates for
each of the nnz values and its length must be exactly twice of the values length.</p></li>
<li><p><strong>ort_device</strong> – <ul>
<li><p>describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is</p></li>
</ul>
<p>suppored for non-numeric data types.</p>
</p></li>
</ul>
</dd>
</dl>
<p>For primitive types, the method will map values and coo_indices arrays into native memory and will use
them as backing storage. It will increment the reference count for numpy arrays and it will decrement it
on GC. The buffers may reside in any storage either CPU or GPU.
For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
on other devices and their memory can not be mapped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.sparse_csr_from_numpy">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sparse_csr_from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outer_indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ort_device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.sparse_csr_from_numpy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.sparse_csr_from_numpy" title="Permalink to this definition">#</a></dt>
<dd><p>Factory method to construct a SparseTensor in CSR format from given arguments</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dense_shape</strong> – 1-D numpy array(int64) or a python list that contains a dense_shape of the
sparse tensor (rows, cols) must be on cpu memory</p></li>
<li><p><strong>values</strong> – a  contiguous, homogeneous 1-D numpy array that contains non-zero elements of the tensor
of a type.</p></li>
<li><p><strong>inner_indices</strong> – contiguous 1-D numpy array(int64) that contains CSR inner indices for the tensor.
Its length must be equal to that of the values.</p></li>
<li><p><strong>outer_indices</strong> – contiguous 1-D numpy array(int64) that contains CSR outer indices for the tensor.
Its length must be equal to the number of rows + 1.</p></li>
<li><p><strong>ort_device</strong> – <ul>
<li><p>describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is</p></li>
</ul>
<p>suppored for non-numeric data types.</p>
</p></li>
</ul>
</dd>
</dl>
<p>For primitive types, the method will map values and indices arrays into native memory and will use them as
backing storage. It will increment the reference count and it will decrement then count when it is GCed.
The buffers may reside in any storage either CPU or GPU.
For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
on other devices and their memory can not be mapped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.to_cuda">
<span class="sig-name descname"><span class="pre">to_cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ort_device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.to_cuda"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.to_cuda" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a copy of this instance on the specified cuda device</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ort_device</strong> – with name ‘cuda’ and valid gpu device id</p>
</dd>
</dl>
<p>The method will throw if:</p>
<ul class="simple">
<li><p>this instance contains strings</p></li>
<li><p>this instance is already on GPU. Cross GPU copy is not supported</p></li>
<li><p>CUDA is not present in this build</p></li>
<li><p>if the specified device is not valid</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.SparseTensor.values">
<span class="sig-name descname"><span class="pre">values</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#SparseTensor.values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.SparseTensor.values" title="Permalink to this definition">#</a></dt>
<dd><p>The method returns a numpy array that is backed by the native memory
if the data type is numeric. Otherwise, the returned numpy array that contains
copies of the strings.</p>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="devices">
<h3>Devices<a class="headerlink" href="#devices" title="Permalink to this heading">#</a></h3>
<section id="iobinding">
<h4>IOBinding<a class="headerlink" href="#iobinding" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.IOBinding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">IOBinding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">session</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding" title="Permalink to this definition">#</a></dt>
<dd><p>This class provides API to bind input/output to a specified device, e.g. GPU.</p>
<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.bind_cpu_input">
<span class="sig-name descname"><span class="pre">bind_cpu_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arr_on_cpu</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.bind_cpu_input"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.bind_cpu_input" title="Permalink to this definition">#</a></dt>
<dd><p>bind an input to array on CPU
:param name: input name
:param arr_on_cpu: input values as a python array on CPU</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.bind_input">
<span class="sig-name descname"><span class="pre">bind_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">element_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_ptr</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.bind_input"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.bind_input" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – input name</p></li>
<li><p><strong>device_type</strong> – e.g. cpu, cuda</p></li>
<li><p><strong>device_id</strong> – device id, e.g. 0</p></li>
<li><p><strong>element_type</strong> – input element type</p></li>
<li><p><strong>shape</strong> – input shape</p></li>
<li><p><strong>buffer_ptr</strong> – memory pointer to input data</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.bind_ortvalue_input">
<span class="sig-name descname"><span class="pre">bind_ortvalue_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ortvalue</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.bind_ortvalue_input"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.bind_ortvalue_input" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – input name</p></li>
<li><p><strong>ortvalue</strong> – OrtValue instance to bind</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.bind_ortvalue_output">
<span class="sig-name descname"><span class="pre">bind_ortvalue_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ortvalue</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.bind_ortvalue_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.bind_ortvalue_output" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – output name</p></li>
<li><p><strong>ortvalue</strong> – OrtValue instance to bind</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.bind_output">
<span class="sig-name descname"><span class="pre">bind_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">element_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_ptr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.bind_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.bind_output" title="Permalink to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – output name</p></li>
<li><p><strong>device_type</strong> – e.g. cpu, cuda, cpu by default</p></li>
<li><p><strong>device_id</strong> – device id, e.g. 0</p></li>
<li><p><strong>element_type</strong> – output element type</p></li>
<li><p><strong>shape</strong> – output shape</p></li>
<li><p><strong>buffer_ptr</strong> – memory pointer to output data</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.copy_outputs_to_cpu">
<span class="sig-name descname"><span class="pre">copy_outputs_to_cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.copy_outputs_to_cpu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.copy_outputs_to_cpu" title="Permalink to this definition">#</a></dt>
<dd><p>Copy output contents to CPU (if on another device). No-op if already on the CPU.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="onnxruntime.IOBinding.get_outputs">
<span class="sig-name descname"><span class="pre">get_outputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#IOBinding.get_outputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.IOBinding.get_outputs" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the output OrtValues from the Run() that preceded the call.
The data buffer of the obtained OrtValues may not reside on CPU memory</p>
</dd></dl>

</dd></dl>

</section>
<section id="ortdevice">
<h4>OrtDevice<a class="headerlink" href="#ortdevice" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.OrtDevice">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">OrtDevice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">c_ort_device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onnxruntime/capi/onnxruntime_inference_collection.html#OrtDevice"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#onnxruntime.OrtDevice" title="Permalink to this definition">#</a></dt>
<dd><p>A data structure that exposes the underlying C++ OrtDevice</p>
<p>Internal constructor</p>
</dd></dl>

</section>
</section>
<section id="internal-classes">
<h3>Internal classes<a class="headerlink" href="#internal-classes" title="Permalink to this heading">#</a></h3>
<p>These classes cannot be instantiated by users but they are returned
by methods or functions of this library.</p>
<section id="modelmetadata">
<h4>ModelMetadata<a class="headerlink" href="#modelmetadata" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">ModelMetadata</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata" title="Permalink to this definition">#</a></dt>
<dd><p>Pre-defined and custom metadata about the model.
It is usually used to identify the model used to run the prediction and
facilitate the comparison.</p>
<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.custom_metadata_map">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">custom_metadata_map</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.custom_metadata_map" title="Permalink to this definition">#</a></dt>
<dd><p>additional metadata</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.description">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">description</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.description" title="Permalink to this definition">#</a></dt>
<dd><p>description of the model</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.domain">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">domain</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.domain" title="Permalink to this definition">#</a></dt>
<dd><p>ONNX domain</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.graph_description">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">graph_description</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.graph_description" title="Permalink to this definition">#</a></dt>
<dd><p>description of the graph hosted in the model</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.graph_name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">graph_name</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.graph_name" title="Permalink to this definition">#</a></dt>
<dd><p>graph name</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.producer_name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">producer_name</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.producer_name" title="Permalink to this definition">#</a></dt>
<dd><p>producer name</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.ModelMetadata.version">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">version</span></span><a class="headerlink" href="#onnxruntime.ModelMetadata.version" title="Permalink to this definition">#</a></dt>
<dd><p>version of the model</p>
</dd></dl>

</dd></dl>

</section>
<section id="nodearg">
<h4>NodeArg<a class="headerlink" href="#nodearg" title="Permalink to this heading">#</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="onnxruntime.NodeArg">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">onnxruntime.</span></span><span class="sig-name descname"><span class="pre">NodeArg</span></span><a class="headerlink" href="#onnxruntime.NodeArg" title="Permalink to this definition">#</a></dt>
<dd><p>Node argument definition, for both input and output,
including arg name, arg type (contains both type and shape).</p>
<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.NodeArg.name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#onnxruntime.NodeArg.name" title="Permalink to this definition">#</a></dt>
<dd><p>node name</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.NodeArg.shape">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">shape</span></span><a class="headerlink" href="#onnxruntime.NodeArg.shape" title="Permalink to this definition">#</a></dt>
<dd><p>node shape (assuming the node holds a tensor)</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="onnxruntime.NodeArg.type">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">type</span></span><a class="headerlink" href="#onnxruntime.NodeArg.type" title="Permalink to this definition">#</a></dt>
<dd><p>node type</p>
</dd></dl>

</dd></dl>

</section>
</section>
</section>
<section id="backend">
<h2>Backend<a class="headerlink" href="#backend" title="Permalink to this heading">#</a></h2>
<p>In addition to the regular API which is optimized for performance and usability,
<em>ONNX Runtime</em> also implements the
<a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/ImplementingAnOnnxBackend.md">ONNX backend API</a>
for verification of <em>ONNX</em> specification conformance.
The following functions are supported:</p>
<dl class="py function">
<dt class="sig sig-object py" id="onnxruntime.backend.is_compatible">
<span class="sig-prename descclassname"><span class="pre">onnxruntime.backend.</span></span><span class="sig-name descname"><span class="pre">is_compatible</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.backend.is_compatible" title="Permalink to this definition">#</a></dt>
<dd><p>Return whether the model is compatible with the backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – unused</p></li>
<li><p><strong>device</strong> – None to use the default device or a string (ex: <cite>‘CPU’</cite>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>boolean</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="onnxruntime.backend.prepare">
<span class="sig-prename descclassname"><span class="pre">onnxruntime.backend.</span></span><span class="sig-name descname"><span class="pre">prepare</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.backend.prepare" title="Permalink to this definition">#</a></dt>
<dd><p>Load the model and creates a <a class="reference internal" href="#onnxruntime.InferenceSession" title="onnxruntime.InferenceSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.InferenceSession</span></code></a>
ready to be used as a backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – ModelProto (returned by <cite>onnx.load</cite>),
string for a filename or bytes for a serialized model</p></li>
<li><p><strong>device</strong> – requested device for the computation,
None means the default one which depends on
the compilation settings</p></li>
<li><p><strong>kwargs</strong> – see <a class="reference internal" href="#onnxruntime.SessionOptions" title="onnxruntime.SessionOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.SessionOptions</span></code></a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#onnxruntime.InferenceSession" title="onnxruntime.InferenceSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.InferenceSession</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="onnxruntime.backend.run">
<span class="sig-prename descclassname"><span class="pre">onnxruntime.backend.</span></span><span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.backend.run" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the prediction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – <a class="reference internal" href="#onnxruntime.InferenceSession" title="onnxruntime.InferenceSession"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.InferenceSession</span></code></a> returned
by function <em>prepare</em></p></li>
<li><p><strong>inputs</strong> – inputs</p></li>
<li><p><strong>device</strong> – requested device for the computation,
None means the default one which depends on
the compilation settings</p></li>
<li><p><strong>kwargs</strong> – see <a class="reference internal" href="#onnxruntime.RunOptions" title="onnxruntime.RunOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">onnxruntime.RunOptions</span></code></a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>predictions</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="onnxruntime.backend.supports_device">
<span class="sig-prename descclassname"><span class="pre">onnxruntime.backend.</span></span><span class="sig-name descname"><span class="pre">supports_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#onnxruntime.backend.supports_device" title="Permalink to this definition">#</a></dt>
<dd><p>Check whether the backend is compiled with particular device support.
In particular it’s used in the testing suite.</p>
</dd></dl>

</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="auto_examples/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Gallery of examples</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="tutorial.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Tutorial</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2018-2023, Microsoft
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">API</a><ul>
<li><a class="reference internal" href="#api-overview">API Overview</a><ul>
<li><a class="reference internal" href="#load-and-run-a-model">Load and run a model</a></li>
<li><a class="reference internal" href="#data-inputs-and-outputs">Data inputs and outputs</a><ul>
<li><a class="reference internal" href="#data-on-cpu">Data on CPU</a></li>
<li><a class="reference internal" href="#data-on-device">Data on device</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#api-details">API Details</a><ul>
<li><a class="reference internal" href="#inferencesession">InferenceSession</a><ul>
<li><a class="reference internal" href="#onnxruntime.InferenceSession"><code class="docutils literal notranslate"><span class="pre">InferenceSession</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.disable_fallback"><code class="docutils literal notranslate"><span class="pre">InferenceSession.disable_fallback()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.enable_fallback"><code class="docutils literal notranslate"><span class="pre">InferenceSession.enable_fallback()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.end_profiling"><code class="docutils literal notranslate"><span class="pre">InferenceSession.end_profiling()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_inputs"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_inputs()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_modelmeta"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_modelmeta()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_outputs"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_outputs()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_overridable_initializers"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_overridable_initializers()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_profiling_start_time_ns"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_profiling_start_time_ns()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_provider_options"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_provider_options()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_providers"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_providers()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.get_session_options"><code class="docutils literal notranslate"><span class="pre">InferenceSession.get_session_options()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.io_binding"><code class="docutils literal notranslate"><span class="pre">InferenceSession.io_binding()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.run"><code class="docutils literal notranslate"><span class="pre">InferenceSession.run()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.run_with_iobinding"><code class="docutils literal notranslate"><span class="pre">InferenceSession.run_with_iobinding()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.run_with_ort_values"><code class="docutils literal notranslate"><span class="pre">InferenceSession.run_with_ort_values()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.run_with_ortvaluevector"><code class="docutils literal notranslate"><span class="pre">InferenceSession.run_with_ortvaluevector()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.InferenceSession.set_providers"><code class="docutils literal notranslate"><span class="pre">InferenceSession.set_providers()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#options">Options</a><ul>
<li><a class="reference internal" href="#runoptions">RunOptions</a><ul>
<li><a class="reference internal" href="#onnxruntime.RunOptions"><code class="docutils literal notranslate"><span class="pre">RunOptions</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.RunOptions.add_run_config_entry"><code class="docutils literal notranslate"><span class="pre">RunOptions.add_run_config_entry()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.RunOptions.get_run_config_entry"><code class="docutils literal notranslate"><span class="pre">RunOptions.get_run_config_entry()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.RunOptions.log_severity_level"><code class="docutils literal notranslate"><span class="pre">RunOptions.log_severity_level</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.RunOptions.log_verbosity_level"><code class="docutils literal notranslate"><span class="pre">RunOptions.log_verbosity_level</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.RunOptions.logid"><code class="docutils literal notranslate"><span class="pre">RunOptions.logid</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.RunOptions.only_execute_path_to_fetches"><code class="docutils literal notranslate"><span class="pre">RunOptions.only_execute_path_to_fetches</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.RunOptions.terminate"><code class="docutils literal notranslate"><span class="pre">RunOptions.terminate</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#sessionoptions">SessionOptions</a><ul>
<li><a class="reference internal" href="#onnxruntime.SessionOptions"><code class="docutils literal notranslate"><span class="pre">SessionOptions</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.add_external_initializers"><code class="docutils literal notranslate"><span class="pre">SessionOptions.add_external_initializers()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.add_free_dimension_override_by_denotation"><code class="docutils literal notranslate"><span class="pre">SessionOptions.add_free_dimension_override_by_denotation()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.add_free_dimension_override_by_name"><code class="docutils literal notranslate"><span class="pre">SessionOptions.add_free_dimension_override_by_name()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.add_initializer"><code class="docutils literal notranslate"><span class="pre">SessionOptions.add_initializer()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.add_session_config_entry"><code class="docutils literal notranslate"><span class="pre">SessionOptions.add_session_config_entry()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.enable_cpu_mem_arena"><code class="docutils literal notranslate"><span class="pre">SessionOptions.enable_cpu_mem_arena</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.enable_mem_pattern"><code class="docutils literal notranslate"><span class="pre">SessionOptions.enable_mem_pattern</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.enable_mem_reuse"><code class="docutils literal notranslate"><span class="pre">SessionOptions.enable_mem_reuse</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.enable_profiling"><code class="docutils literal notranslate"><span class="pre">SessionOptions.enable_profiling</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.execution_mode"><code class="docutils literal notranslate"><span class="pre">SessionOptions.execution_mode</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.execution_order"><code class="docutils literal notranslate"><span class="pre">SessionOptions.execution_order</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.get_session_config_entry"><code class="docutils literal notranslate"><span class="pre">SessionOptions.get_session_config_entry()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.graph_optimization_level"><code class="docutils literal notranslate"><span class="pre">SessionOptions.graph_optimization_level</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.inter_op_num_threads"><code class="docutils literal notranslate"><span class="pre">SessionOptions.inter_op_num_threads</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.intra_op_num_threads"><code class="docutils literal notranslate"><span class="pre">SessionOptions.intra_op_num_threads</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.log_severity_level"><code class="docutils literal notranslate"><span class="pre">SessionOptions.log_severity_level</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.log_verbosity_level"><code class="docutils literal notranslate"><span class="pre">SessionOptions.log_verbosity_level</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.logid"><code class="docutils literal notranslate"><span class="pre">SessionOptions.logid</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.optimized_model_filepath"><code class="docutils literal notranslate"><span class="pre">SessionOptions.optimized_model_filepath</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.profile_file_prefix"><code class="docutils literal notranslate"><span class="pre">SessionOptions.profile_file_prefix</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.register_custom_ops_library"><code class="docutils literal notranslate"><span class="pre">SessionOptions.register_custom_ops_library()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SessionOptions.use_deterministic_compute"><code class="docutils literal notranslate"><span class="pre">SessionOptions.use_deterministic_compute</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#data">Data</a><ul>
<li><a class="reference internal" href="#ortvalue">OrtValue</a><ul>
<li><a class="reference internal" href="#onnxruntime.OrtValue"><code class="docutils literal notranslate"><span class="pre">OrtValue</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.OrtValue.as_sparse_tensor"><code class="docutils literal notranslate"><span class="pre">OrtValue.as_sparse_tensor()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.data_ptr"><code class="docutils literal notranslate"><span class="pre">OrtValue.data_ptr()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.data_type"><code class="docutils literal notranslate"><span class="pre">OrtValue.data_type()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.device_name"><code class="docutils literal notranslate"><span class="pre">OrtValue.device_name()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.element_type"><code class="docutils literal notranslate"><span class="pre">OrtValue.element_type()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.has_value"><code class="docutils literal notranslate"><span class="pre">OrtValue.has_value()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.is_sparse_tensor"><code class="docutils literal notranslate"><span class="pre">OrtValue.is_sparse_tensor()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.is_tensor"><code class="docutils literal notranslate"><span class="pre">OrtValue.is_tensor()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.is_tensor_sequence"><code class="docutils literal notranslate"><span class="pre">OrtValue.is_tensor_sequence()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.numpy"><code class="docutils literal notranslate"><span class="pre">OrtValue.numpy()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.ort_value_from_sparse_tensor"><code class="docutils literal notranslate"><span class="pre">OrtValue.ort_value_from_sparse_tensor()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.ortvalue_from_numpy"><code class="docutils literal notranslate"><span class="pre">OrtValue.ortvalue_from_numpy()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.ortvalue_from_shape_and_type"><code class="docutils literal notranslate"><span class="pre">OrtValue.ortvalue_from_shape_and_type()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.shape"><code class="docutils literal notranslate"><span class="pre">OrtValue.shape()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.OrtValue.update_inplace"><code class="docutils literal notranslate"><span class="pre">OrtValue.update_inplace()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#sparsetensor">SparseTensor</a><ul>
<li><a class="reference internal" href="#onnxruntime.SparseTensor"><code class="docutils literal notranslate"><span class="pre">SparseTensor</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.as_blocksparse_view"><code class="docutils literal notranslate"><span class="pre">SparseTensor.as_blocksparse_view()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.as_coo_view"><code class="docutils literal notranslate"><span class="pre">SparseTensor.as_coo_view()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.as_csrc_view"><code class="docutils literal notranslate"><span class="pre">SparseTensor.as_csrc_view()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.data_type"><code class="docutils literal notranslate"><span class="pre">SparseTensor.data_type()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.dense_shape"><code class="docutils literal notranslate"><span class="pre">SparseTensor.dense_shape()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.device_name"><code class="docutils literal notranslate"><span class="pre">SparseTensor.device_name()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.format"><code class="docutils literal notranslate"><span class="pre">SparseTensor.format()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.sparse_coo_from_numpy"><code class="docutils literal notranslate"><span class="pre">SparseTensor.sparse_coo_from_numpy()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.sparse_csr_from_numpy"><code class="docutils literal notranslate"><span class="pre">SparseTensor.sparse_csr_from_numpy()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.to_cuda"><code class="docutils literal notranslate"><span class="pre">SparseTensor.to_cuda()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.SparseTensor.values"><code class="docutils literal notranslate"><span class="pre">SparseTensor.values()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#devices">Devices</a><ul>
<li><a class="reference internal" href="#iobinding">IOBinding</a><ul>
<li><a class="reference internal" href="#onnxruntime.IOBinding"><code class="docutils literal notranslate"><span class="pre">IOBinding</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.IOBinding.bind_cpu_input"><code class="docutils literal notranslate"><span class="pre">IOBinding.bind_cpu_input()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.IOBinding.bind_input"><code class="docutils literal notranslate"><span class="pre">IOBinding.bind_input()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.IOBinding.bind_ortvalue_input"><code class="docutils literal notranslate"><span class="pre">IOBinding.bind_ortvalue_input()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.IOBinding.bind_ortvalue_output"><code class="docutils literal notranslate"><span class="pre">IOBinding.bind_ortvalue_output()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.IOBinding.bind_output"><code class="docutils literal notranslate"><span class="pre">IOBinding.bind_output()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.IOBinding.copy_outputs_to_cpu"><code class="docutils literal notranslate"><span class="pre">IOBinding.copy_outputs_to_cpu()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.IOBinding.get_outputs"><code class="docutils literal notranslate"><span class="pre">IOBinding.get_outputs()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#ortdevice">OrtDevice</a><ul>
<li><a class="reference internal" href="#onnxruntime.OrtDevice"><code class="docutils literal notranslate"><span class="pre">OrtDevice</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#internal-classes">Internal classes</a><ul>
<li><a class="reference internal" href="#modelmetadata">ModelMetadata</a><ul>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata"><code class="docutils literal notranslate"><span class="pre">ModelMetadata</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.custom_metadata_map"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.custom_metadata_map</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.description"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.description</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.domain"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.domain</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.graph_description"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.graph_description</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.graph_name"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.graph_name</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.producer_name"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.producer_name</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.ModelMetadata.version"><code class="docutils literal notranslate"><span class="pre">ModelMetadata.version</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#nodearg">NodeArg</a><ul>
<li><a class="reference internal" href="#onnxruntime.NodeArg"><code class="docutils literal notranslate"><span class="pre">NodeArg</span></code></a><ul>
<li><a class="reference internal" href="#onnxruntime.NodeArg.name"><code class="docutils literal notranslate"><span class="pre">NodeArg.name</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.NodeArg.shape"><code class="docutils literal notranslate"><span class="pre">NodeArg.shape</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.NodeArg.type"><code class="docutils literal notranslate"><span class="pre">NodeArg.type</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#backend">Backend</a><ul>
<li><a class="reference internal" href="#onnxruntime.backend.is_compatible"><code class="docutils literal notranslate"><span class="pre">is_compatible()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.backend.prepare"><code class="docutils literal notranslate"><span class="pre">prepare()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.backend.run"><code class="docutils literal notranslate"><span class="pre">run()</span></code></a></li>
<li><a class="reference internal" href="#onnxruntime.backend.supports_device"><code class="docutils literal notranslate"><span class="pre">supports_device()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    </body>
</html>