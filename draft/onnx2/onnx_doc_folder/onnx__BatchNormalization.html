
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>BatchNormalization &#8212; ONNX 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic_mod.css?v=0.7.0-1" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sample.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="BatchNormalization - 14 vs 15" href="text_diff_BatchNormalization_14_15.html" />
    <link rel="prev" title="AveragePool - 1 vs 7" href="text_diff_AveragePool_1_7.html" /> 
  </head><body>

<div id="top_nav">
    

    <nav>
        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../index.html" title="Go to homepage">ONNX 0.1 documentation</a></h1>

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">ONNX operators</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../expect.html">Function expect</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Abs.html">Abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Acos.html">Acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Acosh.html">Acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Add.html">Add</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__And.html">And</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ArgMax.html">ArgMax</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ArgMin.html">ArgMin</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Asin.html">Asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Asinh.html">Asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Atan.html">Atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Atanh.html">Atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__AveragePool.html">AveragePool</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">BatchNormalization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="text_diff_BatchNormalization_14_15.html">BatchNormalization - 14 vs 15</a></li>
<li class="toctree-l3"><a class="reference internal" href="text_diff_BatchNormalization_9_14.html">BatchNormalization - 9 vs 14</a></li>
<li class="toctree-l3"><a class="reference internal" href="text_diff_BatchNormalization_7_9.html">BatchNormalization - 7 vs 9</a></li>
<li class="toctree-l3"><a class="reference internal" href="text_diff_BatchNormalization_6_7.html">BatchNormalization - 6 vs 7</a></li>
<li class="toctree-l3"><a class="reference internal" href="text_diff_BatchNormalization_1_6.html">BatchNormalization - 1 vs 6</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Bernoulli.html">Bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__BitShift.html">BitShift</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__BlackmanWindow.html">BlackmanWindow</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Cast.html">Cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__CastLike.html">CastLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Ceil.html">Ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Celu.html">Celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__CenterCropPad.html">CenterCropPad</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Clip.html">Clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Col2Im.html">Col2Im</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Compress.html">Compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Concat.html">Concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ConcatFromSequence.html">ConcatFromSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Constant.html">Constant</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ConstantOfShape.html">ConstantOfShape</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Conv.html">Conv</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ConvInteger.html">ConvInteger</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ConvTranspose.html">ConvTranspose</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Cos.html">Cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Cosh.html">Cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__CumSum.html">CumSum</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__DFT.html">DFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__DepthToSpace.html">DepthToSpace</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__DequantizeLinear.html">DequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Det.html">Det</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Div.html">Div</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__DynamicQuantizeLinear.html">DynamicQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Einsum.html">Einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Elu.html">Elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Equal.html">Equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Erf.html">Erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Exp.html">Exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Expand.html">Expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__EyeLike.html">EyeLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Floor.html">Floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Gather.html">Gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GatherElements.html">GatherElements</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GatherND.html">GatherND</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Gemm.html">Gemm</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GlobalAveragePool.html">GlobalAveragePool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GlobalLpPool.html">GlobalLpPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GlobalMaxPool.html">GlobalMaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Greater.html">Greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GreaterOrEqual.html">GreaterOrEqual</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__GridSample.html">GridSample</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__HammingWindow.html">HammingWindow</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__HannWindow.html">HannWindow</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__HardSigmoid.html">HardSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__HardSwish.html">HardSwish</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Hardmax.html">Hardmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__If.html">If</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__InstanceNormalization.html">InstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__IsInf.html">IsInf</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__IsNaN.html">IsNaN</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LRN.html">LRN</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LayerNormalization.html">LayerNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LeakyRelu.html">LeakyRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Less.html">Less</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LessOrEqual.html">LessOrEqual</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Log.html">Log</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Loop.html">Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LpNormalization.html">LpNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__LpPool.html">LpPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MatMul.html">MatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MatMulInteger.html">MatMulInteger</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Max.html">Max</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MaxPool.html">MaxPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MaxRoiPool.html">MaxRoiPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MaxUnpool.html">MaxUnpool</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Mean.html">Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MeanVarianceNormalization.html">MeanVarianceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__MelWeightMatrix.html">MelWeightMatrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Min.html">Min</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Mod.html">Mod</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Mul.html">Mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Multinomial.html">Multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Neg.html">Neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__NegativeLogLikelihoodLoss.html">NegativeLogLikelihoodLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__NonMaxSuppression.html">NonMaxSuppression</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__NonZero.html">NonZero</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Not.html">Not</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__OneHot.html">OneHot</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Optional.html">Optional</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__OptionalGetElement.html">OptionalGetElement</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__OptionalHasElement.html">OptionalHasElement</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Or.html">Or</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__PRelu.html">PRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Pad.html">Pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Pow.html">Pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__QLinearConv.html">QLinearConv</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__QLinearMatMul.html">QLinearMatMul</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__QuantizeLinear.html">QuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__RandomNormal.html">RandomNormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__RandomNormalLike.html">RandomNormalLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__RandomUniform.html">RandomUniform</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__RandomUniformLike.html">RandomUniformLike</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Range.html">Range</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Reciprocal.html">Reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceL1.html">ReduceL1</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceL2.html">ReduceL2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceLogSum.html">ReduceLogSum</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceLogSumExp.html">ReduceLogSumExp</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceMax.html">ReduceMax</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceMean.html">ReduceMean</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceMin.html">ReduceMin</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceProd.html">ReduceProd</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceSum.html">ReduceSum</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReduceSumSquare.html">ReduceSumSquare</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Relu.html">Relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Reshape.html">Reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Resize.html">Resize</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ReverseSequence.html">ReverseSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__RoiAlign.html">RoiAlign</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Round.html">Round</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__STFT.html">STFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Scan.html">Scan</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Scatter.html">Scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ScatterElements.html">ScatterElements</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ScatterND.html">ScatterND</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Selu.html">Selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceAt.html">SequenceAt</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceConstruct.html">SequenceConstruct</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceEmpty.html">SequenceEmpty</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceErase.html">SequenceErase</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceInsert.html">SequenceInsert</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceLength.html">SequenceLength</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SequenceMap.html">SequenceMap</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Shape.html">Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Shrink.html">Shrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sign.html">Sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sin.html">Sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sinh.html">Sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Size.html">Size</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Slice.html">Slice</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SoftmaxCrossEntropyLoss.html">SoftmaxCrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SpaceToDepth.html">SpaceToDepth</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Split.html">Split</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__SplitToSequence.html">SplitToSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sqrt.html">Sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Squeeze.html">Squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__StringNormalizer.html">StringNormalizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sub.html">Sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Sum.html">Sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Tan.html">Tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__TfIdfVectorizer.html">TfIdfVectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__ThresholdedRelu.html">ThresholdedRelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Tile.html">Tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__TopK.html">TopK</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Transpose.html">Transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Trilu.html">Trilu</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Unique.html">Unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Unsqueeze.html">Unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Where.html">Where</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx__Xor.html">Xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_ArrayFeatureExtractor.html">ai.onnx.ml - ArrayFeatureExtractor</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_Binarizer.html">ai.onnx.ml - Binarizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_CastMap.html">ai.onnx.ml - CastMap</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_CategoryMapper.html">ai.onnx.ml - CategoryMapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_DictVectorizer.html">ai.onnx.ml - DictVectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_FeatureVectorizer.html">ai.onnx.ml - FeatureVectorizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_Imputer.html">ai.onnx.ml - Imputer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_LabelEncoder.html">ai.onnx.ml - LabelEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_LinearClassifier.html">ai.onnx.ml - LinearClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_LinearRegressor.html">ai.onnx.ml - LinearRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_Normalizer.html">ai.onnx.ml - Normalizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_OneHotEncoder.html">ai.onnx.ml - OneHotEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_SVMClassifier.html">ai.onnx.ml - SVMClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_SVMRegressor.html">ai.onnx.ml - SVMRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_Scaler.html">ai.onnx.ml - Scaler</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_TreeEnsembleClassifier.html">ai.onnx.ml - TreeEnsembleClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_TreeEnsembleRegressor.html">ai.onnx.ml - TreeEnsembleRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxml_ZipMap.html">ai.onnx.ml - ZipMap</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxpreviewtraining_Adagrad.html">ai.onnx.preview.training - Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxpreviewtraining_Adam.html">ai.onnx.preview.training - Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxpreviewtraining_Gradient.html">ai.onnx.preview.training - Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_aionnxpreviewtraining_Momentum.html">ai.onnx.preview.training - Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="table_main.html">operator table for domain main</a></li>
<li class="toctree-l2"><a class="reference internal" href="table_ai_onnx_ml.html">operator table for domain ai.onnx.ml</a></li>
<li class="toctree-l2"><a class="reference internal" href="table_ai_onnx_preview_training.html">operator table for domain ai.onnx.preview.training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../onnx_python/index.html">onnx API Overview</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="batchnormalization">
<span id="l-onnx-doc-batchnormalization"></span><h1>BatchNormalization<a class="headerlink" href="#batchnormalization" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#batchnormalization-15" id="id9">BatchNormalization - 15</a></p></li>
<li><p><a class="reference internal" href="#batchnormalization-14" id="id10">BatchNormalization - 14</a></p></li>
<li><p><a class="reference internal" href="#batchnormalization-9" id="id11">BatchNormalization - 9</a></p></li>
<li><p><a class="reference internal" href="#batchnormalization-7" id="id12">BatchNormalization - 7</a></p></li>
<li><p><a class="reference internal" href="#batchnormalization-6" id="id13">BatchNormalization - 6</a></p></li>
<li><p><a class="reference internal" href="#batchnormalization-1" id="id14">BatchNormalization - 1</a></p></li>
</ul>
</nav>
<section id="batchnormalization-15">
<span id="l-onnx-op-batchnormalization-15"></span><h2><a class="toc-backref" href="#id9" role="doc-backlink">BatchNormalization - 15</a><a class="headerlink" href="#batchnormalization-15" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong></p>
<ul class="simple">
<li><p><strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#BatchNormalization">BatchNormalization (GitHub)</a></p></li>
<li><p><strong>domain</strong>: <strong>main</strong></p></li>
<li><p><strong>since_version</strong>: <strong>15</strong></p></li>
<li><p><strong>function</strong>: False</p></li>
<li><p><strong>support_level</strong>: SupportType.COMMON</p></li>
<li><p><strong>shape inference</strong>: True</p></li>
</ul>
<p>This version of the operator has been available
<strong>since version 15</strong>.</p>
<p><strong>Summary</strong></p>
<p>Carries out batch normalization as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>. Depending on the mode it is being run,
There are five required inputs ‘X’, ‘scale’, ‘B’, ‘input_mean’ and
‘input_var’.
Note that ‘input_mean’ and ‘input_var’ are expected to be the estimated
statistics in inference mode (training_mode=False, default),
and the running statistics in training mode (training_mode=True).
There are multiple cases for the number of outputs, which we list below:</p>
<p>Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (training_mode=False)</p>
<p>When training_mode=False, extra outputs are invalid.
The outputs are updated as follows when training_mode=True:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">input_mean</span> <span class="o">*</span> <span class="n">momentum</span> <span class="o">+</span> <span class="n">current_mean</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span>
<span class="n">running_var</span> <span class="o">=</span> <span class="n">input_var</span> <span class="o">*</span> <span class="n">momentum</span> <span class="o">+</span> <span class="n">current_var</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span>

<span class="n">Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">current_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">current_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">B</span>

<span class="n">where</span><span class="p">:</span>

<span class="n">current_mean</span> <span class="o">=</span> <span class="n">ReduceMean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">all_except_channel_index</span><span class="p">)</span>
<span class="n">current_var</span> <span class="o">=</span>  <span class="n">ReduceVar</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">all_except_channel_index</span><span class="p">)</span>

<span class="n">Notice</span> <span class="n">that</span> <span class="n">ReduceVar</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">the</span> <span class="n">population</span> <span class="n">variance</span><span class="p">,</span> <span class="ow">and</span> <span class="n">it</span> <span class="n">equals</span> <span class="n">to</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">sqrd</span><span class="p">(</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">x_avg</span><span class="p">))</span> <span class="o">/</span> <span class="n">N</span>
<span class="n">where</span> <span class="n">N</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">population</span> <span class="n">size</span> <span class="p">(</span><span class="n">this</span> <span class="n">formula</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">use</span> <span class="n">sample</span> <span class="n">size</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.</p>
<p>When training_mode=False:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">input_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">input_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">B</span>
</pre></div>
</div>
<p>For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C * D1 * D2 * … * Dn) before a BatchNormalization Op.
This operator has <strong>optional</strong> inputs/outputs. See <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/IR.md">ONNX</a> for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument’s name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.</p>
<p><strong>Attributes</strong></p>
<ul class="simple">
<li><p><strong>epsilon</strong>:
The epsilon value to use to avoid division by zero.</p></li>
<li><p><strong>momentum</strong>:
Factor used in computing the running mean and variance.e.g.,
running_mean = running_mean * momentum + mean * (1 - momentum).</p></li>
<li><p><strong>training_mode</strong>:
If set to true, it indicates BatchNormalization is being used for
training, and outputs 1, 2, 3, and 4 would be populated.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions are in the
form of (N x C x D1 x D2 … Dn), where N is the batch size, C is
the number of channels. Statistics are computed for every channel of
C over N and D1 to Dn dimensions. For image data, input dimensions
become (N x C x H x W). The op also accepts single dimension input
of size N in which case C is assumed to be 1</p></li>
<li><p><strong>scale</strong> (heterogeneous) - <strong>T1</strong>:
Scale tensor of shape (C).</p></li>
<li><p><strong>B</strong> (heterogeneous) - <strong>T1</strong>:
Bias tensor of shape (C).</p></li>
<li><p><strong>input_mean</strong> (heterogeneous) - <strong>T2</strong>:
running (training) or estimated (testing) mean tensor of shape (C).</p></li>
<li><p><strong>input_var</strong> (heterogeneous) - <strong>T2</strong>:
running (training) or estimated (testing) variance tensor of shape
(C).</p></li>
</ul>
<p><strong>Outputs</strong></p>
<p>Between 1 and 3 outputs.</p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
The output tensor of the same shape as X</p></li>
<li><p><strong>running_mean</strong> (optional, heterogeneous) - <strong>T2</strong>:
The running mean after the BatchNormalization operator.</p></li>
<li><p><strong>running_var</strong> (optional, heterogeneous) - <strong>T2</strong>:
The running variance after the BatchNormalization operator. This op
uses the population size (N) for calculating variance, and not the
sample size N-1.</p></li>
</ul>
<p><strong>Type Constraints</strong></p>
<ul class="simple">
<li><p><strong>T</strong> in (
tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output types to float tensors.</p></li>
<li><p><strong>T1</strong> in (
tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16)
):
Constrain scale and bias types to float tensors.</p></li>
<li><p><strong>T2</strong> in (
tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16)
):
Constrain mean and variance types to float tensors.</p></li>
</ul>
<p><strong>Examples</strong></p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="text_diff_BatchNormalization_14_15.html">BatchNormalization - 14 vs 15</a></li>
</ul>
</div>
</section>
<section id="batchnormalization-14">
<span id="l-onnx-op-batchnormalization-14"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">BatchNormalization - 14</a><a class="headerlink" href="#batchnormalization-14" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong></p>
<ul class="simple">
<li><p><strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#BatchNormalization">BatchNormalization (GitHub)</a></p></li>
<li><p><strong>domain</strong>: <strong>main</strong></p></li>
<li><p><strong>since_version</strong>: <strong>14</strong></p></li>
<li><p><strong>function</strong>: False</p></li>
<li><p><strong>support_level</strong>: SupportType.COMMON</p></li>
<li><p><strong>shape inference</strong>: True</p></li>
</ul>
<p>This version of the operator has been available
<strong>since version 14</strong>.</p>
<p><strong>Summary</strong></p>
<p>Carries out batch normalization as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>. Depending on the mode it is being run,
There are five required inputs ‘X’, ‘scale’, ‘B’, ‘input_mean’ and
‘input_var’.
Note that ‘input_mean’ and ‘input_var’ are expected to be the estimated
statistics in inference mode (training_mode=False, default),
and the running statistics in training mode (training_mode=True).
There are multiple cases for the number of outputs, which we list below:</p>
<p>Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (training_mode=False)</p>
<p>When training_mode=False, extra outputs are invalid.
The outputs are updated as follows when training_mode=True:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">input_mean</span> <span class="o">*</span> <span class="n">momentum</span> <span class="o">+</span> <span class="n">current_mean</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span>
<span class="n">running_var</span> <span class="o">=</span> <span class="n">input_var</span> <span class="o">*</span> <span class="n">momentum</span> <span class="o">+</span> <span class="n">current_var</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span>

<span class="n">Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">current_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">current_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">B</span>

<span class="n">where</span><span class="p">:</span>

<span class="n">current_mean</span> <span class="o">=</span> <span class="n">ReduceMean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">all_except_channel_index</span><span class="p">)</span>
<span class="n">current_var</span> <span class="o">=</span>  <span class="n">ReduceVar</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">all_except_channel_index</span><span class="p">)</span>

<span class="n">Notice</span> <span class="n">that</span> <span class="n">ReduceVar</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">the</span> <span class="n">population</span> <span class="n">variance</span><span class="p">,</span> <span class="ow">and</span> <span class="n">it</span> <span class="n">equals</span> <span class="n">to</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">sqrd</span><span class="p">(</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">x_avg</span><span class="p">))</span> <span class="o">/</span> <span class="n">N</span>
<span class="n">where</span> <span class="n">N</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">population</span> <span class="n">size</span> <span class="p">(</span><span class="n">this</span> <span class="n">formula</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">use</span> <span class="n">sample</span> <span class="n">size</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>When training_mode=False:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">input_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">input_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">B</span>
</pre></div>
</div>
<p>For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C * D1 * D2 * … * Dn) before a BatchNormalization Op.
This operator has <strong>optional</strong> inputs/outputs. See <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/IR.md">ONNX</a> for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument’s name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.</p>
<p><strong>Attributes</strong></p>
<ul class="simple">
<li><p><strong>epsilon</strong>:
The epsilon value to use to avoid division by zero.</p></li>
<li><p><strong>momentum</strong>:
Factor used in computing the running mean and variance.e.g.,
running_mean = running_mean * momentum + mean * (1 - momentum).</p></li>
<li><p><strong>training_mode</strong>:
If set to true, it indicates BatchNormalization is being used for
training, and outputs 1, 2, 3, and 4 would be populated.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions are in the
form of (N x C x D1 x D2 … Dn), where N is the batch size, C is
the number of channels. Statistics are computed for every channel of
C over N and D1 to Dn dimensions. For image data, input dimensions
become (N x C x H x W). The op also accepts single dimension input
of size N in which case C is assumed to be 1</p></li>
<li><p><strong>scale</strong> (heterogeneous) - <strong>T</strong>:
Scale tensor of shape (C).</p></li>
<li><p><strong>B</strong> (heterogeneous) - <strong>T</strong>:
Bias tensor of shape (C).</p></li>
<li><p><strong>input_mean</strong> (heterogeneous) - <strong>U</strong>:
running (training) or estimated (testing) mean tensor of shape (C).</p></li>
<li><p><strong>input_var</strong> (heterogeneous) - <strong>U</strong>:
running (training) or estimated (testing) variance tensor of shape
(C).</p></li>
</ul>
<p><strong>Outputs</strong></p>
<p>Between 1 and 3 outputs.</p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
The output tensor of the same shape as X</p></li>
<li><p><strong>running_mean</strong> (optional, heterogeneous) - <strong>U</strong>:
The running mean after the BatchNormalization operator.</p></li>
<li><p><strong>running_var</strong> (optional, heterogeneous) - <strong>U</strong>:
The running variance after the BatchNormalization operator. This op
uses the population size (N) for calculating variance, and not the
sample size N-1.</p></li>
</ul>
<p><strong>Type Constraints</strong></p>
<ul class="simple">
<li><p><strong>T</strong> in (
tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output types to float tensors.</p></li>
<li><p><strong>U</strong> in (
tensor(bfloat16),
tensor(double),
tensor(float),
tensor(float16)
):
Constrain mean and variance types to float tensors. It allows all
float type for U.</p></li>
</ul>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="text_diff_BatchNormalization_9_14.html">BatchNormalization - 9 vs 14</a></li>
</ul>
</div>
</section>
<section id="batchnormalization-9">
<span id="l-onnx-op-batchnormalization-9"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink">BatchNormalization - 9</a><a class="headerlink" href="#batchnormalization-9" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong></p>
<ul class="simple">
<li><p><strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#BatchNormalization">BatchNormalization (GitHub)</a></p></li>
<li><p><strong>domain</strong>: <strong>main</strong></p></li>
<li><p><strong>since_version</strong>: <strong>9</strong></p></li>
<li><p><strong>function</strong>: False</p></li>
<li><p><strong>support_level</strong>: SupportType.COMMON</p></li>
<li><p><strong>shape inference</strong>: True</p></li>
</ul>
<p>This version of the operator has been available
<strong>since version 9</strong>.</p>
<p><strong>Summary</strong></p>
<p>Carries out batch normalization as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>. Depending on the mode it is being run,
there are multiple cases for the number of outputs, which we list below:</p>
<p>Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)</p>
<p>For previous (depreciated) non-spatial cases, implementors are suggested
to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.
This operator has <strong>optional</strong> inputs/outputs. See <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/IR.md">ONNX</a> for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument’s name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.</p>
<p><strong>Attributes</strong></p>
<ul class="simple">
<li><p><strong>epsilon</strong>:
The epsilon value to use to avoid division by zero.</p></li>
<li><p><strong>momentum</strong>:
Factor used in computing the running mean and variance.e.g.,
running_mean = running_mean * momentum + mean * (1 - momentum).</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions are in the
form of (N x C x D1 x D2 … Dn), where N is the batch size, C is
the number of channels. Statistics are computed for every channel of
C over N and D1 to Dn dimensions. For image data, input dimensions
become (N x C x H x W). The op also accepts single dimension input
of size N in which case C is assumed to be 1</p></li>
<li><p><strong>scale</strong> (heterogeneous) - <strong>T</strong>:
Scale tensor of shape (C).</p></li>
<li><p><strong>B</strong> (heterogeneous) - <strong>T</strong>:
Bias tensor of shape (C).</p></li>
<li><p><strong>mean</strong> (heterogeneous) - <strong>T</strong>:
running (training) or estimated (testing) mean tensor of shape (C).</p></li>
<li><p><strong>var</strong> (heterogeneous) - <strong>T</strong>:
running (training) or estimated (testing) variance tensor of shape
(C).</p></li>
</ul>
<p><strong>Outputs</strong></p>
<p>Between 1 and 5 outputs.</p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
The output tensor of the same shape as X</p></li>
<li><p><strong>mean</strong> (optional, heterogeneous) - <strong>T</strong>:
The running mean after the BatchNormalization operator.</p></li>
<li><p><strong>var</strong> (optional, heterogeneous) - <strong>T</strong>:
The running variance after the BatchNormalization operator.</p></li>
<li><p><strong>saved_mean</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved mean used during training to speed up gradient computation.</p></li>
<li><p><strong>saved_var</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved variance used during training to speed up gradient
computation.</p></li>
</ul>
<p><strong>Type Constraints</strong></p>
<ul class="simple">
<li><p><strong>T</strong> in (
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output types to float tensors.</p></li>
</ul>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="text_diff_BatchNormalization_7_9.html">BatchNormalization - 7 vs 9</a></li>
</ul>
</div>
</section>
<section id="batchnormalization-7">
<span id="l-onnx-op-batchnormalization-7"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink">BatchNormalization - 7</a><a class="headerlink" href="#batchnormalization-7" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong></p>
<ul class="simple">
<li><p><strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#BatchNormalization">BatchNormalization (GitHub)</a></p></li>
<li><p><strong>domain</strong>: <strong>main</strong></p></li>
<li><p><strong>since_version</strong>: <strong>7</strong></p></li>
<li><p><strong>function</strong>: False</p></li>
<li><p><strong>support_level</strong>: SupportType.COMMON</p></li>
<li><p><strong>shape inference</strong>: True</p></li>
</ul>
<p>This version of the operator has been available
<strong>since version 7</strong>.</p>
<p><strong>Summary</strong></p>
<p>Carries out batch normalization as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>. Depending on the mode it is being run,
there are multiple cases for the number of outputs, which we list below:</p>
<p>Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)</p>
<blockquote>
<div><p>This operator has <strong>optional</strong> inputs/outputs. See <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/IR.md">ONNX</a> for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument’s name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.</p>
</div></blockquote>
<p><strong>Attributes</strong></p>
<ul class="simple">
<li><p><strong>epsilon</strong>:
The epsilon value to use to avoid division by zero.</p></li>
<li><p><strong>momentum</strong>:
Factor used in computing the running mean and variance.e.g.,
running_mean = running_mean * momentum + mean * (1 - momentum).</p></li>
<li><p><strong>spatial</strong>:
If true, compute the mean and variance across per activation. If
false, compute the mean and variance across per feature over each
mini-batch.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions for image
case are (N x C x H x W), where N is the batch size, C is the number
of channels, and H and W are the height and the width of the data.
For non image case, the dimensions are in the form of (N x C x D1 x
D2 … Dn), where N is the batch size.</p></li>
<li><p><strong>scale</strong> (heterogeneous) - <strong>T</strong>:
If spatial is true, the dimension of scale is (C). If spatial is
false, the dimensions of scale are (C x D1 x … x Dn)</p></li>
<li><p><strong>B</strong> (heterogeneous) - <strong>T</strong>:
If spatial is true, the dimension of bias is (C). If spatial is
false, the dimensions of bias are (C x D1 x … x Dn)</p></li>
<li><p><strong>mean</strong> (heterogeneous) - <strong>T</strong>:
If spatial is true, the dimension of the running mean (training) or
the estimated mean (testing) is (C). If spatial is false, the
dimensions of the running mean (training) or the estimated mean
(testing) are (C x D1 x … x Dn).</p></li>
<li><p><strong>var</strong> (heterogeneous) - <strong>T</strong>:
If spatial is true, the dimension of the running variance(training)
or the estimated variance (testing) is (C). If spatial is false, the
dimensions of the running variance(training) or the estimated
variance (testing) are (C x D1 x … x Dn).</p></li>
</ul>
<p><strong>Outputs</strong></p>
<p>Between 1 and 5 outputs.</p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
The output tensor of the same shape as X</p></li>
<li><p><strong>mean</strong> (optional, heterogeneous) - <strong>T</strong>:
The running mean after the BatchNormalization operator.</p></li>
<li><p><strong>var</strong> (optional, heterogeneous) - <strong>T</strong>:
The running variance after the BatchNormalization operator.</p></li>
<li><p><strong>saved_mean</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved mean used during training to speed up gradient computation.</p></li>
<li><p><strong>saved_var</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved variance used during training to speed up gradient
computation.</p></li>
</ul>
<p><strong>Type Constraints</strong></p>
<ul class="simple">
<li><p><strong>T</strong> in (
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output types to float tensors.</p></li>
</ul>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="text_diff_BatchNormalization_6_7.html">BatchNormalization - 6 vs 7</a></li>
</ul>
</div>
</section>
<section id="batchnormalization-6">
<span id="l-onnx-op-batchnormalization-6"></span><h2><a class="toc-backref" href="#id13" role="doc-backlink">BatchNormalization - 6</a><a class="headerlink" href="#batchnormalization-6" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong></p>
<ul class="simple">
<li><p><strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#BatchNormalization">BatchNormalization (GitHub)</a></p></li>
<li><p><strong>domain</strong>: <strong>main</strong></p></li>
<li><p><strong>since_version</strong>: <strong>6</strong></p></li>
<li><p><strong>function</strong>: False</p></li>
<li><p><strong>support_level</strong>: SupportType.COMMON</p></li>
<li><p><strong>shape inference</strong>: True</p></li>
</ul>
<p>This version of the operator has been available
<strong>since version 6</strong>.</p>
<p><strong>Summary</strong></p>
<p>Carries out batch normalization as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>. Depending on the mode it is being run,
there are multiple cases for the number of outputs, which we list below:</p>
<p>Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)</p>
<p><strong>Attributes</strong></p>
<ul class="simple">
<li><p><strong>epsilon</strong>:
The epsilon value to use to avoid division by zero, default is
1e-5f.</p></li>
<li><p><strong>is_test</strong>:
If set to nonzero, run spatial batch normalization in test mode,
default is 0.</p></li>
<li><p><strong>momentum</strong>:
Factor used in computing the running mean and variance.e.g.,
running_mean = running_mean * momentum + mean * (1 - momentum),
default is 0.9f.</p></li>
<li><p><strong>spatial</strong>:
If true, compute the mean and variance across all spatial elements
If false, compute the mean and variance across per feature.Default
is 1.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
Input data tensor from the previous operator; dimensions for image
case are (N x C x H x W), where N is the batch size, C is the number
of channels, and H and W are the height and the width of the data.
For non image case, the dimensions are in the form of (N x C x D1 x
D2 … Dn), where N is the batch size.</p></li>
<li><p><strong>scale</strong> (heterogeneous) - <strong>T</strong>:
The scale as a 1-dimensional tensor of size C to be applied to the
output.</p></li>
<li><p><strong>B</strong> (heterogeneous) - <strong>T</strong>:
The bias as a 1-dimensional tensor of size C to be applied to the
output.</p></li>
<li><p><strong>mean</strong> (heterogeneous) - <strong>T</strong>:
The running mean (training) or the estimated mean (testing) as a
1-dimensional tensor of size C.</p></li>
<li><p><strong>var</strong> (heterogeneous) - <strong>T</strong>:
The running variance (training) or the estimated variance (testing)
as a 1-dimensional tensor of size C.</p></li>
</ul>
<p><strong>Outputs</strong></p>
<p>Between 1 and 5 outputs.</p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
The output tensor of the same shape as X.</p></li>
<li><p><strong>mean</strong> (optional, heterogeneous) - <strong>T</strong>:
The running mean after the BatchNormalization operator. Must be in-
place with the input mean. Should not be used for testing.</p></li>
<li><p><strong>var</strong> (optional, heterogeneous) - <strong>T</strong>:
The running variance after the BatchNormalization operator. Must be
in-place with the input var. Should not be used for testing.</p></li>
<li><p><strong>saved_mean</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved mean used during training to speed up gradient computation.
Should not be used for testing.</p></li>
<li><p><strong>saved_var</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved variance used during training to speed up gradient
computation. Should not be used for testing.</p></li>
</ul>
<p><strong>Type Constraints</strong></p>
<ul class="simple">
<li><p><strong>T</strong> in (
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output types to float tensors.</p></li>
</ul>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="text_diff_BatchNormalization_1_6.html">BatchNormalization - 1 vs 6</a></li>
</ul>
</div>
</section>
<section id="batchnormalization-1">
<span id="l-onnx-op-batchnormalization-1"></span><h2><a class="toc-backref" href="#id14" role="doc-backlink">BatchNormalization - 1</a><a class="headerlink" href="#batchnormalization-1" title="Permalink to this heading">¶</a></h2>
<p><strong>Version</strong></p>
<ul class="simple">
<li><p><strong>name</strong>: <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Operators.md#BatchNormalization">BatchNormalization (GitHub)</a></p></li>
<li><p><strong>domain</strong>: <strong>main</strong></p></li>
<li><p><strong>since_version</strong>: <strong>1</strong></p></li>
<li><p><strong>function</strong>: False</p></li>
<li><p><strong>support_level</strong>: SupportType.COMMON</p></li>
<li><p><strong>shape inference</strong>: False</p></li>
</ul>
<p>This version of the operator has been available
<strong>since version 1</strong>.</p>
<p><strong>Summary</strong></p>
<p>Carries out batch normalization as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>. Depending on the mode it is being run,
there are multiple cases for the number of outputs, which we list below:</p>
<p>Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #2: Y (test mode)</p>
<p><strong>Attributes</strong></p>
<ul class="simple">
<li><p><strong>consumed_inputs</strong> (required):
legacy optimization attribute.</p></li>
<li><p><strong>epsilon</strong>:
The epsilon value to use to avoid division by zero, default is
1e-5f.</p></li>
<li><p><strong>is_test</strong>:
If set to nonzero, run spatial batch normalization in test mode,
default is 0.</p></li>
<li><p><strong>momentum</strong>:
Factor used in computing the running mean and variance.e.g.,
running_mean = running_mean * momentum + mean * (1 - momentum),
default is 0.9f.</p></li>
<li><p><strong>spatial</strong>:
If true, compute the mean and variance across all spatial elements
If false, compute the mean and variance across per feature.Default
is 1.</p></li>
</ul>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p><strong>X</strong> (heterogeneous) - <strong>T</strong>:
The input 4-dimensional tensor of shape NCHW.</p></li>
<li><p><strong>scale</strong> (heterogeneous) - <strong>T</strong>:
The scale as a 1-dimensional tensor of size C to be applied to the
output.</p></li>
<li><p><strong>B</strong> (heterogeneous) - <strong>T</strong>:
The bias as a 1-dimensional tensor of size C to be applied to the
output.</p></li>
<li><p><strong>mean</strong> (heterogeneous) - <strong>T</strong>:
The running mean (training) or the estimated mean (testing) as a
1-dimensional tensor of size C.</p></li>
<li><p><strong>var</strong> (heterogeneous) - <strong>T</strong>:
The running variance (training) or the estimated variance (testing)
as a 1-dimensional tensor of size C.</p></li>
</ul>
<p><strong>Outputs</strong></p>
<p>Between 1 and 5 outputs.</p>
<ul class="simple">
<li><p><strong>Y</strong> (heterogeneous) - <strong>T</strong>:
The output 4-dimensional tensor of the same shape as X.</p></li>
<li><p><strong>mean</strong> (optional, heterogeneous) - <strong>T</strong>:
The running mean after the BatchNormalization operator. Must be in-
place with the input mean. Should not be used for testing.</p></li>
<li><p><strong>var</strong> (optional, heterogeneous) - <strong>T</strong>:
The running variance after the BatchNormalization operator. Must be
in-place with the input var. Should not be used for testing.</p></li>
<li><p><strong>saved_mean</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved mean used during training to speed up gradient computation.
Should not be used for testing.</p></li>
<li><p><strong>saved_var</strong> (optional, heterogeneous) - <strong>T</strong>:
Saved variance used during training to speed up gradient
computation. Should not be used for testing.</p></li>
</ul>
<p><strong>Type Constraints</strong></p>
<ul class="simple">
<li><p><strong>T</strong> in (
tensor(double),
tensor(float),
tensor(float16)
):
Constrain input and output types to float tensors.</p></li>
</ul>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon"><</span><span>Page contents<span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">></span><span>Page contents:<span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">BatchNormalization</a><ul>
<li><a class="reference internal" href="#batchnormalization-15">BatchNormalization - 15</a></li>
<li><a class="reference internal" href="#batchnormalization-14">BatchNormalization - 14</a></li>
<li><a class="reference internal" href="#batchnormalization-9">BatchNormalization - 9</a></li>
<li><a class="reference internal" href="#batchnormalization-7">BatchNormalization - 7</a></li>
<li><a class="reference internal" href="#batchnormalization-6">BatchNormalization - 6</a></li>
<li><a class="reference internal" href="#batchnormalization-1">BatchNormalization - 1</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
                <a href="text_diff_AveragePool_1_7.html">
                    <span class="icon"><</span><span>AveragePool - 1 vs 7</span></a>
                
            </div>

            <div class="right">
                
                    <a href="text_diff_BatchNormalization_14_15.html"><span>BatchNormalization - 14 vs 15</span><span class="icon">></span></a>
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
        &#169; Copyright 2022.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.1.1.
    </div>

<p id="theme_credit">Styled using the <a href="https://github.com/piccolo-orm/piccolo_theme">Piccolo Theme</a></p>
  </body>
</html>