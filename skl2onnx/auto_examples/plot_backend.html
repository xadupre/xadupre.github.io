<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">

  <title>ONNX Runtime Backend for ONNX</title>

  <link rel="stylesheet" href="../_static/sphinx-modern-theme-modified.css" type="text/css"/>
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 1rem;">
    <div id="sidebar" class="col-xs-12 col-sm-3">
      <a href="../index.html">
        <img style="margin-bottom: 0.5rem;" class="img-fluid" src="../_static/logo_main.png"/>
      </a>
      <div id="searchbox" style="display: none" role="search">
        <form class="form-inline" action="../search.html" method="get">
          <div class="form-group">
            <label class="sr-only" for="searchInput">Search</label>
            <input type="text" class="form-control" name="q" id="searchInput" placeholder="Search">
          </div>
          <button type="submit" class="btn btn-secondary" style="display:none">Go</button>
          <input type="hidden" name="check_keywords" value="yes"/>
          <input type="hidden" name="area" value="default"/>
        </form>
      </div>

      <hr>

        <div id="toc">
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorial.html#step-1-train-a-model-using-your-favorite-framework">Step 1: Train a model using your favorite framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial.html#step-2-convert-or-export-the-model-into-onnx-format">Step 2: Convert or export the model into ONNX format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial.html#step-3-load-and-run-the-model-using-onnx-runtime">Step 3: Load and run the model using ONNX Runtime</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_summary.html">API Summary</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_summary.html#converters">Converters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_summary.html#manipulate-onnx-graphs">Manipulate ONNX graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_summary.html#registered-functions">Registered functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_summary.html#parsers">Parsers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_summary.html#utils-for-contributors">Utils for contributors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api_summary.html#concepts">Concepts</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Gallery of examples</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">ONNX Runtime Backend for ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_pipeline.html">Draw a pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_metadata.html">Metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_convert_model.html">Train, convert and predict a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_errors_onnxruntime.html">Errors with onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_pipeline_lightgbm.html">Convert a pipeline with a LightGbm model</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_complex_pipeline.html">Convert a pipeline with ColumnTransformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_tfidfvectorizer.html">TfIdfVectorizer with ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_errors_pipeline.html">Errors while converting a pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_intermediate_outputs.html">Walk through intermediate outputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_transfer_learning.html">Transfer learning with ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_custom_model.html">Write your own converter for your own model</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../supported.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">Convert a pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../pipeline.html#convert-complex-pipelines">Convert complex pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pipeline.html#parser-shape-calculator-converter">Parser, shape calculator, converter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pipeline.html#new-converters-in-a-pipeline">New converters in a pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pipeline.html#titanic-example">Titanic example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pipeline.html#parameterize-the-conversion">Parameterize the conversion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../parameterized.html">Converters with options</a></li>
</ul>

        </div>
    </div>
    <div class="col-xs-12 col-sm-9">
      <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-auto-examples-plot-backend-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="onnx-runtime-backend-for-onnx">
<span id="l-example-backend-api"></span><span id="sphx-glr-auto-examples-plot-backend-py"></span><h1>ONNX Runtime Backend for ONNX<a class="headerlink" href="#onnx-runtime-backend-for-onnx" title="Permalink to this headline">¶</a></h1>
<p id="index-0"><em>ONNX Runtime</em> extends the
<a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/ImplementingAnOnnxBackend.md">onnx backend API</a>
to run predictions using this runtime.
Let’s use the API to compute the prediction
of a simple logistic regression model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">onnxruntime.backend</span> <span class="kn">as</span> <span class="nn">backend</span>
<span class="kn">from</span> <span class="nn">onnx</span> <span class="kn">import</span> <span class="n">load</span>

<span class="n">name</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">get_example</span><span class="p">(</span><span class="s2">&quot;logreg_iris.onnx&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

<span class="n">rep</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">label</span><span class="p">,</span> <span class="n">proba</span> <span class="o">=</span> <span class="n">rep</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;label={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;probabilities={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">proba</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>label=[1]
probabilities=[{0: 0.02731134556233883, 1: 0.5175684094429016, 2: 0.4551202654838562}]
</pre></div>
</div>
<p>The device depends on how the package was compiled,
GPU or CPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">get_device</span>
<span class="k">print</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>CPU-MKL-DNN
</pre></div>
</div>
<p>The backend can also directly load the model
without using <em>onnx</em>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rep</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">label</span><span class="p">,</span> <span class="n">proba</span> <span class="o">=</span> <span class="n">rep</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;label={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;probabilities={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">proba</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>label=[1]
probabilities=[{0: 0.02731134556233883, 1: 0.5175684094429016, 2: 0.4551202654838562}]
</pre></div>
</div>
<p>The backend API is implemented by other frameworks
and makes it easier to switch between multiple runtimes
with the same API.</p>
<p><strong>Versions used for this example</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span><span class="o">,</span> <span class="nn">sklearn</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;numpy:&quot;</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;scikit-learn:&quot;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">onnx</span><span class="o">,</span> <span class="nn">onnxruntime</span><span class="o">,</span> <span class="nn">skl2onnx</span><span class="o">,</span> <span class="nn">onnxmltools</span><span class="o">,</span> <span class="nn">lightgbm</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;onnx: &quot;</span><span class="p">,</span> <span class="n">onnx</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;onnxruntime: &quot;</span><span class="p">,</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;skl2onnx: &quot;</span><span class="p">,</span> <span class="n">skl2onnx</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>numpy: 1.16.1
scikit-learn: 0.20.2
onnx:  1.4.1
onnxruntime:  0.2.1
skl2onnx:  1.4.2
</pre></div>
</div>
<p><strong>Total running time of the script:</strong> ( 0 minutes  2.256 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-backend-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" download="" href="../_downloads/a24bfe4f53e3156f769fe21b57034aec/plot_backend.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_backend.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" download="" href="../_downloads/af39656f1279ea029b5934ee867d9c9f/plot_backend.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_backend.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>

    </div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.0.0/jquery.min.js"
        integrity="sha384-THPy051/pYDQGanwU6poAc/hOdQxjnOEXzbT+OuUAFqNqFjL+4IGLBgCJC3ZOShY"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.2.0/js/tether.min.js"
        integrity="sha384-Plbmg8JY28KFelvJVai01l8WyZzrYWG825m+cZ0eDDS1f7d/js6ikvy1+X+guPIB"
        crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.3/js/bootstrap.min.js"
        integrity="sha384-ux8v3A6CPtOTqOzMKiuo3d/DomGaaClxFYdCu2HPMBEkf6x2xiDyJ7gkXU0MWwaD"
        crossorigin="anonymous"></script>
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/0.6.0/lunr.min.js"></script>
<script src="../_static/searchtools.js"></script>
<script>$('#searchbox').show(0)</script>
</body>
</html>