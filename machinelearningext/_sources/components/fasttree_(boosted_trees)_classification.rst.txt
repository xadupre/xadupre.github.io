

.. _l-fasttree-(boosted-trees)-classification:

FastTree (Boosted Trees) Classification
=======================================

.. only:: not md

    
    The documentation is generated based on the sources available at
    :epkg:`dotnet/machinelearning` and released under :epkg:`MIT License`.
    
    

.. only:: md

    **Type:** binaryclassifiertrainer
    **Aliases:** *FastTreeBinaryClassification, FastTreeClassification, FastTree, ft, ftc, FastRankBinaryClassification, FastRankBinaryClassificationWrapper, FastRankClassification, fr, btc, frc, fastrank, fastrankwrapper*
    **Namespace:** Microsoft.ML.Trainers.FastTree
    **Assembly:** Microsoft.ML.FastTree.dll

.. only:: not md

    **Type:** binaryclassifiertrainer
    **Aliases:** *FastTreeBinaryClassification, FastTreeClassification, FastTree, ft, ftc, FastRankBinaryClassification, FastRankBinaryClassificationWrapper, FastRankClassification, fr, btc, frc, fastrank, fastrankwrapper*
    **Namespace:** Microsoft.ML.Trainers.FastTree
    **Assembly:** Microsoft.ML.FastTree.dll
    **Microsoft Documentation:** `FastTree (Boosted Trees) Classification <https://docs.microsoft.com/dotnet/api/microsoft.ml.trainers.fasttree.fasttree (boosted trees) classification>`_

**Description**

Uses a logit-boost boosted tree learner to perform binary classification.


**Parameters**

.. list-table::
    :widths: 5 5 5 20
    :header-rows: 1

    * - Name
      - Short name
      - Default
      - Description
    * - allowEmptyTrees
      - allowempty
      - True
      - When a root split is impossible, allow training to proceed
    * - baggingSize
      - bag
      - 0
      - Number of trees in each bag (0 for disabling bagging)
    * - baggingTrainFraction
      - bagfrac
      - 0.7
      - Percentage of training examples used in each bag
    * - baselineAlphaRisk
      - basealpha
      - 
      - Baseline alpha for tradeoffs of risk (0 is normal training)
    * - baselineScoresFormula
      - basescores
      - 
      - Freeform defining the scores that should be used as the baseline ranker
    * - bestStepRankingRegressionTrees
      - bsr
      - False
      - Use best regression step trees?
    * - bias
      - 
      - 0
      - Bias for calculating gradient for each feature bin for a categorical feature.
    * - bundling
      - bundle
      - None
      - Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1): Bundle low population, Bundle.Adjacent(2): Neighbor low population bundle.
    * - categoricalSplit
      - cat
      - False
      - Whether to do split based on multiple categorical feature values.
    * - compressEnsemble
      - cmp
      - False
      - Compress the tree Ensemble
    * - diskTranspose
      - dt
      - 
      - Whether to utilize the disk or the data's native transposition facilities (where applicable) when performing the transpose
    * - dropoutRate
      - tdrop
      - 0
      - Dropout rate for tree regularization
    * - earlyStoppingMetrics
      - esmt
      - 0
      - Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking, 1:NDCG@1, 3:NDCG@3)
    * - earlyStoppingRule
      - esr
      - 
      - Early stopping rule. (Validation set (/valid) is required.)
    * - enablePruning
      - pruning
      - False
      - Enable post-training pruning to avoid overfitting. (a validation set is required)
    * - entropyCoefficient
      - e
      - 0
      - The entropy (regularization) coefficient between 0 and 1
    * - executionTimes
      - et
      - False
      - Print execution time breakdown to stdout
    * - featureCompressionLevel
      - fcomp
      - 1
      - The level of feature compression to use
    * - featureFirstUsePenalty
      - ffup
      - 0
      - The feature first use penalty coefficient
    * - featureFlocks
      - flocks
      - True
      - Whether to collectivize features during dataset preparation to speed up training
    * - featureFraction
      - ff
      - 1
      - The fraction of features (chosen randomly) to use on each iteration
    * - featureReusePenalty
      - frup
      - 0
      - The feature re-use penalty (regularization) coefficient
    * - featureSelectSeed
      - r3
      - 123
      - The seed of the active feature selection
    * - filterZeroLambdas
      - fzl
      - False
      - Filter zero lambdas during training
    * - gainConfidenceLevel
      - gainconf
      - 0
      - Tree fitting gain confidence requirement (should be in the range [0,1) ).
    * - getDerivativesSampleRate
      - sr
      - 1
      - Sample each query 1 in k times in the GetDerivatives function
    * - histogramPoolSize
      - ps
      - -1
      - The number of histograms in the pool (between 2 and numLeaves)
    * - learningRates
      - lr
      - 0.2
      - The learning rate
    * - maxBins
      - mb
      - 255
      - Maximum number of distinct values (bins) per feature
    * - maxCategoricalGroupsPerNode
      - mcg
      - 64
      - Maximum categorical split groups to consider when splitting on a categorical feature. Split groups are a collection of split points. This is used to reduce overfitting when there many categorical features.
    * - maxCategoricalSplitPoints
      - maxcat
      - 64
      - Maximum categorical split points to consider when splitting on a categorical feature.
    * - maxTreeOutput
      - mo
      - 100
      - Upper bound on absolute value of single tree output
    * - maxTreesAfterCompression
      - cmpmax
      - -1
      - Maximum Number of trees after compression
    * - minDocsForCategoricalSplit
      - mdo
      - 100
      - Minimum categorical doc count in a bin to consider for a split.
    * - minDocsPercentageForCategoricalSplit
      - mdop
      - 0.001
      - Minimum categorical docs percentage in a bin to consider for a split.
    * - minDocumentsInLeafs
      - mil
      - 10
      - The minimal number of documents allowed in a leaf of a regression tree, out of the subsampled data
    * - minStepSize
      - minstep
      - 0
      - Minimum line search step size
    * - numLeaves
      - nl
      - 20
      - The max number of leaves in each regression tree
    * - numPostBracketSteps
      - lssteps
      - 0
      - Number of post-bracket line search steps
    * - numThreads
      - t
      - 
      - The number of threads to use
    * - numTrees
      - iter
      - 100
      - Total number of decision trees to create in the ensemble
    * - optimizationAlgorithm
      - oa
      - GradientDescent
      - Optimization algorithm to be used (GradientDescent, AcceleratedGradientDescent)
    * - parallelTrainer
      - parag
      - Microsoft. ML. Trainers. FastTree. SingleTrainerFactory
      - Allows to choose Parallel FastTree Learning Algorithm
    * - positionDiscountFreeform
      - pdff
      - 
      - The discount freeform which specifies the per position discounts of documents in a query (uses a single variable P for position where P=0 is first position)
    * - printTestGraph
      - graph
      - False
      - Print metrics graph for the first test set
    * - printTrainValidGraph
      - graphtv
      - False
      - Print Train and Validation metrics in graph
    * - pruningThreshold
      - prth
      - 0.004
      - The tolerance threshold for pruning
    * - pruningWindowSize
      - prws
      - 5
      - The moving window size for pruning
    * - randomStart
      - rs
      - False
      - Training starts from random ordering (determined by /r1)
    * - rngSeed
      - r1
      - 123
      - The seed of the random number generator
    * - shrinkage
      - shrk
      - 1
      - Shrinkage
    * - smoothing
      - s
      - 0
      - Smoothing paramter for tree regularization
    * - softmaxTemperature
      - smtemp
      - 0
      - The temperature of the randomized softmax distribution for choosing the feature
    * - sparsifyThreshold
      - sp
      - 0.7
      - Sparsity level needed to use sparse feature representation
    * - splitFraction
      - sf
      - 1
      - The fraction of features (chosen randomly) to use on each split
    * - testFrequency
      - tf
      - 2147483647
      - Calculate metric values for train/valid/test every k rounds
    * - unbalancedSets
      - us
      - False
      - Should we use derivatives optimized for unbalanced sets
    * - useLineSearch
      - ls
      - False
      - Should we use line search for a step size
    * - useTolerantPruning
      - prtol
      - False
      - Use window and tolerance for pruning
    * - writeLastEnsemble
      - hl
      - False
      - Write the last ensemble instead of the one determined by early stopping
